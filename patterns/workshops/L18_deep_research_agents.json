{
  "$schema": "../learning-workshop-pattern.json",
  "workshopId": "L18_deep_research_agents",
  "name": "Deep Research Agents: Multi-Step Reasoning and Discovery",
  "technology": {
    "category": "ai_framework",
    "stack": "LangGraph + Research Tools",
    "language": "Python",
    "version": "LangGraph 0.2+"
  },
  "level": "advanced",
  "prerequisites": {
    "workshops": ["L3_langgraph_workflows", "L7_langchain_fundamentals"],
    "knowledge": [
      "LangGraph state machines and conditional routing",
      "LangChain tools and agents",
      "Python async/await patterns",
      "Understanding of search APIs (Tavily, SerpAPI)",
      "Basic knowledge of reasoning patterns (chain-of-thought, reflection)"
    ],
    "tools": [
      "Python 3.10+",
      "OpenAI or Anthropic API key",
      "Tavily API key or SerpAPI key (for search)",
      "VS Code with Python extension"
    ]
  },
  "duration": {
    "total_hours": 2.5,
    "concept_minutes": 30,
    "demo_minutes": 30,
    "exercise_minutes": 45,
    "challenge_minutes": 30,
    "reflection_minutes": 15
  },
  "learning_objectives": [
    {
      "objective": "Design multi-step research workflows with iterative refinement",
      "bloom_level": "create",
      "verification": "Creates a research workflow that decomposes queries, performs iterative searches, and refines results"
    },
    {
      "objective": "Implement chain-of-thought and reflection patterns for reasoning",
      "bloom_level": "apply",
      "verification": "Implements self-critique and verification loops that improve research quality"
    },
    {
      "objective": "Build source discovery and validation pipelines",
      "bloom_level": "apply",
      "verification": "Creates a pipeline that discovers sources, validates credibility, and scores reliability"
    },
    {
      "objective": "Create synthesis agents that aggregate and summarize findings",
      "bloom_level": "apply",
      "verification": "Builds a synthesis agent that combines multiple sources with proper citation"
    },
    {
      "objective": "Implement human-in-the-loop for research quality control",
      "bloom_level": "apply",
      "verification": "Adds checkpoints for human review and approval in research workflow"
    }
  ],
  "knowledge_files": [
    "research-agent-patterns.json",
    "langgraph-workflows.json",
    "reasoning-patterns.json"
  ],
  "phases": [
    {
      "phaseId": "concept",
      "name": "Deep Research Architecture and Reasoning Patterns",
      "type": "concept",
      "duration_minutes": 30,
      "description": "Understanding deep research systems like Perplexity/Gemini Deep Research and reasoning patterns",
      "content": {
        "topics": [
          "Deep Research Architecture: Query decomposition, iterative search, synthesis",
          "Multi-step reasoning: Chain-of-thought, tree-of-thought, reflection loops",
          "Query planning: Breaking complex questions into sub-queries",
          "Iterative refinement: Using search results to generate better queries",
          "Source discovery: Finding diverse, credible sources",
          "Credibility scoring: Domain authority, recency, citation count",
          "Reflection patterns: Self-critique, verification, confidence scoring",
          "Synthesis: Aggregating findings with proper citation and attribution",
          "Uncertainty handling: Expressing confidence levels and gaps",
          "Human-in-the-loop: Quality gates and approval checkpoints"
        ],
        "diagrams": [
          "Deep research workflow architecture",
          "Query decomposition tree",
          "Iterative search refinement loop",
          "Reflection and self-critique pattern",
          "Source validation pipeline",
          "Synthesis aggregation flow"
        ],
        "key_points": [
          "Deep research requires multiple search iterations, not single queries",
          "Query decomposition enables parallel exploration of sub-topics",
          "Reflection loops improve quality through self-critique",
          "Source validation prevents misinformation",
          "Synthesis must preserve attribution and handle conflicts",
          "Confidence scoring helps users understand reliability",
          "Human oversight ensures quality and ethical research"
        ]
      },
      "facilitator_notes": "Emphasize the iterative nature of deep research. Show examples of how single searches fail compared to multi-step research. Discuss the importance of verification and citation.",
      "common_questions": [
        "How many search iterations should I perform?",
        "How do I prevent infinite loops in iterative search?",
        "What makes a source credible?",
        "How do I handle conflicting information from sources?",
        "When should I ask for human approval?"
      ]
    },
    {
      "phaseId": "demo",
      "name": "Building a Deep Research Agent",
      "type": "demo",
      "duration_minutes": 30,
      "description": "Live coding a research agent that searches, validates, and synthesizes",
      "content": {
        "topics": [
          "Setting up LangGraph state for research workflow",
          "Implementing query decomposition node",
          "Creating search nodes with Tavily/SerpAPI",
          "Building source validation and credibility scoring",
          "Implementing reflection node for self-critique",
          "Creating synthesis node with citation",
          "Adding confidence scoring and uncertainty handling",
          "Integrating human approval checkpoint",
          "Visualizing the research graph"
        ],
        "code_examples": [
          "Research state schema with queries, sources, findings",
          "Query decomposition function",
          "Search tool integration",
          "Credibility scoring algorithm",
          "Reflection and self-critique node",
          "Synthesis with citation formatting",
          "Human-in-the-loop checkpoint"
        ],
        "key_points": [
          "State should track queries, sources, findings, and confidence",
          "Query decomposition improves coverage",
          "Credibility scoring prevents low-quality sources",
          "Reflection catches gaps and inconsistencies",
          "Synthesis must cite sources properly",
          "Confidence scores help users interpret results"
        ]
      },
      "facilitator_notes": "Show real search results and demonstrate how iterative refinement improves quality. Explain credibility scoring factors."
    },
    {
      "phaseId": "exercise_1",
      "name": "Chain-of-Thought Reasoning with Self-Critique",
      "type": "exercise",
      "duration_minutes": 20,
      "description": "Implement reasoning with reflection loops",
      "content": {
        "topics": [
          "Create reasoning node that generates chain-of-thought",
          "Implement self-critique node that evaluates reasoning",
          "Build reflection loop that refines answers",
          "Add confidence scoring based on critique",
          "Handle cases where reflection improves vs degrades quality"
        ]
      }
    },
    {
      "phaseId": "exercise_2",
      "name": "Source Discovery with Credibility Scoring",
      "type": "exercise",
      "duration_minutes": 25,
      "description": "Build source validation pipeline",
      "content": {
        "topics": [
          "Implement search tool integration",
          "Create source metadata extraction",
          "Build credibility scoring function",
          "Filter sources by credibility threshold",
          "Rank sources by relevance and credibility",
          "Handle source diversity (avoid duplicates)"
        ]
      }
    },
    {
      "phaseId": "challenge",
      "name": "Full Deep Research Pipeline",
      "type": "challenge",
      "duration_minutes": 30,
      "description": "Create complete deep research system with multi-source synthesis",
      "content": {
        "topics": [
          "Query decomposition and planning",
          "Iterative search with refinement",
          "Source validation and credibility scoring",
          "Reflection and self-critique loops",
          "Multi-source synthesis with citation",
          "Confidence scoring and uncertainty handling",
          "Human approval checkpoint"
        ]
      }
    },
    {
      "phaseId": "reflection",
      "name": "Research Ethics, Citation, and Bias Awareness",
      "type": "reflection",
      "duration_minutes": 15,
      "description": "Consolidate learning and discuss ethical considerations",
      "content": {
        "topics": [
          "Summary of deep research patterns",
          "Research ethics: Attribution, bias, misinformation",
          "Citation best practices",
          "Handling conflicting sources",
          "Bias detection and mitigation",
          "Transparency in AI research",
          "Production considerations: Rate limits, costs, monitoring"
        ],
        "key_points": [
          "Always cite sources to enable verification",
          "Be transparent about confidence and uncertainty",
          "Detect and mitigate bias in sources",
          "Handle conflicts by presenting multiple perspectives",
          "Human oversight ensures ethical research",
          "Monitor for misinformation and low-quality sources"
        ]
      }
    }
  ],
  "exercises": [
    {
      "exerciseId": "ex1_reasoning_reflection",
      "name": "Chain-of-Thought Reasoning with Self-Critique",
      "type": "guided",
      "difficulty": "medium",
      "duration_minutes": 20,
      "description": "Implement reasoning with reflection loops that improve answer quality",
      "starter_code": "from typing import TypedDict, Annotated\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph.message import add_messages\n\nclass ReasoningState(TypedDict):\n    messages: Annotated[list, add_messages]\n    question: str\n    initial_reasoning: str | None\n    critique: str | None\n    refined_answer: str | None\n    confidence: float\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\n# TODO: Create reasoning node that generates chain-of-thought\ndef reasoning_node(state: ReasoningState) -> ReasoningState:\n    \"\"\"Generate initial reasoning with chain-of-thought.\"\"\"\n    question = state['question']\n    \n    # TODO: Prompt LLM to think step-by-step about the question\n    prompt = f\"\"\"Answer the following question using chain-of-thought reasoning.\n    Show your thinking process step by step.\n    \n    Question: {question}\n    \n    Reasoning:\"\"\"\n    \n    # TODO: Get LLM response\n    # response = llm.invoke(...)\n    \n    return {\n        'initial_reasoning': \"[Your reasoning here]\",\n        'confidence': 0.7\n    }\n\n# TODO: Create self-critique node\ndef critique_node(state: ReasoningState) -> ReasoningState:\n    \"\"\"Critique the initial reasoning.\"\"\"\n    question = state['question']\n    reasoning = state['initial_reasoning']\n    \n    # TODO: Prompt LLM to critique the reasoning\n    prompt = f\"\"\"Critique the following reasoning. Identify:\n    1. Logical gaps or errors\n    2. Missing information\n    3. Unsupported claims\n    4. Areas of uncertainty\n    \n    Question: {question}\n    Reasoning: {reasoning}\n    \n    Critique:\"\"\"\n    \n    # TODO: Get critique\n    \n    return {\n        'critique': \"[Your critique here]\"\n    }\n\n# TODO: Create refinement node\ndef refine_node(state: ReasoningState) -> ReasoningState:\n    \"\"\"Refine the answer based on critique.\"\"\"\n    question = state['question']\n    reasoning = state['initial_reasoning']\n    critique = state['critique']\n    \n    # TODO: Prompt LLM to refine answer addressing critique\n    \n    return {\n        'refined_answer': \"[Refined answer]\",\n        'confidence': 0.9  # Higher confidence after refinement\n    }\n\n# TODO: Create routing function\ndef should_refine(state: ReasoningState) -> str:\n    \"\"\"Decide if refinement is needed.\"\"\"\n    # TODO: Check if critique found significant issues\n    # Return 'refine' if needed, 'end' otherwise\n    return 'refine'\n\n# TODO: Build graph\ngraph = StateGraph(ReasoningState)\n# Add nodes and edges...",
      "solution_code": "from typing import TypedDict, Annotated, Literal\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langgraph.graph.message import add_messages\n\nclass ReasoningState(TypedDict):\n    messages: Annotated[list, add_messages]\n    question: str\n    initial_reasoning: str | None\n    critique: str | None\n    refined_answer: str | None\n    confidence: float\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\nreasoning_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a careful reasoner. Think step-by-step.\"),\n    (\"human\", \"Answer the following question using chain-of-thought reasoning. Show your thinking process step by step.\\n\\nQuestion: {question}\\n\\nReasoning:\")\n])\n\ncritique_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a critical reviewer. Identify flaws and gaps.\"),\n    (\"human\", \"Critique the following reasoning. Identify:\\n1. Logical gaps or errors\\n2. Missing information\\n3. Unsupported claims\\n4. Areas of uncertainty\\n\\nQuestion: {question}\\nReasoning: {reasoning}\\n\\nCritique:\")\n])\n\nrefine_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a careful reasoner. Refine your answer based on critique.\"),\n    (\"human\", \"Refine your answer to address the critique.\\n\\nQuestion: {question}\\nOriginal Reasoning: {reasoning}\\nCritique: {critique}\\n\\nRefined Answer:\")\n])\n\ndef reasoning_node(state: ReasoningState) -> ReasoningState:\n    \"\"\"Generate initial reasoning with chain-of-thought.\"\"\"\n    question = state['question']\n    \n    chain = reasoning_prompt | llm\n    response = chain.invoke({\"question\": question})\n    reasoning = response.content\n    \n    return {\n        'initial_reasoning': reasoning,\n        'confidence': 0.7,\n        'messages': [AIMessage(content=f\"Initial reasoning: {reasoning}\")]\n    }\n\ndef critique_node(state: ReasoningState) -> ReasoningState:\n    \"\"\"Critique the initial reasoning.\"\"\"\n    question = state['question']\n    reasoning = state['initial_reasoning']\n    \n    chain = critique_prompt | llm\n    response = chain.invoke({\"question\": question, \"reasoning\": reasoning})\n    critique = response.content\n    \n    return {\n        'critique': critique,\n        'messages': state['messages'] + [AIMessage(content=f\"Critique: {critique}\")]\n    }\n\ndef refine_node(state: ReasoningState) -> ReasoningState:\n    \"\"\"Refine the answer based on critique.\"\"\"\n    question = state['question']\n    reasoning = state['initial_reasoning']\n    critique = state['critique']\n    \n    chain = refine_prompt | llm\n    response = chain.invoke({\n        \"question\": question,\n        \"reasoning\": reasoning,\n        \"critique\": critique\n    })\n    refined = response.content\n    \n    # Assess if critique found significant issues\n    critique_lower = critique.lower()\n    has_issues = any(word in critique_lower for word in [\"error\", \"gap\", \"missing\", \"unsupported\", \"uncertain\"])\n    confidence = 0.9 if has_issues else 0.85\n    \n    return {\n        'refined_answer': refined,\n        'confidence': confidence,\n        'messages': state['messages'] + [AIMessage(content=f\"Refined answer: {refined}\")]\n    }\n\ndef should_refine(state: ReasoningState) -> Literal['refine', 'end']:\n    \"\"\"Decide if refinement is needed.\"\"\"\n    critique = state.get('critique', '')\n    if not critique:\n        return 'end'\n    \n    # Check if critique found significant issues\n    critique_lower = critique.lower()\n    has_issues = any(word in critique_lower for word in [\"error\", \"gap\", \"missing\", \"unsupported\"])\n    \n    return 'refine' if has_issues else 'end'\n\ngraph = StateGraph(ReasoningState)\ngraph.add_node('reason', reasoning_node)\ngraph.add_node('critique', critique_node)\ngraph.add_node('refine', refine_node)\n\ngraph.set_entry_point('reason')\ngraph.add_edge('reason', 'critique')\ngraph.add_conditional_edges('critique', should_refine, {\n    'refine': 'refine',\n    'end': END\n})\ngraph.add_edge('refine', END)\n\napp = graph.compile()\n\n# Test\nresult = app.invoke({\n    'question': 'What are the main causes of climate change?',\n    'messages': [],\n    'initial_reasoning': None,\n    'critique': None,\n    'refined_answer': None,\n    'confidence': 0.0\n})\n\nprint(f\"Final answer: {result.get('refined_answer', result.get('initial_reasoning'))}\")\nprint(f\"Confidence: {result['confidence']}\")",
      "hints": [
        "Use ChatPromptTemplate for structured prompts",
        "Chain-of-thought should show step-by-step thinking",
        "Critique should identify specific issues, not just praise",
        "Refinement should address each critique point",
        "Confidence should increase after successful refinement"
      ],
      "verification": "Reasoning node generates chain-of-thought, critique identifies issues, refinement improves answer",
      "common_mistakes": [
        "Not showing actual reasoning steps in chain-of-thought",
        "Critique being too generic or not actionable",
        "Refinement not addressing specific critique points",
        "Not updating confidence scores",
        "Forgetting to add messages to state"
      ]
    },
    {
      "exerciseId": "ex2_source_discovery",
      "name": "Source Discovery with Credibility Scoring",
      "type": "guided",
      "difficulty": "hard",
      "duration_minutes": 25,
      "description": "Build source validation pipeline with credibility scoring",
      "starter_code": "from typing import TypedDict, Annotated, List\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.messages import AIMessage\nfrom langgraph.graph.message import add_messages\nfrom pydantic import BaseModel\n\nclass Source(BaseModel):\n    url: str\n    title: str\n    snippet: str\n    domain: str\n    credibility_score: float = 0.0\n\nclass SourceDiscoveryState(TypedDict):\n    messages: Annotated[list, add_messages]\n    query: str\n    sources: List[Source]\n    filtered_sources: List[Source]\n\n# TODO: Create search node (simulate with mock data for demo)\ndef search_node(state: SourceDiscoveryState) -> SourceDiscoveryState:\n    \"\"\"Search for sources.\"\"\"\n    query = state['query']\n    \n    # TODO: In real implementation, call Tavily or SerpAPI\n    # For demo, create mock sources\n    mock_sources = [\n        Source(\n            url=\"https://example.com/article1\",\n            title=\"Reliable Source Article\",\n            snippet=\"This is a well-researched article...\",\n            domain=\"example.com\",\n            credibility_score=0.0\n        ),\n        # TODO: Add more mock sources\n    ]\n    \n    return {\n        'sources': mock_sources\n    }\n\n# TODO: Create credibility scoring node\ndef score_credibility_node(state: SourceDiscoveryState) -> SourceDiscoveryState:\n    \"\"\"Score source credibility.\"\"\"\n    sources = state['sources']\n    \n    # TODO: Implement credibility scoring based on:\n    # - Domain authority (known domains get higher scores)\n    # - Recency (if available)\n    # - URL patterns (.edu, .gov get higher scores)\n    # - Title quality\n    \n    scored_sources = []\n    for source in sources:\n        score = 0.5  # Base score\n        \n        # TODO: Add domain authority scoring\n        # if source.domain in trusted_domains:\n        #     score += 0.3\n        \n        # TODO: Add URL pattern scoring\n        # if '.edu' in source.url or '.gov' in source.url:\n        #     score += 0.2\n        \n        # TODO: Update source with score\n        # source.credibility_score = score\n        scored_sources.append(source)\n    \n    return {\n        'sources': scored_sources\n    }\n\n# TODO: Create filtering node\ndef filter_sources_node(state: SourceDiscoveryState) -> SourceDiscoveryState:\n    \"\"\"Filter sources by credibility threshold.\"\"\"\n    sources = state['sources']\n    threshold = 0.6  # Minimum credibility score\n    \n    # TODO: Filter sources above threshold\n    # filtered = [s for s in sources if s.credibility_score >= threshold]\n    \n    return {\n        'filtered_sources': sources  # TODO: Replace with filtered\n    }\n\n# TODO: Build graph\ngraph = StateGraph(SourceDiscoveryState)\n# Add nodes and edges...",
      "solution_code": "from typing import TypedDict, Annotated, List\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.messages import AIMessage\nfrom langgraph.graph.message import add_messages\nfrom pydantic import BaseModel\n\nclass Source(BaseModel):\n    url: str\n    title: str\n    snippet: str\n    domain: str\n    credibility_score: float = 0.0\n\nclass SourceDiscoveryState(TypedDict):\n    messages: Annotated[list, add_messages]\n    query: str\n    sources: List[Source]\n    filtered_sources: List[Source]\n\n# Trusted domains (in production, use a larger list)\nTRUSTED_DOMAINS = {\n    'edu': 0.3,\n    'gov': 0.3,\n    'wikipedia.org': 0.2,\n    'nature.com': 0.25,\n    'science.org': 0.25,\n    'arxiv.org': 0.2\n}\n\n# Known low-quality domains\nLOW_QUALITY_DOMAINS = {\n    'blogspot.com': -0.2,\n    'wordpress.com': -0.1\n}\n\ndef search_node(state: SourceDiscoveryState) -> SourceDiscoveryState:\n    \"\"\"Search for sources (mock implementation).\"\"\"\n    query = state['query']\n    \n    # In real implementation, use Tavily or SerpAPI:\n    # from tavily import TavilyClient\n    # client = TavilyClient(api_key=os.getenv('TAVILY_API_KEY'))\n    # results = client.search(query=query, max_results=10)\n    \n    # Mock sources for demo\n    mock_sources = [\n        Source(\n            url=\"https://www.nasa.gov/climate-change/\",\n            title=\"NASA Climate Change Evidence\",\n            snippet=\"Comprehensive data on climate change from NASA...\",\n            domain=\"nasa.gov\",\n            credibility_score=0.0\n        ),\n        Source(\n            url=\"https://en.wikipedia.org/wiki/Climate_change\",\n            title=\"Climate Change - Wikipedia\",\n            snippet=\"Overview of climate change science...\",\n            domain=\"wikipedia.org\",\n            credibility_score=0.0\n        ),\n        Source(\n            url=\"https://randomblog.blogspot.com/climate\",\n            title=\"My Thoughts on Climate\",\n            snippet=\"I think climate change might not be real...\",\n            domain=\"blogspot.com\",\n            credibility_score=0.0\n        ),\n        Source(\n            url=\"https://www.science.org/climate-research\",\n            title=\"Climate Research - Science Magazine\",\n            snippet=\"Peer-reviewed research on climate...\",\n            domain=\"science.org\",\n            credibility_score=0.0\n        )\n    ]\n    \n    return {\n        'sources': mock_sources,\n        'messages': [AIMessage(content=f\"Found {len(mock_sources)} sources for query: {query}\")]\n    }\n\ndef score_credibility_node(state: SourceDiscoveryState) -> SourceDiscoveryState:\n    \"\"\"Score source credibility.\"\"\"\n    sources = state['sources']\n    \n    scored_sources = []\n    for source in sources:\n        score = 0.5  # Base score\n        \n        # Domain authority scoring\n        domain_lower = source.domain.lower()\n        for trusted_domain, bonus in TRUSTED_DOMAINS.items():\n            if trusted_domain in domain_lower:\n                score += bonus\n                break\n        \n        # Penalize low-quality domains\n        for low_domain, penalty in LOW_QUALITY_DOMAINS.items():\n            if low_domain in domain_lower:\n                score += penalty\n                break\n        \n        # URL pattern scoring\n        if '.edu' in source.url or '.gov' in source.url:\n            score += 0.2\n        \n        # Title quality (simple heuristic)\n        title_lower = source.title.lower()\n        if any(word in title_lower for word in ['research', 'study', 'peer-reviewed', 'journal']):\n            score += 0.1\n        \n        # Ensure score is between 0 and 1\n        score = max(0.0, min(1.0, score))\n        \n        # Update source\n        source.credibility_score = score\n        scored_sources.append(source)\n    \n    # Sort by credibility\n    scored_sources.sort(key=lambda s: s.credibility_score, reverse=True)\n    \n    return {\n        'sources': scored_sources,\n        'messages': state['messages'] + [AIMessage(content=f\"Scored {len(scored_sources)} sources\")]\n    }\n\ndef filter_sources_node(state: SourceDiscoveryState) -> SourceDiscoveryState:\n    \"\"\"Filter sources by credibility threshold.\"\"\"\n    sources = state['sources']\n    threshold = 0.6  # Minimum credibility score\n    \n    filtered = [s for s in sources if s.credibility_score >= threshold]\n    \n    # Also remove duplicates (simple URL-based)\n    seen_urls = set()\n    unique_filtered = []\n    for source in filtered:\n        if source.url not in seen_urls:\n            seen_urls.add(source.url)\n            unique_filtered.append(source)\n    \n    return {\n        'filtered_sources': unique_filtered,\n        'messages': state['messages'] + [AIMessage(content=f\"Filtered to {len(unique_filtered)} credible sources\")]\n    }\n\ngraph = StateGraph(SourceDiscoveryState)\ngraph.add_node('search', search_node)\ngraph.add_node('score', score_credibility_node)\ngraph.add_node('filter', filter_sources_node)\n\ngraph.set_entry_point('search')\ngraph.add_edge('search', 'score')\ngraph.add_edge('score', 'filter')\ngraph.add_edge('filter', END)\n\napp = graph.compile()\n\n# Test\nresult = app.invoke({\n    'query': 'What causes climate change?',\n    'messages': [],\n    'sources': [],\n    'filtered_sources': []\n})\n\nprint(f\"\\nFiltered {len(result['filtered_sources'])} credible sources:\")\nfor source in result['filtered_sources']:\n    print(f\"  [{source.credibility_score:.2f}] {source.title} - {source.domain}\")",
      "hints": [
        "Use domain-based heuristics for credibility scoring",
        "Consider URL patterns (.edu, .gov) for authority",
        "Filter by threshold and remove duplicates",
        "Sort sources by credibility score",
        "In production, use real search APIs (Tavily, SerpAPI)"
      ],
      "verification": "Sources are scored, filtered by threshold, and sorted by credibility",
      "common_mistakes": [
        "Not implementing actual credibility scoring logic",
        "Forgetting to filter low-quality sources",
        "Not handling duplicate sources",
        "Not sorting by credibility",
        "Using mock data instead of real search APIs"
      ]
    }
  ],
  "challenges": [
    {
      "challengeId": "deep_research_pipeline",
      "name": "Full Deep Research Pipeline with Multi-Source Synthesis",
      "description": "Build a complete deep research system that decomposes queries, performs iterative searches, validates sources, reflects on findings, and synthesizes results",
      "requirements": [
        "Implement query decomposition that breaks complex questions into sub-queries",
        "Create iterative search with refinement (at least 2 iterations)",
        "Build source validation with credibility scoring",
        "Implement reflection loop with self-critique",
        "Create synthesis agent that aggregates findings with proper citation",
        "Add confidence scoring and uncertainty handling",
        "Include human approval checkpoint before final output",
        "Handle maximum iteration limits to prevent infinite loops",
        "Provide source citations in final output"
      ],
      "evaluation_criteria": [
        "Query decomposition creates meaningful sub-queries",
        "Iterative search improves result quality",
        "Sources are validated and filtered by credibility",
        "Reflection identifies gaps and improves answers",
        "Synthesis combines sources with proper attribution",
        "Confidence scores reflect actual uncertainty",
        "Human checkpoint works correctly",
        "System handles edge cases (no results, conflicting sources)",
        "Final output includes citations"
      ],
      "stretch_goals": [
        "Implement parallel sub-query execution",
        "Add source diversity scoring (avoid similar sources)",
        "Create visual graph of research flow",
        "Integrate with LangSmith for tracing",
        "Add bias detection in sources",
        "Implement query rewriting based on initial results"
      ]
    }
  ],
  "resources": {
    "official_docs": [
      "https://langchain-ai.github.io/langgraph/",
      "https://python.langchain.com/docs/use_cases/research/",
      "https://docs.tavily.com/",
      "https://serpapi.com/"
    ],
    "tutorials": [
      "LangGraph Research Agent Tutorial",
      "Building Deep Research Systems - LangChain Blog",
      "Chain-of-Thought Reasoning Patterns"
    ],
    "videos": [
      "Deep Research Agents Explained",
      "Building Perplexity-like Research Systems"
    ],
    "community": [
      "LangChain Discord #langgraph",
      "GitHub Discussions - Research Agents",
      "Stack Overflow - langgraph tag"
    ]
  },
  "assessment": {
    "knowledge_check": [
      {
        "question": "What is the key difference between single-search and deep research systems?",
        "type": "short_answer",
        "answer": "Deep research systems perform multiple iterative searches, decompose queries into sub-queries, validate sources, reflect on findings, and synthesize results. Single-search systems make one query and return results without refinement or validation.",
        "explanation": "Deep research requires iteration and reflection for quality"
      },
      {
        "question": "What are the main components of a reflection loop?",
        "type": "short_answer",
        "answer": "A reflection loop consists of: 1) Initial reasoning/answer generation, 2) Self-critique that identifies gaps and errors, 3) Refinement that addresses critique points, 4) Confidence scoring based on critique findings. The loop can iterate multiple times until quality is sufficient.",
        "explanation": "Reflection improves quality through self-critique"
      },
      {
        "question": "How should you handle conflicting information from multiple sources?",
        "type": "short_answer",
        "answer": "Present multiple perspectives, cite conflicting sources, explain the conflict, assess source credibility, and express uncertainty. Don't arbitrarily choose one side - let users see the full picture with proper attribution.",
        "explanation": "Transparency and citation are essential for ethical research"
      },
      {
        "question": "What factors should be considered in credibility scoring?",
        "type": "short_answer",
        "answer": "Domain authority (trusted domains like .edu, .gov), URL patterns, recency of information, citation count, peer review status, and source diversity. Avoid low-quality domains and check for bias.",
        "explanation": "Credibility scoring prevents misinformation"
      }
    ],
    "practical_assessment": "Build a deep research agent that: decomposes a complex query, performs iterative searches, validates sources, reflects on findings, synthesizes results with citations, and includes human approval checkpoint",
    "self_assessment": [
      "Can I design multi-step research workflows?",
      "Do I understand chain-of-thought and reflection patterns?",
      "Can I implement source discovery and validation?",
      "Can I create synthesis agents with proper citation?",
      "Do I know when to add human-in-the-loop checkpoints?",
      "Can I handle uncertainty and express confidence appropriately?"
    ]
  },
  "next_steps": {
    "next_workshop": "L5_crewai_multiagent",
    "practice_projects": [
      "Build a research assistant for academic papers",
      "Create a fact-checking agent with source validation",
      "Implement a competitive analysis research system",
      "Build a news aggregation system with bias detection"
    ],
    "deeper_learning": [
      "Advanced reasoning patterns (tree-of-thought, self-consistency)",
      "Multi-agent research systems with specialized agents",
      "Bias detection and mitigation techniques",
      "Production deployment of research agents"
    ]
  },
  "axiom_zero_integration": {
    "love_moments": [
      "Celebrating when reflection improves answer quality",
      "Encouraging careful source validation",
      "Patient debugging of iterative search loops",
      "Appreciating well-cited research outputs"
    ],
    "truth_moments": [
      "Emphasizing verification and citation requirements",
      "Honest about limitations and uncertainty",
      "Clear explanation of credibility scoring factors",
      "Transparent about conflicts and multiple perspectives",
      "Acknowledging when research is incomplete or uncertain"
    ],
    "beauty_moments": [
      "Elegant query decomposition trees",
      "Clean reflection loops that improve quality",
      "Well-structured synthesis with proper attribution",
      "Beautiful citation formatting",
      "Graceful handling of uncertainty and conflicts"
    ]
  }
}
