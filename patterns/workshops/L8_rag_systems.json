{
  "$schema": "../learning-workshop-pattern.json",
  "workshopId": "L8_rag_systems",
  "name": "RAG System Architecture",
  "technology": {
    "category": "ai_framework",
    "stack": "LangChain + Vector DB",
    "language": "Python",
    "version": "LangChain 0.3+"
  },
  "level": "fundamentals",
  "prerequisites": {
    "workshops": ["L7_langchain_fundamentals"],
    "knowledge": [
      "Python programming",
      "Basic understanding of embeddings and vector similarity",
      "LangChain fundamentals (chains, retrievers)",
      "Understanding of document processing"
    ],
    "tools": [
      "Python 3.10+",
      "OpenAI API key (or other LLM provider)",
      "Vector database (Chroma, Pinecone, or FAISS)",
      "VS Code or similar IDE"
    ]
  },
  "duration": {
    "total_hours": 2.5,
    "concept_minutes": 30,
    "demo_minutes": 30,
    "exercise_minutes": 45,
    "challenge_minutes": 30,
    "reflection_minutes": 15
  },
  "learning_objectives": [
    {
      "objective": "Implement document loading and chunking strategies for different document types",
      "bloom_level": "apply",
      "verification": "Successfully loads and chunks documents with appropriate strategies"
    },
    {
      "objective": "Select and use embedding models for different use cases",
      "bloom_level": "apply",
      "verification": "Chooses appropriate embedding model and generates embeddings"
    },
    {
      "objective": "Perform vector store operations: indexing, querying, and updating",
      "bloom_level": "apply",
      "verification": "Creates vector store, indexes documents, and retrieves relevant chunks"
    },
    {
      "objective": "Implement different retrieval strategies: similarity search, MMR, and hybrid search",
      "bloom_level": "apply",
      "verification": "Implements multiple retrieval strategies and compares results"
    },
    {
      "objective": "Evaluate RAG systems using RAGAS metrics",
      "bloom_level": "apply",
      "verification": "Runs RAGAS evaluation and interprets metrics"
    }
  ],
  "knowledge_files": [
    "rag-patterns.json",
    "vector-database-patterns.json",
    "langchain-patterns.json"
  ],
  "phases": [
    {
      "phaseId": "concept",
      "name": "RAG Architecture Overview",
      "type": "concept",
      "duration_minutes": 30,
      "description": "Understand the components and flow of RAG systems",
      "content": {
        "topics": [
          "RAG Pipeline: Load → Chunk → Embed → Store → Retrieve → Generate",
          "Document Loaders: PDF, HTML, Markdown, CSV",
          "Text Splitting: Chunk size, overlap, and strategies",
          "Embeddings: Model selection and dimensionality",
          "Vector Stores: Chroma, Pinecone, FAISS, Weaviate",
          "Retrieval Strategies: Similarity, MMR, Hybrid",
          "Generation: Context injection and prompt engineering",
          "Evaluation: RAGAS metrics (faithfulness, answer relevance, context precision)"
        ],
        "diagrams": [
          "RAG pipeline flow diagram",
          "Chunking strategies visualization",
          "Vector similarity search",
          "MMR vs similarity search comparison",
          "RAGAS evaluation metrics"
        ],
        "key_points": [
          "Chunk size affects retrieval quality (too small = fragmented, too large = noisy)",
          "Overlap prevents context loss at chunk boundaries",
          "Embedding model choice impacts retrieval accuracy",
          "MMR balances relevance and diversity",
          "Hybrid search combines semantic and keyword matching",
          "Evaluation is crucial for production RAG systems"
        ]
      },
      "facilitator_notes": "Emphasize the importance of chunking strategy. Show how different retrieval methods affect results.",
      "common_questions": [
        "What chunk size should I use?",
        "When should I use MMR vs similarity search?",
        "How do I choose an embedding model?",
        "What makes a good RAG evaluation?"
      ]
    },
    {
      "phaseId": "demo",
      "name": "Building a Simple RAG",
      "type": "demo",
      "duration_minutes": 30,
      "description": "Live coding a complete RAG system",
      "content": {
        "topics": [
          "Loading documents with LangChain loaders",
          "Splitting text with RecursiveCharacterTextSplitter",
          "Creating embeddings with OpenAI",
          "Storing in Chroma vector store",
          "Building retrieval chain",
          "Testing retrieval quality",
          "Adding conversation memory"
        ],
        "code_examples": [
          "Document loading and chunking",
          "Vector store creation and indexing",
          "Retrieval chain with LCEL",
          "Conversational RAG with memory"
        ],
        "key_points": [
          "Chunk size of 1000 with 200 overlap works well for most cases",
          "Always test retrieval quality before generation",
          "Include source citations in responses",
          "Memory enables follow-up questions"
        ]
      },
      "facilitator_notes": "Show real retrieval results and explain why certain chunks are retrieved. Discuss chunking trade-offs."
    },
    {
      "phaseId": "exercise_1",
      "name": "Chunking Experiments",
      "type": "exercise",
      "duration_minutes": 45,
      "description": "Experiment with different chunking strategies",
      "content": {
        "topics": [
          "Try different chunk sizes",
          "Experiment with overlap values",
          "Test semantic chunking",
          "Compare retrieval quality",
          "Analyze chunk boundaries"
        ]
      }
    },
    {
      "phaseId": "challenge",
      "name": "Conversational RAG",
      "type": "challenge",
      "duration_minutes": 30,
      "description": "Build a RAG system with conversation memory",
      "content": {
        "topics": [
          "Add conversation memory",
          "Handle follow-up questions",
          "Implement query rewriting",
          "Add source citations",
          "Evaluate with RAGAS"
        ]
      }
    },
    {
      "phaseId": "reflection",
      "name": "Key Takeaways and Production Considerations",
      "type": "reflection",
      "duration_minutes": 15,
      "description": "Consolidate learning and discuss production deployment",
      "content": {
        "topics": [
          "Summary of RAG patterns",
          "Chunking best practices",
          "Retrieval strategy selection",
          "Production considerations (scaling, monitoring, costs)",
          "Evaluation and continuous improvement",
          "Resources for continued learning"
        ],
        "key_points": [
          "Chunking strategy is critical for retrieval quality",
          "Test retrieval before optimizing generation",
          "Use MMR for diverse results",
          "Hybrid search improves recall",
          "Regular evaluation ensures quality"
        ]
      }
    }
  ],
  "exercises": [
    {
      "exerciseId": "ex1_chunking",
      "name": "Chunking Experiments",
      "type": "guided",
      "difficulty": "medium",
      "duration_minutes": 45,
      "description": "Experiment with different chunking strategies and compare results",
      "starter_code": "from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI\n\n# TODO: Load a sample document\n# loader = TextLoader(\"sample.txt\")\n# documents = loader.load()\n\n# TODO: Create different text splitters\n# Strategy 1: Small chunks (200 chars, 50 overlap)\nsplitter_small = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    chunk_overlap=50\n)\n\n# Strategy 2: Medium chunks (1000 chars, 200 overlap)\nsplitter_medium = # TODO\n\n# Strategy 3: Large chunks (2000 chars, 400 overlap)\nsplitter_large = # TODO\n\n# TODO: Split documents with each strategy\n# chunks_small = splitter_small.split_documents(documents)\n# chunks_medium = # TODO\n# chunks_large = # TODO\n\n# TODO: Create embeddings\nembeddings = OpenAIEmbeddings()\n\n# TODO: Create vector stores for each strategy\n# vectorstore_small = Chroma.from_documents(chunks_small, embeddings)\n# vectorstore_medium = # TODO\n# vectorstore_large = # TODO\n\n# TODO: Test retrieval with a query\nquery = \"What is the main topic?\"\n\n# TODO: Retrieve from each vector store and compare\n# results_small = vectorstore_small.similarity_search(query, k=3)\n# results_medium = # TODO\n# results_large = # TODO\n\n# TODO: Print and compare results\nprint(\"Small chunks:\")\n# for doc in results_small:\n#     print(doc.page_content[:200])\n\nprint(\"\\nMedium chunks:\")\n# TODO\n\nprint(\"\\nLarge chunks:\")\n# TODO",
      "solution_code": "from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI\n\n# Load a sample document (create sample.txt with some text)\n# For demo, we'll create sample text\nsample_text = \"\"\"\nArtificial Intelligence (AI) has revolutionized many industries.\nMachine learning algorithms can now process vast amounts of data.\nDeep learning neural networks have achieved remarkable results.\nNatural language processing enables computers to understand human language.\nComputer vision allows machines to interpret visual information.\nThese technologies are transforming healthcare, finance, and transportation.\n\"\"\"\n\n# Save to file for loader\nwith open(\"sample.txt\", \"w\") as f:\n    f.write(sample_text)\n\nloader = TextLoader(\"sample.txt\")\ndocuments = loader.load()\n\n# Strategy 1: Small chunks (200 chars, 50 overlap)\nsplitter_small = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    chunk_overlap=50\n)\n\n# Strategy 2: Medium chunks (1000 chars, 200 overlap)\nsplitter_medium = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\n\n# Strategy 3: Large chunks (2000 chars, 400 overlap)\nsplitter_large = RecursiveCharacterTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=400\n)\n\n# Split documents\nchunks_small = splitter_small.split_documents(documents)\nchunks_medium = splitter_medium.split_documents(documents)\nchunks_large = splitter_large.split_documents(documents)\n\nprint(f\"Small chunks: {len(chunks_small)}\")\nprint(f\"Medium chunks: {len(chunks_medium)}\")\nprint(f\"Large chunks: {len(chunks_large)}\")\n\n# Create embeddings\nembeddings = OpenAIEmbeddings()\n\n# Create vector stores\nvectorstore_small = Chroma.from_documents(chunks_small, embeddings)\nvectorstore_medium = Chroma.from_documents(chunks_medium, embeddings)\nvectorstore_large = Chroma.from_documents(chunks_large, embeddings)\n\n# Test retrieval\nquery = \"What technologies are mentioned?\"\n\nresults_small = vectorstore_small.similarity_search(query, k=3)\nresults_medium = vectorstore_medium.similarity_search(query, k=3)\nresults_large = vectorstore_large.similarity_search(query, k=3)\n\nprint(\"\\n=== Small chunks results ===\")\nfor i, doc in enumerate(results_small, 1):\n    print(f\"\\nChunk {i}:\")\n    print(doc.page_content[:200])\n\nprint(\"\\n=== Medium chunks results ===\")\nfor i, doc in enumerate(results_medium, 1):\n    print(f\"\\nChunk {i}:\")\n    print(doc.page_content[:200])\n\nprint(\"\\n=== Large chunks results ===\")\nfor i, doc in enumerate(results_large, 1):\n    print(f\"\\nChunk {i}:\")\n    print(doc.page_content[:200])\n\n# Analysis\nprint(\"\\n=== Analysis ===\")\nprint(\"Small chunks: More granular, may miss context\")\nprint(\"Medium chunks: Balanced context and granularity\")\nprint(\"Large chunks: More context, may include irrelevant info\")",
      "hints": [
        "Use RecursiveCharacterTextSplitter for text documents",
        "Chunk size is in characters, not tokens",
        "Overlap prevents losing context at boundaries",
        "Test with real queries to see quality differences",
        "Consider document type when choosing chunk size"
      ],
      "verification": "Successfully creates and compares three different chunking strategies",
      "common_mistakes": [
        "Confusing chunk size (characters) with token count",
        "Too much overlap wastes tokens",
        "Not testing retrieval quality",
        "Using same chunk size for all document types",
        "Not considering document structure"
      ]
    }
  ],
  "challenges": [
    {
      "challengeId": "conversational_rag",
      "name": "Conversational RAG System",
      "description": "Build a RAG system with conversation memory and advanced retrieval",
      "requirements": [
        "Load and process documents (at least 3 documents)",
        "Implement appropriate chunking strategy",
        "Create vector store and index documents",
        "Add conversation memory for context",
        "Implement query rewriting for follow-up questions",
        "Use MMR retrieval for diverse results",
        "Add source citations to responses",
        "Evaluate system with RAGAS metrics"
      ],
      "evaluation_criteria": [
        "Successfully loads and indexes documents",
        "Retrieves relevant chunks for questions",
        "Handles follow-up questions with context",
        "Provides source citations",
        "RAGAS metrics show good performance",
        "System handles edge cases gracefully"
      ],
      "stretch_goals": [
        "Implement hybrid search (vector + keyword)",
        "Add multi-query retrieval",
        "Implement reranking",
        "Add streaming responses",
        "Create web interface"
      ]
    }
  ],
  "resources": {
    "official_docs": [
      "https://python.langchain.com/docs/use_cases/question_answering/",
      "https://python.langchain.com/docs/integrations/vectorstores/",
      "https://docs.ragas.io/"
    ],
    "tutorials": [
      "LangChain RAG Tutorial",
      "RAGAS Evaluation Guide"
    ],
    "community": [
      "LangChain Discord",
      "RAGAS GitHub Discussions"
    ]
  },
  "assessment": {
    "knowledge_check": [
      {
        "question": "What are the key components of a RAG system?",
        "type": "multiple_choice",
        "answer": "Document loader, text splitter, embeddings, vector store, retriever, and LLM",
        "explanation": "Each component handles a specific part of the retrieval and generation pipeline"
      },
      {
        "question": "What is MMR and when should you use it?",
        "type": "short_answer",
        "answer": "MMR (Maximum Marginal Relevance) balances relevance and diversity in retrieval. Use it when you want diverse results rather than just the most similar chunks, which helps avoid redundant information.",
        "explanation": "MMR prevents retrieving multiple very similar chunks"
      },
      {
        "question": "What factors should you consider when choosing chunk size?",
        "type": "short_answer",
        "answer": "Consider document type (code vs prose), model context window, retrieval granularity needs, and semantic coherence. Smaller chunks provide more precise retrieval but may lose context. Larger chunks preserve context but may include irrelevant information.",
        "explanation": "There's no one-size-fits-all chunk size"
      }
    ],
    "practical_assessment": "Build a working RAG system that can answer questions about documents with conversation memory",
    "self_assessment": [
      "Can I implement document loading and chunking?",
      "Do I understand how to choose embedding models?",
      "Can I create and query vector stores?",
      "Do I know when to use different retrieval strategies?",
      "Can I evaluate RAG systems with RAGAS?"
    ]
  },
  "next_steps": {
    "next_workshop": "L9_advanced_rag",
    "practice_projects": [
      "Documentation Q&A system",
      "Research assistant with web search",
      "Codebase knowledge base"
    ],
    "deeper_learning": [
      "Advanced retrieval techniques (reranking, multi-query)",
      "Hybrid search implementation",
      "Production RAG optimization"
    ]
  },
  "axiom_zero_integration": {
    "love_moments": [
      "Celebrating successful document retrieval",
      "Encouraging experimentation with chunking strategies",
      "Patient debugging of retrieval quality issues"
    ],
    "truth_moments": [
      "Honest discussion of RAG limitations (hallucination, context limits)",
      "Clear explanation of retrieval vs generation trade-offs",
      "Acknowledging when simpler search works better"
    ],
    "beauty_moments": [
      "Elegant RAG pipeline compositions",
      "The satisfaction of seeing relevant chunks retrieved",
      "Clean, well-structured retrieval chains"
    ]
  }
}
