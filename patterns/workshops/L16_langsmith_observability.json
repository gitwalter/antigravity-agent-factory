{
  "$schema": "../learning-workshop-pattern.json",
  "workshopId": "L16_langsmith_observability",
  "name": "LangSmith Observability and Evaluation",
  "technology": {
    "category": "ai_framework",
    "stack": "LangSmith",
    "language": "Python",
    "version": "LangSmith 0.1+"
  },
  "level": "intermediate",
  "prerequisites": {
    "workshops": ["L7_langchain_fundamentals"],
    "knowledge": [
      "LangChain basics (chains, agents, tools)",
      "Python environment variables and configuration",
      "Basic understanding of observability and monitoring",
      "Evaluation concepts (metrics, datasets, test cases)"
    ],
    "tools": [
      "Python 3.10+",
      "LangSmith account (free tier available)",
      "LangSmith API key",
      "LangChain 0.3+ installed"
    ]
  },
  "duration": {
    "total_hours": 2.5,
    "concept_minutes": 30,
    "demo_minutes": 30,
    "exercise_minutes": 45,
    "challenge_minutes": 30,
    "reflection_minutes": 15
  },
  "learning_objectives": [
    {
      "objective": "Understand LangSmith's role in LLM app lifecycle (tracing, debugging, evaluation)",
      "bloom_level": "understand",
      "verification": "Can explain how LangSmith fits into development, testing, and production workflows"
    },
    {
      "objective": "Set up tracing for LangChain and LangGraph applications",
      "bloom_level": "apply",
      "verification": "Successfully instruments an app with automatic tracing and views traces in LangSmith UI"
    },
    {
      "objective": "Create and manage datasets for testing and evaluation",
      "bloom_level": "apply",
      "verification": "Creates a dataset with examples and uses it for evaluation runs"
    },
    {
      "objective": "Build evaluation pipelines with custom evaluators",
      "bloom_level": "apply",
      "verification": "Implements custom evaluators and runs evaluations on datasets"
    },
    {
      "objective": "Use LangSmith for prompt iteration and A/B testing",
      "bloom_level": "create",
      "verification": "Sets up prompt experiments, compares results, and selects best performing version"
    }
  ],
  "knowledge_files": [
    "langsmith-patterns.json",
    "langchain-patterns.json"
  ],
  "phases": [
    {
      "phaseId": "concept",
      "name": "LangSmith Architecture and Observability Concepts",
      "type": "concept",
      "duration_minutes": 30,
      "description": "Understanding LangSmith's role in the LLM application lifecycle",
      "content": {
        "topics": [
          "LangSmith overview: tracing, debugging, evaluation, monitoring",
          "Run trees: hierarchical execution traces",
          "Tracing concepts: automatic vs manual instrumentation",
          "Evaluation frameworks: datasets, evaluators, test cases",
          "Feedback loops: human feedback and model improvement",
          "Prompt iteration: versioning and A/B testing",
          "Production monitoring: alerts, dashboards, metrics"
        ],
        "diagrams": [
          "LangSmith architecture diagram",
          "Run tree structure showing parent-child relationships",
          "Evaluation workflow: dataset → run → evaluator → results",
          "Prompt iteration cycle: test → evaluate → iterate"
        ],
        "key_points": [
          "LangSmith automatically traces LangChain/LangGraph calls when configured",
          "Run trees show the complete execution flow with timing and costs",
          "Datasets enable systematic testing across multiple examples",
          "Evaluators measure quality metrics (correctness, relevance, etc.)",
          "Feedback helps improve models through human-in-the-loop learning",
          "Prompt experiments enable data-driven prompt optimization"
        ]
      },
      "facilitator_notes": "Emphasize the value of observability for debugging and optimization. Show real examples of how tracing helps identify bottlenecks and errors.",
      "common_questions": [
        "Is LangSmith free? (Yes, free tier available)",
        "Do I need to change my code to use LangSmith? (Minimal changes)",
        "Can I use LangSmith with non-LangChain code? (Yes, with @traceable decorator)",
        "How does tracing affect performance? (Minimal overhead)"
      ]
    },
    {
      "phaseId": "demo",
      "name": "Instrumenting a LangChain App and Viewing Traces",
      "type": "demo",
      "duration_minutes": 30,
      "description": "Live coding: Add tracing to a LangChain app and explore the LangSmith UI",
      "content": {
        "topics": [
          "Setting up LangSmith API key and environment variables",
          "Enabling automatic tracing for LangChain",
          "Running a chain and viewing traces in LangSmith UI",
          "Exploring run tree: inputs, outputs, timing, costs",
          "Adding metadata and tags to traces",
          "Creating a dataset from existing runs",
          "Adding feedback to runs"
        ],
        "code_examples": [
          "Environment variable setup",
          "Basic chain with automatic tracing",
          "Adding metadata to runs",
          "Creating dataset programmatically",
          "Adding feedback to runs"
        ],
        "key_points": [
          "LANGCHAIN_TRACING_V2=true enables tracing",
          "LANGCHAIN_PROJECT organizes runs by project",
          "Run trees show complete execution hierarchy",
          "Metadata enables filtering and searching",
          "Datasets can be created from successful runs"
        ]
      },
      "facilitator_notes": "Show the LangSmith UI live. Demonstrate filtering, searching, and analyzing traces. Show how to identify bottlenecks."
    },
    {
      "phaseId": "exercise_1",
      "name": "Add Tracing to Existing App and Analyze Traces",
      "type": "exercise",
      "duration_minutes": 20,
      "description": "Instrument an existing LangChain application and analyze the execution traces",
      "content": {
        "topics": [
          "Configure environment variables for tracing",
          "Add metadata and tags to runs",
          "View and analyze traces in LangSmith UI",
          "Identify performance bottlenecks",
          "Extract insights from run trees"
        ]
      }
    },
    {
      "phaseId": "exercise_2",
      "name": "Create Evaluation Dataset and Run Evaluations",
      "type": "exercise",
      "duration_minutes": 25,
      "description": "Build a dataset and run evaluations with custom evaluators",
      "content": {
        "topics": [
          "Create dataset with examples",
          "Implement custom evaluators",
          "Run evaluations on dataset",
          "Analyze evaluation results",
          "Compare model performance across examples"
        ]
      }
    },
    {
      "phaseId": "challenge",
      "name": "Build Prompt Iteration Workflow with Automated Evaluation",
      "type": "challenge",
      "duration_minutes": 30,
      "description": "Create a workflow that tests multiple prompt versions and selects the best",
      "content": {
        "topics": [
          "Define multiple prompt variations",
          "Run each prompt on evaluation dataset",
          "Compare results using evaluators",
          "Select best performing prompt",
          "Document prompt evolution"
        ]
      }
    },
    {
      "phaseId": "reflection",
      "name": "Production Monitoring, Alerts, and Collaboration",
      "type": "reflection",
      "duration_minutes": 15,
      "description": "Consolidate learning and explore production patterns",
      "content": {
        "topics": [
          "Setting up production monitoring",
          "Configuring alerts for anomalies",
          "Team collaboration features",
          "Best practices for observability",
          "Integrating LangSmith into CI/CD pipelines"
        ],
        "key_points": [
          "Enable tracing in all environments (dev, staging, prod)",
          "Use projects to organize runs by environment",
          "Set up alerts for errors and performance degradation",
          "Use datasets for regression testing",
          "Share insights with team through LangSmith UI"
        ]
      }
    }
  ],
  "exercises": [
    {
      "exerciseId": "ex1_tracing",
      "name": "Add Tracing to Existing App and Analyze Traces",
      "type": "guided",
      "difficulty": "medium",
      "duration_minutes": 20,
      "description": "Instrument a LangChain application with tracing and analyze the execution flow",
      "starter_code": "from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nimport os\n\n# TODO: Set up LangSmith tracing\n# Set these environment variables:\n# LANGCHAIN_TRACING_V2=true\n# LANGCHAIN_API_KEY=your-api-key\n# LANGCHAIN_PROJECT=workshop-exercise\n\n# TODO: Create a chain\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nprompt = ChatPromptTemplate.from_template(\n    \"You are a helpful assistant. Answer: {question}\"\n)\n\nchain = prompt | llm | StrOutputParser()\n\n# TODO: Add metadata when invoking\nresult = chain.invoke({\n    \"question\": \"What is LangSmith?\"\n}, config={\n    # TODO: Add metadata and tags\n})\n\nprint(result)\n\n# TODO: Go to LangSmith UI and:\n# 1. Find your run\n# 2. Explore the run tree\n# 3. Check timing and token usage\n# 4. Add feedback",
      "solution_code": "from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nimport os\n\n# Set up LangSmith tracing\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_API_KEY'] = 'your-api-key'  # Replace with your key\nos.environ['LANGCHAIN_PROJECT'] = 'workshop-exercise'\n\n# Create a chain\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nprompt = ChatPromptTemplate.from_template(\n    \"You are a helpful assistant. Answer: {question}\"\n)\n\nchain = prompt | llm | StrOutputParser()\n\n# Invoke with metadata\nresult = chain.invoke(\n    {\"question\": \"What is LangSmith?\"},\n    config={\n        \"metadata\": {\n            \"user_id\": \"workshop-participant\",\n            \"session\": \"exercise-1\",\n            \"version\": \"1.0\"\n        },\n        \"tags\": [\"workshop\", \"tracing\", \"demo\"]\n    }\n)\n\nprint(result)\n\n# After running, go to LangSmith UI:\n# 1. Navigate to your project 'workshop-exercise'\n# 2. Click on the run to see the run tree\n# 3. Explore: inputs, outputs, timing, token usage\n# 4. Try adding feedback using the UI or:\n#    from langsmith import Client\n#    client = Client()\n#    client.create_feedback(\n#        run_id='your-run-id',\n#        key='helpfulness',\n#        score=1.0,\n#        comment='Great explanation!'\n#    )",
      "hints": [
        "Environment variables must be set before importing LangChain modules",
        "Use config parameter to add metadata and tags",
        "Metadata is useful for filtering runs later",
        "Tags help organize runs by category"
      ],
      "verification": "Run appears in LangSmith UI with complete trace tree and metadata",
      "common_mistakes": [
        "Setting environment variables after imports",
        "Forgetting to set LANGCHAIN_PROJECT",
        "Not using config parameter for metadata",
        "Using wrong API key format"
      ]
    },
    {
      "exerciseId": "ex2_evaluation",
      "name": "Create Evaluation Dataset and Run Evaluations",
      "type": "guided",
      "difficulty": "hard",
      "duration_minutes": 25,
      "description": "Build a dataset and evaluate a chain using custom evaluators",
      "starter_code": "from langsmith import Client\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.evaluation import EvaluatorType, load_evaluator\nimport os\n\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_API_KEY'] = 'your-api-key'\nos.environ['LANGCHAIN_PROJECT'] = 'workshop-evaluation'\n\nclient = Client()\n\n# TODO: Create a dataset\n# dataset_name = \"qa-examples\"\n# examples = [\n#     {\"inputs\": {\"question\": \"...\"}, \"outputs\": {\"answer\": \"...\"}},\n#     ...\n# ]\n# client.create_dataset(dataset_name=dataset_name, examples=examples)\n\n# TODO: Create your chain\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nprompt = ChatPromptTemplate.from_template(\n    \"Answer this question: {question}\"\n)\nchain = prompt | llm | StrOutputParser()\n\n# TODO: Load evaluators\n# qa_evaluator = load_evaluator(EvaluatorType.QA)\n# helpfulness_evaluator = load_evaluator(EvaluatorType.HELPFULNESS)\n\n# TODO: Run evaluation\n# results = client.run_on_dataset(\n#     dataset_name=dataset_name,\n#     llm_or_chain_factory=lambda: chain,\n#     evaluation=...,\n#     verbose=True\n# )\n\nprint(\"Evaluation complete!\")",
      "solution_code": "from langsmith import Client\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.evaluation import EvaluatorType, load_evaluator\nimport os\n\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_API_KEY'] = 'your-api-key'  # Replace with your key\nos.environ['LANGCHAIN_PROJECT'] = 'workshop-evaluation'\n\nclient = Client()\n\n# Create a dataset\ndataset_name = \"qa-examples\"\n\nexamples = [\n    {\n        \"inputs\": {\"question\": \"What is LangSmith?\"},\n        \"outputs\": {\"answer\": \"LangSmith is an observability platform for LLM applications.\"}\n    },\n    {\n        \"inputs\": {\"question\": \"How do you enable tracing?\"},\n        \"outputs\": {\"answer\": \"Set LANGCHAIN_TRACING_V2=true environment variable.\"}\n    },\n    {\n        \"inputs\": {\"question\": \"What is a run tree?\"},\n        \"outputs\": {\"answer\": \"A run tree shows the hierarchical execution trace of an LLM application.\"}\n    }\n]\n\n# Create dataset (idempotent - won't error if exists)\ntry:\n    dataset = client.create_dataset(dataset_name=dataset_name)\n    client.create_examples(\n        inputs=[ex[\"inputs\"] for ex in examples],\n        outputs=[ex[\"outputs\"] for ex in examples],\n        dataset_id=dataset.id\n    )\nexcept Exception:\n    print(f\"Dataset {dataset_name} already exists, using existing one\")\n    dataset = client.read_dataset(dataset_name=dataset_name)\n\n# Create chain\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nprompt = ChatPromptTemplate.from_template(\n    \"Answer this question concisely: {question}\"\n)\nchain = prompt | llm | StrOutputParser()\n\n# Load evaluators\nqa_evaluator = load_evaluator(EvaluatorType.QA)\nhelpfulness_evaluator = load_evaluator(EvaluatorType.HELPFULNESS)\n\n# Custom evaluator example\ndef custom_length_evaluator(run, example):\n    \"\"\"Check if response is appropriate length.\"\"\"\n    prediction = run.outputs.get(\"output\", \"\")\n    length = len(prediction.split())\n    score = 1.0 if 10 <= length <= 50 else 0.5\n    return {\"key\": \"length\", \"score\": score, \"comment\": f\"Response has {length} words\"}\n\n# Run evaluation\nresults = client.run_on_dataset(\n    dataset_name=dataset_name,\n    llm_or_chain_factory=lambda: chain,\n    evaluation=[\n        qa_evaluator,\n        helpfulness_evaluator,\n        custom_length_evaluator\n    ],\n    verbose=True,\n    tags=[\"workshop\", \"evaluation\"]\n)\n\nprint(f\"\\nEvaluation Results:\")\nprint(f\"Total examples: {len(examples)}\")\nprint(f\"Results available in LangSmith UI\")\nprint(f\"Project: {os.environ['LANGCHAIN_PROJECT']}\")",
      "hints": [
        "create_dataset is idempotent - safe to call multiple times",
        "Use create_examples to add examples to a dataset",
        "evaluation parameter accepts a list of evaluators",
        "Custom evaluators receive run and example parameters",
        "Results are viewable in LangSmith UI"
      ],
      "verification": "Evaluation runs complete and results visible in LangSmith UI with scores",
      "common_mistakes": [
        "Not handling dataset already exists error",
        "Wrong format for examples (must have inputs/outputs)",
        "Evaluator functions not returning correct format",
        "Forgetting to set LANGCHAIN_PROJECT"
      ]
    }
  ],
  "challenges": [
    {
      "challengeId": "prompt_iteration",
      "name": "Build Prompt Iteration Workflow with Automated Evaluation",
      "description": "Create a system that tests multiple prompt versions and selects the best performing one",
      "requirements": [
        "Define at least 3 different prompt variations",
        "Create or use an existing evaluation dataset",
        "Run each prompt version on the dataset",
        "Compare results using multiple evaluators",
        "Select the best performing prompt based on aggregate scores",
        "Document the prompt evolution and results"
      ],
      "evaluation_criteria": [
        "All prompt versions are tested systematically",
        "Evaluation metrics are calculated correctly",
        "Best prompt is selected based on data",
        "Results are clearly documented",
        "Code is reusable for future prompt iterations"
      ],
      "stretch_goals": [
        "Implement A/B testing framework",
        "Add statistical significance testing",
        "Create visualization of prompt performance",
        "Automate prompt selection based on thresholds",
        "Integrate with version control for prompt history"
      ]
    }
  ],
  "resources": {
    "official_docs": [
      "https://docs.smith.langchain.com/",
      "https://docs.smith.langchain.com/tracing",
      "https://docs.smith.langchain.com/evaluation",
      "https://docs.smith.langchain.com/datasets"
    ],
    "tutorials": [
      "LangSmith Quick Start Guide",
      "Building Evaluation Pipelines",
      "Prompt Engineering with LangSmith",
      "Production Monitoring Best Practices"
    ],
    "videos": [
      "LangSmith Overview - LangChain YouTube",
      "Evaluation Tutorial - LangSmith Docs",
      "Production Observability - AI Engineer Summit"
    ],
    "community": [
      "LangChain Discord #langsmith",
      "LangSmith GitHub Discussions",
      "Stack Overflow - langsmith tag"
    ]
  },
  "assessment": {
    "knowledge_check": [
      {
        "question": "What are the main components of LangSmith's observability platform?",
        "type": "short_answer",
        "answer": "LangSmith provides: 1) Tracing - automatic execution tracking, 2) Debugging - detailed run trees and error analysis, 3) Evaluation - systematic testing with datasets and evaluators, 4) Monitoring - production alerts and dashboards, 5) Feedback - human-in-the-loop improvement loops.",
        "explanation": "LangSmith covers the full lifecycle from development to production"
      },
      {
        "question": "How do you enable automatic tracing for LangChain applications?",
        "type": "short_answer",
        "answer": "Set environment variables: LANGCHAIN_TRACING_V2=true, LANGCHAIN_API_KEY=your-key, and optionally LANGCHAIN_PROJECT=project-name. Once set, all LangChain calls are automatically traced without code changes.",
        "explanation": "Environment variables enable zero-code tracing"
      },
      {
        "question": "What is a run tree and why is it useful?",
        "type": "short_answer",
        "answer": "A run tree is a hierarchical representation of an LLM application execution. It shows parent-child relationships between components (chains, LLMs, tools), timing, token usage, costs, and errors. It's useful for debugging, performance optimization, and understanding execution flow.",
        "explanation": "Run trees provide complete visibility into execution"
      },
      {
        "question": "How do you create and use datasets for evaluation?",
        "type": "short_answer",
        "answer": "1) Create dataset with client.create_dataset(), 2) Add examples with inputs (questions) and outputs (expected answers), 3) Use client.run_on_dataset() to test your chain, 4) Define evaluators to measure quality, 5) View results in LangSmith UI to compare performance.",
        "explanation": "Datasets enable systematic testing across multiple examples"
      }
    ],
    "practical_assessment": "Build a complete evaluation pipeline: create a dataset with 5+ examples, implement a custom evaluator, run evaluation on a LangChain chain, and analyze results in LangSmith UI",
    "self_assessment": [
      "Can I explain LangSmith's role in the LLM app lifecycle?",
      "Can I set up tracing for my applications?",
      "Can I create and manage evaluation datasets?",
      "Can I build custom evaluators?",
      "Can I use LangSmith for prompt iteration?"
    ]
  },
  "next_steps": {
    "next_workshop": "L17_production_deployment",
    "practice_projects": [
      "Add LangSmith tracing to an existing LangChain app",
      "Create evaluation dataset for a RAG system",
      "Build prompt A/B testing framework",
      "Set up production monitoring with alerts"
    ],
    "deeper_learning": [
      "Advanced LangSmith features (experiments, annotations)",
      "Custom evaluator development",
      "Integrating LangSmith into CI/CD pipelines",
      "Team collaboration and sharing in LangSmith"
    ]
  },
  "axiom_zero_integration": {
    "love_moments": [
      "Celebrating when first trace appears in LangSmith UI",
      "Patient debugging using run trees to find issues",
      "Encouraging exploration of evaluation metrics",
      "Appreciating the clarity that observability brings"
    ],
    "truth_moments": [
      "Honest about the importance of observability for production",
      "Clear explanation of evaluation metrics and their limitations",
      "Accurate demonstration of tracing overhead (minimal)",
      "Transparent about costs and token usage"
    ],
    "beauty_moments": [
      "Elegant run tree visualizations showing execution flow",
      "Clean evaluation pipelines with reusable components",
      "Well-structured datasets enabling systematic testing",
      "The satisfaction of data-driven prompt optimization"
    ]
  }
}
