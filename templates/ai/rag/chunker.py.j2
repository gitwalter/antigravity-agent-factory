"""
{{ chunker_name | default('Document Chunker') }} - Document Chunking Template

Purpose: {{ chunker_purpose | default('Document chunking with recursive splitting and semantic chunking strategies') }}
Author: {{ author | default('Cursor Agent Factory') }}
Date: {{ date | default('2026-02-08') }}

Axiom Alignment:
- A1 (Verifiability): Chunks maintain source attribution
- A3 (Transparency): Chunking strategy is explicit and configurable
- A2 (Correctness): Semantic chunking preserves meaning boundaries

This chunker provides:
- Recursive character text splitting
- Semantic chunking based on embeddings
- Parent-child chunking for hierarchical retrieval
- Configurable chunk sizes and overlap
"""

from typing import List, Optional, Dict, Any
from langchain_core.documents import Document
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    TokenTextSplitter
)
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings
from pydantic import BaseModel, Field
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChunkingConfig(BaseModel):
    """Configuration for chunking strategies."""
    chunk_size: int = Field(default={{ chunk_size | default(1000) }}, ge=1, description="Target chunk size")
    chunk_overlap: int = Field(default={{ chunk_overlap | default(200) }}, ge=0, description="Overlap between chunks")
    separators: List[str] = Field(
        default_factory=lambda: ["\n\n", "\n", ". ", " ", ""],
        description="Separators for recursive splitting"
    )
    length_function: Any = Field(default=len, description="Function to measure text length")


class ChunkingResult(BaseModel):
    """Result from chunking operation."""
    chunks: List[Document] = Field(description="List of chunked documents")
    num_chunks: int = Field(description="Total number of chunks created")
    strategy: str = Field(description="Chunking strategy used")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")


class {{ chunker_class_name | default('DocumentChunker') }}:
    """
    {{ chunker_name | default('Document Chunker') }} - Multiple Chunking Strategies
    
    Provides various chunking approaches for different document types and use cases.
    
    Strategies:
    1. Recursive Character: General purpose, hierarchical splitting
    2. Semantic: Split based on semantic similarity (requires embeddings)
    3. Parent-Child: Store small chunks, retrieve larger parents
    4. Fixed-Size: Simple fixed-size chunks
    
    Example:
        >>> chunker = {{ chunker_class_name | default('DocumentChunker') }}()
        >>> docs = [Document(page_content="Long document...")]
        >>> result = chunker.recursive_character_split(docs)
        >>> print(f"Created {result.num_chunks} chunks")
    """
    
    def __init__(
        self,
        config: Optional[ChunkingConfig] = None,
        embedding_model: str = "{{ embedding_model | default('text-embedding-ada-002') }}"
    ):
        """
        Initialize chunking strategies.
        
        Args:
            config: Chunking configuration (uses defaults if None)
            embedding_model: Embedding model for semantic chunking
        """
        self.config = config or ChunkingConfig()
        self.embedding_model = embedding_model
        self.embeddings: Optional[OpenAIEmbeddings] = None
        
        logger.info(
            f"Initialized {{ chunker_class_name | default('DocumentChunker') }} "
            f"with chunk_size={self.config.chunk_size}"
        )
    
    def _get_embeddings(self) -> OpenAIEmbeddings:
        """Get or create embeddings instance."""
        if self.embeddings is None:
            self.embeddings = OpenAIEmbeddings(model=self.embedding_model)
        return self.embeddings
    
    def recursive_character_split(
        self,
        documents: List[Document],
        **kwargs
    ) -> ChunkingResult:
        """
        Split documents using recursive character text splitter.
        
        This is the most common strategy - splits by trying separators in order
        (paragraphs, sentences, words) until chunks are the right size.
        
        Args:
            documents: Documents to chunk
            **kwargs: Additional arguments for RecursiveCharacterTextSplitter
            
        Returns:
            Chunking result with chunks and metadata
            
        Example:
            >>> result = chunker.recursive_character_split(documents)
            >>> print(f"Created {result.num_chunks} chunks")
        """
        logger.info(f"Recursive character splitting {len(documents)} documents...")
        
        # Merge config and kwargs
        splitter_kwargs = {
            "chunk_size": self.config.chunk_size,
            "chunk_overlap": self.config.chunk_overlap,
            "length_function": self.config.length_function,
            "separators": self.config.separators,
            **kwargs
        }
        
        text_splitter = RecursiveCharacterTextSplitter(**splitter_kwargs)
        
        # Split documents
        chunks = text_splitter.split_documents(documents)
        
        # Preserve source metadata (A1 - Verifiability)
        for i, chunk in enumerate(chunks):
            if "chunk_index" not in chunk.metadata:
                chunk.metadata["chunk_index"] = i
            if "chunking_strategy" not in chunk.metadata:
                chunk.metadata["chunking_strategy"] = "recursive_character"
        
        logger.info(f"Created {len(chunks)} chunks using recursive character splitting")
        
        return ChunkingResult(
            chunks=chunks,
            num_chunks=len(chunks),
            strategy="recursive_character",
            metadata={
                "chunk_size": self.config.chunk_size,
                "chunk_overlap": self.config.chunk_overlap
            }
        )
    
    def semantic_split(
        self,
        documents: List[Document],
        breakpoint_threshold_type: str = "{{ breakpoint_threshold_type | default('percentile') }}",
        **kwargs
    ) -> ChunkingResult:
        """
        Split documents using semantic chunking.
        
        Splits based on semantic similarity - groups similar sentences together
        and splits at semantic boundaries. Requires embeddings.
        
        Args:
            documents: Documents to chunk
            breakpoint_threshold_type: Threshold type ("percentile", "standard_deviation", "interquartile")
            **kwargs: Additional arguments for SemanticChunker
            
        Returns:
            Chunking result with chunks and metadata
            
        Example:
            >>> result = chunker.semantic_split(documents)
            >>> print(f"Created {result.num_chunks} semantic chunks")
        """
        logger.info(f"Semantic splitting {len(documents)} documents...")
        
        try:
            embeddings = self._get_embeddings()
            
            # Merge config and kwargs
            splitter_kwargs = {
                "embeddings": embeddings,
                "breakpoint_threshold_type": breakpoint_threshold_type,
                **kwargs
            }
            
            text_splitter = SemanticChunker(**splitter_kwargs)
            
            # Split documents
            chunks = text_splitter.split_documents(documents)
            
            # Preserve source metadata (A1 - Verifiability)
            for i, chunk in enumerate(chunks):
                if "chunk_index" not in chunk.metadata:
                    chunk.metadata["chunk_index"] = i
                if "chunking_strategy" not in chunk.metadata:
                    chunk.metadata["chunking_strategy"] = "semantic"
            
            logger.info(f"Created {len(chunks)} chunks using semantic splitting")
            
            return ChunkingResult(
                chunks=chunks,
                num_chunks=len(chunks),
                strategy="semantic",
                metadata={
                    "breakpoint_threshold_type": breakpoint_threshold_type
                }
            )
            
        except Exception as e:
            logger.error(f"Error in semantic splitting: {e}")
            # Fallback to recursive character splitting
            logger.warning("Falling back to recursive character splitting")
            return self.recursive_character_split(documents, **kwargs)
    
    def parent_child_split(
        self,
        documents: List[Document],
        parent_chunk_size: Optional[int] = None,
        child_chunk_size: Optional[int] = None,
        **kwargs
    ) -> ChunkingResult:
        """
        Split documents using parent-child chunking.
        
        Creates small child chunks for retrieval, but maintains larger parent
        chunks for context. Useful for maintaining context while enabling
        precise retrieval.
        
        Args:
            documents: Documents to chunk
            parent_chunk_size: Size of parent chunks (defaults to chunk_size * 2)
            child_chunk_size: Size of child chunks (defaults to chunk_size / 2)
            **kwargs: Additional arguments
            
        Returns:
            Chunking result with chunks and metadata
        """
        logger.info(f"Parent-child splitting {len(documents)} documents...")
        
        parent_size = parent_chunk_size or (self.config.chunk_size * 2)
        child_size = child_chunk_size or (self.config.chunk_size // 2)
        
        # Create parent chunks
        parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=parent_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=self.config.separators
        )
        parent_chunks = parent_splitter.split_documents(documents)
        
        # Create child chunks from parents
        child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=child_size,
            chunk_overlap=self.config.chunk_overlap // 2,
            separators=self.config.separators
        )
        child_chunks = child_splitter.split_documents(parent_chunks)
        
        # Add parent references to child metadata (A1 - Verifiability)
        parent_idx = 0
        for child in child_chunks:
            # Determine which parent this child belongs to
            if "parent_id" not in child.metadata:
                child.metadata["parent_id"] = parent_idx
                child.metadata["parent_chunk_size"] = parent_size
                child.metadata["chunking_strategy"] = "parent_child"
            
            # Update parent index when we've processed enough children
            if len(child.page_content) >= child_size:
                parent_idx += 1
        
        logger.info(
            f"Created {len(child_chunks)} child chunks from {len(parent_chunks)} parent chunks"
        )
        
        return ChunkingResult(
            chunks=child_chunks,
            num_chunks=len(child_chunks),
            strategy="parent_child",
            metadata={
                "parent_chunks": len(parent_chunks),
                "child_chunks": len(child_chunks),
                "parent_chunk_size": parent_size,
                "child_chunk_size": child_size
            }
        )
    
    def fixed_size_split(
        self,
        documents: List[Document],
        **kwargs
    ) -> ChunkingResult:
        """
        Split documents using fixed-size character splitting.
        
        Simple strategy that splits at fixed character boundaries.
        Fast but may break sentences/paragraphs.
        
        Args:
            documents: Documents to chunk
            **kwargs: Additional arguments
            
        Returns:
            Chunking result with chunks and metadata
        """
        logger.info(f"Fixed-size splitting {len(documents)} documents...")
        
        text_splitter = CharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            length_function=self.config.length_function,
            **kwargs
        )
        
        chunks = text_splitter.split_documents(documents)
        
        # Preserve source metadata (A1 - Verifiability)
        for i, chunk in enumerate(chunks):
            if "chunk_index" not in chunk.metadata:
                chunk.metadata["chunk_index"] = i
            if "chunking_strategy" not in chunk.metadata:
                chunk.metadata["chunking_strategy"] = "fixed_size"
        
        logger.info(f"Created {len(chunks)} chunks using fixed-size splitting")
        
        return ChunkingResult(
            chunks=chunks,
            num_chunks=len(chunks),
            strategy="fixed_size",
            metadata={
                "chunk_size": self.config.chunk_size,
                "chunk_overlap": self.config.chunk_overlap
            }
        )
    
    def chunk(
        self,
        documents: List[Document],
        strategy: str = "{{ default_strategy | default('recursive_character') }}",
        **kwargs
    ) -> ChunkingResult:
        """
        Chunk documents using specified strategy.
        
        Args:
            documents: Documents to chunk
            strategy: Chunking strategy ("recursive_character", "semantic", "parent_child", "fixed_size")
            **kwargs: Strategy-specific arguments
            
        Returns:
            Chunking result
        """
        if strategy == "recursive_character":
            return self.recursive_character_split(documents, **kwargs)
        elif strategy == "semantic":
            return self.semantic_split(documents, **kwargs)
        elif strategy == "parent_child":
            return self.parent_child_split(documents, **kwargs)
        elif strategy == "fixed_size":
            return self.fixed_size_split(documents, **kwargs)
        else:
            raise ValueError(f"Unknown chunking strategy: {strategy}")


# Example usage
if __name__ == "__main__":
    from langchain_core.documents import Document
    
    # Create sample documents
    documents = [
        Document(
            page_content="""
            Machine learning is a subset of artificial intelligence that focuses on 
            algorithms and statistical models. These systems learn from data without 
            being explicitly programmed. Deep learning, a subset of machine learning, 
            uses neural networks with multiple layers to model and understand complex patterns.
            
            Natural language processing (NLP) is another important area of AI that 
            enables computers to understand, interpret, and generate human language. 
            Modern NLP systems use transformer architectures and large language models 
            to achieve state-of-the-art performance on various tasks.
            """,
            metadata={"source": "ai_intro.txt"}
        )
    ]
    
    # Create chunker
    chunker = {{ chunker_class_name | default('DocumentChunker') }}(
        config=ChunkingConfig(
            chunk_size={{ chunk_size | default(1000) }},
            chunk_overlap={{ chunk_overlap | default(200) }}
        )
    )
    
    # Test different strategies
    print("=== Recursive Character Splitting ===")
    result1 = chunker.recursive_character_split(documents)
    print(f"Created {result1.num_chunks} chunks")
    for i, chunk in enumerate(result1.chunks[:3]):
        print(f"\nChunk {i+1}: {chunk.page_content[:100]}...")
    
    print("\n=== Semantic Splitting ===")
    result2 = chunker.semantic_split(documents)
    print(f"Created {result2.num_chunks} chunks")
    for i, chunk in enumerate(result2.chunks[:3]):
        print(f"\nChunk {i+1}: {chunk.page_content[:100]}...")
    
    print("\n=== Parent-Child Splitting ===")
    result3 = chunker.parent_child_split(documents)
    print(f"Created {result3.num_chunks} child chunks from {result3.metadata['parent_chunks']} parent chunks")
    for i, chunk in enumerate(result3.chunks[:3]):
        print(f"\nChunk {i+1} (parent {chunk.metadata.get('parent_id', 'unknown')}): {chunk.page_content[:100]}...")
