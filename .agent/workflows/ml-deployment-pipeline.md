---
description: Comprehensive workflow for deploying machine learning models to production including model export, containerization, ...
---

# Ml Deployment Pipeline

Comprehensive workflow for deploying machine learning models to production including model export, containerization, serving, and monitoring.

**Version:** 1.0.0  
**Created:** 2026-02-09  
**Agent:** mlops-engineer

> **Note:** Directory paths referenced in this workflow ({directories.knowledge}/, {directories.skills}/, {directories.patterns}/, etc.) are configurable via `{directories.config}/settings.json`.

## Trigger Conditions

This workflow is activated when:

- User requests "deploy model", "model serving", "ML deployment"
- User mentions "model serving", "inference endpoint", "production model"
- User requests "deploy to production" or "model hosting"
- User asks to "serve ML model"

**Trigger Examples:**
- "Deploy PyTorch model to production"
- "Create inference endpoint for ML model"
- "Deploy model with vLLM"
- "Set up model serving on Kubernetes"

## Steps

### Export Model

### Optimize Model (Optional)

### Create Inference Server

### Create Dockerfile

### Build and Test Container

### Choose Deployment Platform

### Deploy Model Service

### Configure Endpoints

### Production Configuration

### Deploy to Production

### Set Up CI/CD

### Set Up Monitoring

### Configure Auto-Scaling

### Monitor Model Performance
