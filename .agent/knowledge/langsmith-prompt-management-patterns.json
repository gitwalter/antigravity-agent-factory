{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "langsmith-prompt-management-patterns",
  "name": "LangSmith Prompt Management Patterns",
  "title": "LangSmith Prompt Management Patterns",
  "description": "Patterns for managing prompts with LangSmith Hub including versioning, A/B testing, evaluation, and integration strategies",
  "version": "1.0.0",
  "category": "core",
  "axiomAlignment": {
    "A1_verifiability": "Versioned prompts and evaluation enable verifiable prompt behavior",
    "A2_user_primacy": "Prompt management serves user AI application goals",
    "A3_transparency": "Hub and tracing make prompt usage and changes explicit",
    "A4_non_harm": "Evaluation and A/B testing prevent harmful prompt deployment",
    "A5_consistency": "Unified LangSmith patterns across versioning, evaluation, and deployment"
  },
  "related_skills": [
    "langsmith-prompts",
    "langsmith-tracing",
    "langchain-usage",
    "prompt-optimization"
  ],
  "related_knowledge": [
    "prompt-engineering.json",
    "llm-evaluation-patterns.json",
    "langchain-patterns.json"
  ],
  "patterns": {
    "hub_integration": {
      "push_prompts": {
        "description": "Push prompts to LangSmith Hub for version control",
        "pattern": "Create prompt -> Push to Hub -> Get versioned prompt",
        "example": "from langchain import hub\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a {role}. {instructions}'),\n    ('human', '{input}')\n])\n\nhub.push('my-org/agent-prompt', prompt, new_repo_is_public=False)",
        "best_practices": [
          "Use semantic versioning",
          "Include clear descriptions",
          "Tag prompts by use case",
          "Set appropriate visibility"
        ],
        "use_when": "Apply when push prompts is needed in LangSmith Prompt Management Patterns",
        "code_example": "from langchain import hub\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a {role}. {instructions}'),\n    ('human', '{input}')\n])\n\nhub.push('my-org/agent-prompt', prompt, new_repo_is_public=False)"
      },
      "pull_prompts": {
        "description": "Pull prompts from Hub for use in chains",
        "pattern": "Pull prompt from Hub -> Use in chain",
        "example": "prompt = hub.pull('my-org/agent-prompt')\nprompt_v2 = hub.pull('my-org/agent-prompt:v2')\nchain = prompt | llm | parser",
        "use_when": "Production deployments; Consistent prompt usage; Team collaboration",
        "code_example": "prompt = hub.pull('my-org/agent-prompt')\nprompt_v2 = hub.pull('my-org/agent-prompt:v2')\nchain = prompt | llm | parser",
        "best_practices": [
          "Document the pattern usage and rationale in code comments",
          "Validate implementation against domain requirements before deployment"
        ]
      },
      "prompt_organization": {
        "description": "Organize prompts in Hub with naming conventions",
        "structure": {
          "agents": "prompts/agents/analyst-v1, researcher, etc.",
          "tools": "prompts/tools/sql-generator, api-caller, etc.",
          "evaluation": "prompts/evaluation/relevance-judge, quality-judge, etc."
        },
        "best_practices": [
          "Use consistent naming",
          "Group by domain",
          "Separate dev and prod",
          "Include version numbers"
        ],
        "use_when": "Apply when prompt organization is needed in LangSmith Prompt Management Patterns",
        "code_example": "# Implement prompt_organization per langsmith-prompts-patterns patterns\n# See description and related documentation"
      },
      "description": "Category grouping: push_prompts, pull_prompts, prompt_organization",
      "use_when": "Apply when hub integration patterns are needed in LangSmith Prompt Management Patterns",
      "code_example": "# See child patterns under hub_integration for implementation examples",
      "best_practices": [
        "Refer to specific child patterns for implementation details",
        "Apply the category pattern appropriate to your use case"
      ]
    },
    "versioning_strategies": {
      "semantic_versioning": {
        "description": "Use semantic versioning for prompts (v1, v2, etc.)",
        "pattern": "prompt-name:v1, prompt-name:v2, prompt-name:v3",
        "best_practices": [
          "Increment major version for breaking changes",
          "Increment minor for additions",
          "Use tags for variants"
        ],
        "use_when": "Apply when semantic versioning is needed in LangSmith Prompt Management Patterns",
        "code_example": "# Implement semantic_versioning per langsmith-prompts-patterns patterns\n# See description and related documentation"
      },
      "version_comparison": {
        "description": "Compare prompt versions before updating",
        "pattern": "Pull current version -> Compare -> Push new version",
        "example": "current = hub.pull('my-org/agent-prompt')\n# Compare with new prompt\nhub.push('my-org/agent-prompt', new_prompt)",
        "use_when": "Updating production prompts; Tracking changes; Review processes",
        "code_example": "current = hub.pull('my-org/agent-prompt')\n# Compare with new prompt\nhub.push('my-org/agent-prompt', new_prompt)",
        "best_practices": [
          "Document the pattern usage and rationale in code comments",
          "Validate implementation against domain requirements before deployment"
        ]
      },
      "environment_separation": {
        "description": "Separate prompts for dev, staging, and production",
        "pattern": "prompt-name-dev, prompt-name-staging, prompt-name-prod",
        "best_practices": [
          "Never share dev/prod prompts",
          "Test in staging first",
          "Use environment-specific naming"
        ],
        "use_when": "Apply when environment separation is needed in LangSmith Prompt Management Patterns",
        "code_example": "# Implement environment_separation per langsmith-prompts-patterns patterns\n# See description and related documentation"
      },
      "branching_strategy": {
        "description": "Use branches for experimental prompts",
        "pattern": "main branch for stable -> feature branches for experiments",
        "use_when": "Testing new approaches; Experimental features; Team collaboration",
        "code_example": "# Implement branching_strategy per langsmith-prompts-patterns patterns\n# See description and related documentation",
        "best_practices": [
          "Document the pattern usage and rationale in code comments",
          "Validate implementation against domain requirements before deployment"
        ]
      },
      "description": "Category grouping: semantic_versioning, version_comparison, environment_separation, branching_strategy",
      "use_when": "Apply when versioning strategies patterns are needed in LangSmith Prompt Management Patterns",
      "code_example": "# See child patterns under versioning_strategies for implementation examples",
      "best_practices": [
        "Refer to specific child patterns for implementation details",
        "Apply the category pattern appropriate to your use case"
      ]
    },
    "ab_testing": {
      "variant_selection": {
        "description": "Select prompt variants for A/B testing",
        "pattern": "Randomly select variant -> Execute -> Log results",
        "example": "import random\nfrom langsmith import traceable\n\n@traceable(tags=['ab-test'])\nasync def run_with_ab_test(input_data: dict, test_name: str = 'prompt_v1_vs_v2'):\n    variant = random.choice(['control', 'treatment'])\n    if variant == 'control':\n        prompt = hub.pull('my-org/agent-prompt:v1')\n    else:\n        prompt = hub.pull('my-org/agent-prompt:v2')\n    chain = prompt | llm | parser\n    result = await chain.ainvoke(input_data)\n    return {'result': result, 'variant': variant}",
        "best_practices": [
          "Use consistent test names",
          "Tag runs for analysis",
          "Ensure equal distribution",
          "Track metrics per variant"
        ],
        "use_when": "Apply when variant selection is needed in LangSmith Prompt Management Patterns",
        "code_example": "import random\nfrom langsmith import traceable\n\n@traceable(tags=['ab-test'])\nasync def run_with_ab_test(input_data: dict, test_name: str = 'prompt_v1_vs_v2'):\n    variant = random.choice(['control', 'treatment'])\n    if variant == 'control':\n        prompt = hub.pull('my-org/agent-prompt:v1')\n    else:\n        prompt = hub.pull('my-org/agent-prompt:v2')\n    chain = prompt | llm | parser\n    result = await chain.ainvoke(input_data)\n    return {'result': result, 'variant': variant}"
      },
      "metrics_tracking": {
        "description": "Track metrics for each variant",
        "metrics": [
          "Response quality",
          "User satisfaction",
          "Cost per request",
          "Latency",
          "Error rates"
        ],
        "use_when": "Comparing prompt versions; Optimizing prompts; Measuring improvements",
        "code_example": "# Implement metrics_tracking per langsmith-prompts-patterns patterns\n# See description and related documentation",
        "best_practices": [
          "Document the pattern usage and rationale in code comments",
          "Validate implementation against domain requirements before deployment"
        ]
      },
      "statistical_analysis": {
        "description": "Analyze A/B test results statistically",
        "best_practices": [
          "Ensure sufficient sample size",
          "Use appropriate statistical tests",
          "Consider multiple metrics",
          "Account for confounding factors"
        ],
        "use_when": "Apply when statistical analysis is needed in LangSmith Prompt Management Patterns",
        "code_example": "# Implement statistical_analysis per langsmith-prompts-patterns patterns\n# See description and related documentation"
      },
      "description": "Category grouping: variant_selection, metrics_tracking, statistical_analysis",
      "use_when": "Apply when ab testing patterns are needed in LangSmith Prompt Management Patterns",
      "code_example": "# See child patterns under ab_testing for implementation examples",
      "best_practices": [
        "Refer to specific child patterns for implementation details",
        "Apply the category pattern appropriate to your use case"
      ]
    },
    "evaluation_patterns": {
      "dataset_creation": {
        "description": "Create evaluation datasets for prompt testing",
        "pattern": "Create dataset -> Add examples -> Run evaluation",
        "example": "from langsmith import Client\n\nclient = Client()\ndataset = client.create_dataset('prompt-eval')\nclient.create_example(\n    dataset_id=dataset.id,\n    inputs={'input': 'What is Python?'},\n    outputs={'expected': 'Python is a programming language...'}\n)",
        "best_practices": [
          "Include diverse examples",
          "Cover edge cases",
          "Include expected outputs",
          "Regularly update dataset"
        ],
        "use_when": "Apply when dataset creation is needed in LangSmith Prompt Management Patterns",
        "code_example": "from langsmith import Client\n\nclient = Client()\ndataset = client.create_dataset('prompt-eval')\nclient.create_example(\n    dataset_id=dataset.id,\n    inputs={'input': 'What is Python?'},\n    outputs={'expected': 'Python is a programming language...'}\n)"
      },
      "evaluator_functions": {
        "description": "Define evaluators to measure prompt effectiveness",
        "types": [
          "Relevance evaluator",
          "Quality evaluator",
          "Correctness evaluator",
          "Custom evaluators"
        ],
        "example": "def relevance_evaluator(run, example):\n    output = run.outputs.get('output', '')\n    expected = example.outputs.get('expected', '')\n    overlap = len(set(output.split()) & set(expected.split()))\n    return {'score': min(overlap / 10, 1.0)}",
        "best_practices": [
          "Use LLM-based evaluators for complex metrics",
          "Combine multiple evaluators",
          "Validate evaluator accuracy"
        ],
        "use_when": "Apply when evaluator functions is needed in LangSmith Prompt Management Patterns",
        "code_example": "def relevance_evaluator(run, example):\n    output = run.outputs.get('output', '')\n    expected = example.outputs.get('expected', '')\n    overlap = len(set(output.split()) & set(expected.split()))\n    return {'score': min(overlap / 10, 1.0)}"
      },
      "evaluation_execution": {
        "description": "Run evaluations on prompt datasets",
        "pattern": "Load dataset -> Run chain -> Evaluate -> Analyze results",
        "example": "from langsmith.evaluation import evaluate\n\nresults = evaluate(\n    lambda x: chain.invoke(x),\n    data='prompt-eval',\n    evaluators=[relevance_evaluator]\n)",
        "use_when": "Regression testing; Prompt optimization; Quality assurance",
        "code_example": "from langsmith.evaluation import evaluate\n\nresults = evaluate(\n    lambda x: chain.invoke(x),\n    data='prompt-eval',\n    evaluators=[relevance_evaluator]\n)",
        "best_practices": [
          "Document the pattern usage and rationale in code comments",
          "Validate implementation against domain requirements before deployment"
        ]
      },
      "continuous_evaluation": {
        "description": "Continuously evaluate prompts in production",
        "pattern": "Sample production runs -> Evaluate -> Alert on degradation",
        "best_practices": [
          "Sample representative requests",
          "Set quality thresholds",
          "Alert on degradation",
          "Track trends over time"
        ],
        "use_when": "Apply when continuous evaluation is needed in LangSmith Prompt Management Patterns",
        "code_example": "# Implement continuous_evaluation per langsmith-prompts-patterns patterns\n# See description and related documentation"
      },
      "description": "Category grouping: dataset_creation, evaluator_functions, evaluation_execution, continuous_evaluation",
      "use_when": "Apply when evaluation patterns patterns are needed in LangSmith Prompt Management Patterns",
      "code_example": "# See child patterns under evaluation_patterns for implementation examples",
      "best_practices": [
        "Refer to specific child patterns for implementation details",
        "Apply the category pattern appropriate to your use case"
      ]
    },
    "prompt_templates": {
      "variable_templates": {
        "description": "Create reusable prompts with variables",
        "pattern": "Define template with placeholders -> Partial application -> Use",
        "example": "prompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a {role} assistant.\\n\\nContext: {context}\\n\\nInstructions: {instructions}'),\n    ('human', '{input}')\n])\n\nanalyst_prompt = prompt.partial(\n    role='data analyst',\n    output_format='JSON with analysis and confidence keys'\n)",
        "use_when": "Reusable prompts; Role-specific variants; Context injection",
        "code_example": "prompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a {role} assistant.\\n\\nContext: {context}\\n\\nInstructions: {instructions}'),\n    ('human', '{input}')\n])\n\nanalyst_prompt = prompt.partial(\n    role='data analyst',\n    output_format='JSON with analysis and confidence keys'\n)",
        "best_practices": [
          "Document the pattern usage and rationale in code comments",
          "Validate implementation against domain requirements before deployment"
        ]
      },
      "complex_templates": {
        "description": "Build complex prompts with multiple components",
        "components": [
          "System messages",
          "Context placeholders",
          "History placeholders",
          "Instructions",
          "Output format specifications"
        ],
        "best_practices": [
          "Use MessagesPlaceholder for history",
          "Separate concerns clearly",
          "Make templates composable"
        ],
        "use_when": "Apply when complex templates is needed in LangSmith Prompt Management Patterns",
        "code_example": "# Implement complex_templates per langsmith-prompts-patterns patterns\n# See description and related documentation"
      },
      "description": "Category grouping: variable_templates, complex_templates",
      "use_when": "Apply when prompt templates patterns are needed in LangSmith Prompt Management Patterns",
      "code_example": "# See child patterns under prompt_templates for implementation examples",
      "best_practices": [
        "Refer to specific child patterns for implementation details",
        "Apply the category pattern appropriate to your use case"
      ]
    },
    "integration_patterns": {
      "chain_integration": {
        "description": "Integrate Hub prompts into LangChain chains",
        "pattern": "Pull prompt -> Combine with LLM -> Add parsers -> Create chain",
        "example": "prompt = hub.pull('my-org/agent-prompt')\nchain = prompt | llm | StrOutputParser()",
        "best_practices": [
          "Keep chains modular",
          "Use consistent patterns",
          "Test chains independently"
        ],
        "use_when": "Apply when chain integration is needed in LangSmith Prompt Management Patterns",
        "code_example": "prompt = hub.pull('my-org/agent-prompt')\nchain = prompt | llm | StrOutputParser()"
      },
      "tracing_integration": {
        "description": "Use LangSmith tracing with Hub prompts",
        "pattern": "Enable tracing -> Use Hub prompts -> View traces",
        "benefits": [
          "Debug prompt issues",
          "Monitor performance",
          "Analyze usage patterns"
        ],
        "use_when": "Apply when tracing integration is needed in LangSmith Prompt Management Patterns",
        "code_example": "# Implement tracing_integration per langsmith-prompts-patterns patterns\n# See description and related documentation",
        "best_practices": [
          "Document the pattern usage and rationale in code comments",
          "Validate implementation against domain requirements before deployment"
        ]
      },
      "production_deployment": {
        "description": "Deploy Hub prompts to production",
        "workflow": [
          "Develop in dev environment",
          "Test with evaluation dataset",
          "A/B test in staging",
          "Deploy to production",
          "Monitor performance"
        ],
        "best_practices": [
          "Version all production prompts",
          "Keep dev/prod separate",
          "Monitor for degradation",
          "Have rollback plan"
        ],
        "use_when": "Apply when production deployment is needed in LangSmith Prompt Management Patterns",
        "code_example": "# Implement production_deployment per langsmith-prompts-patterns patterns\n# See description and related documentation"
      },
      "description": "Category grouping: chain_integration, tracing_integration, production_deployment",
      "use_when": "Apply when integration patterns patterns are needed in LangSmith Prompt Management Patterns",
      "code_example": "# See child patterns under integration_patterns for implementation examples",
      "best_practices": [
        "Refer to specific child patterns for implementation details",
        "Apply the category pattern appropriate to your use case"
      ]
    }
  },
  "best_practices": [
    "Use semantic versioning for prompts (v1, v2, v3) and increment major version for breaking changes, minor for additions",
    "Include clear descriptions when pushing prompts to Hub explaining purpose, use case, and any special considerations",
    "Test prompts on evaluation datasets before deploying to production to catch regressions and quality issues",
    "Create and maintain evaluation datasets with diverse examples covering edge cases and expected behaviors",
    "Tag prompts by use case, domain, and environment (dev/staging/prod) for easy discovery and organization",
    "Keep production and dev prompts completely separate using environment-specific naming conventions",
    "Use LangSmith Hub for all prompts instead of hardcoding to enable version control, collaboration, and easy updates",
    "Run A/B tests before major prompt changes by comparing variants on production traffic with statistical significance",
    "Monitor prompt performance in production using LangSmith tracing, metrics tracking, and quality alerts",
    "Use consistent naming conventions (org/domain/prompt-name:version) across all prompts for maintainability",
    "Document prompt changes including rationale, expected impact, and evaluation results in commit messages or changelogs",
    "Review prompts with team before production deployment to ensure quality, alignment, and catch issues early",
    "Have rollback strategy ready for prompt changes by keeping previous versions accessible and tested"
  ],
  "anti_patterns": [
    {
      "name": "Hardcoded Prompts",
      "problem": "Embedding prompts directly in code prevents version control, makes updates difficult, and blocks team collaboration",
      "solution": "Use LangSmith Hub for all prompts. Pull prompts from Hub at runtime to enable versioning, easy updates, and collaboration"
    },
    {
      "name": "No Versioning",
      "problem": "Updating prompts without versioning makes it impossible to track changes, rollback, or compare versions",
      "solution": "Always version prompts using semantic versioning (v1, v2, etc.) and tag versions in Hub for easy reference"
    },
    {
      "name": "No Testing Before Deployment",
      "problem": "Deploying prompts without testing leads to production degradation, regressions, and poor user experience",
      "solution": "Create evaluation datasets, test prompts on representative examples, and run regression tests before production deployment"
    },
    {
      "name": "Shared Dev/Prod Prompts",
      "problem": "Using the same prompts for development and production means breaking changes immediately affect users",
      "solution": "Separate prompts by environment using naming conventions (prompt-dev, prompt-prod) and never share between environments"
    },
    {
      "name": "No Evaluation Metrics",
      "problem": "Without evaluation, you can't measure prompt effectiveness, detect degradation, or optimize performance",
      "solution": "Create evaluation datasets, define evaluator functions, run regular evaluations, and track metrics over time"
    },
    {
      "name": "No A/B Testing",
      "problem": "Deploying prompt changes without A/B testing prevents comparing variants and risks deploying worse prompts",
      "solution": "Run A/B tests before major changes by randomly assigning variants, tracking metrics per variant, and analyzing statistically"
    },
    {
      "name": "No Production Monitoring",
      "problem": "Prompt degradation, quality issues, and failures go unnoticed without monitoring, leading to poor user experience",
      "solution": "Enable LangSmith tracing, monitor key metrics (quality, latency, errors), set up alerts for threshold violations"
    },
    {
      "name": "Inconsistent Naming Conventions",
      "problem": "Inconsistent prompt naming makes it difficult to find, manage, and understand prompt relationships",
      "solution": "Use consistent naming conventions (org/domain/prompt-name:version) and document naming standards for the team"
    }
  ]
}