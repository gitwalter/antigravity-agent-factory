{
  "id": "langsmith-prompt-management-patterns",
  "name": "LangSmith Prompt Management Patterns",
  "description": "Patterns for managing prompts with LangSmith Hub including versioning, A/B testing, evaluation, and integration strategies",
  "version": "1.0.0",
  "category": "agent-development",
  "patterns": {
    "hub_integration": {
      "push_prompts": {
        "description": "Push prompts to LangSmith Hub for version control",
        "pattern": "Create prompt -> Push to Hub -> Get versioned prompt",
        "example": "from langchain import hub\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a {role}. {instructions}'),\n    ('human', '{input}')\n])\n\nhub.push('my-org/agent-prompt', prompt, new_repo_is_public=False)",
        "best_practices": [
          "Use semantic versioning",
          "Include clear descriptions",
          "Tag prompts by use case",
          "Set appropriate visibility"
        ]
      },
      "pull_prompts": {
        "description": "Pull prompts from Hub for use in chains",
        "pattern": "Pull prompt from Hub -> Use in chain",
        "example": "prompt = hub.pull('my-org/agent-prompt')\nprompt_v2 = hub.pull('my-org/agent-prompt:v2')\nchain = prompt | llm | parser",
        "use_when": [
          "Production deployments",
          "Consistent prompt usage",
          "Team collaboration"
        ]
      },
      "prompt_organization": {
        "description": "Organize prompts in Hub with naming conventions",
        "structure": {
          "agents": "prompts/agents/analyst-v1, researcher, etc.",
          "tools": "prompts/tools/sql-generator, api-caller, etc.",
          "evaluation": "prompts/evaluation/relevance-judge, quality-judge, etc."
        },
        "best_practices": [
          "Use consistent naming",
          "Group by domain",
          "Separate dev and prod",
          "Include version numbers"
        ]
      }
    },
    "versioning_strategies": {
      "semantic_versioning": {
        "description": "Use semantic versioning for prompts (v1, v2, etc.)",
        "pattern": "prompt-name:v1, prompt-name:v2, prompt-name:v3",
        "best_practices": [
          "Increment major version for breaking changes",
          "Increment minor for additions",
          "Use tags for variants"
        ]
      },
      "version_comparison": {
        "description": "Compare prompt versions before updating",
        "pattern": "Pull current version -> Compare -> Push new version",
        "example": "current = hub.pull('my-org/agent-prompt')\n# Compare with new prompt\nhub.push('my-org/agent-prompt', new_prompt)",
        "use_when": [
          "Updating production prompts",
          "Tracking changes",
          "Review processes"
        ]
      },
      "environment_separation": {
        "description": "Separate prompts for dev, staging, and production",
        "pattern": "prompt-name-dev, prompt-name-staging, prompt-name-prod",
        "best_practices": [
          "Never share dev/prod prompts",
          "Test in staging first",
          "Use environment-specific naming"
        ]
      },
      "branching_strategy": {
        "description": "Use branches for experimental prompts",
        "pattern": "main branch for stable -> feature branches for experiments",
        "use_when": [
          "Testing new approaches",
          "Experimental features",
          "Team collaboration"
        ]
      }
    },
    "ab_testing": {
      "variant_selection": {
        "description": "Select prompt variants for A/B testing",
        "pattern": "Randomly select variant -> Execute -> Log results",
        "example": "import random\nfrom langsmith import traceable\n\n@traceable(tags=['ab-test'])\nasync def run_with_ab_test(input_data: dict, test_name: str = 'prompt_v1_vs_v2'):\n    variant = random.choice(['control', 'treatment'])\n    if variant == 'control':\n        prompt = hub.pull('my-org/agent-prompt:v1')\n    else:\n        prompt = hub.pull('my-org/agent-prompt:v2')\n    chain = prompt | llm | parser\n    result = await chain.ainvoke(input_data)\n    return {'result': result, 'variant': variant}",
        "best_practices": [
          "Use consistent test names",
          "Tag runs for analysis",
          "Ensure equal distribution",
          "Track metrics per variant"
        ]
      },
      "metrics_tracking": {
        "description": "Track metrics for each variant",
        "metrics": [
          "Response quality",
          "User satisfaction",
          "Cost per request",
          "Latency",
          "Error rates"
        ],
        "use_when": [
          "Comparing prompt versions",
          "Optimizing prompts",
          "Measuring improvements"
        ]
      },
      "statistical_analysis": {
        "description": "Analyze A/B test results statistically",
        "best_practices": [
          "Ensure sufficient sample size",
          "Use appropriate statistical tests",
          "Consider multiple metrics",
          "Account for confounding factors"
        ]
      }
    },
    "evaluation_patterns": {
      "dataset_creation": {
        "description": "Create evaluation datasets for prompt testing",
        "pattern": "Create dataset -> Add examples -> Run evaluation",
        "example": "from langsmith import Client\n\nclient = Client()\ndataset = client.create_dataset('prompt-eval')\nclient.create_example(\n    dataset_id=dataset.id,\n    inputs={'input': 'What is Python?'},\n    outputs={'expected': 'Python is a programming language...'}\n)",
        "best_practices": [
          "Include diverse examples",
          "Cover edge cases",
          "Include expected outputs",
          "Regularly update dataset"
        ]
      },
      "evaluator_functions": {
        "description": "Define evaluators to measure prompt effectiveness",
        "types": [
          "Relevance evaluator",
          "Quality evaluator",
          "Correctness evaluator",
          "Custom evaluators"
        ],
        "example": "def relevance_evaluator(run, example):\n    output = run.outputs.get('output', '')\n    expected = example.outputs.get('expected', '')\n    overlap = len(set(output.split()) & set(expected.split()))\n    return {'score': min(overlap / 10, 1.0)}",
        "best_practices": [
          "Use LLM-based evaluators for complex metrics",
          "Combine multiple evaluators",
          "Validate evaluator accuracy"
        ]
      },
      "evaluation_execution": {
        "description": "Run evaluations on prompt datasets",
        "pattern": "Load dataset -> Run chain -> Evaluate -> Analyze results",
        "example": "from langsmith.evaluation import evaluate\n\nresults = evaluate(\n    lambda x: chain.invoke(x),\n    data='prompt-eval',\n    evaluators=[relevance_evaluator]\n)",
        "use_when": [
          "Regression testing",
          "Prompt optimization",
          "Quality assurance"
        ]
      },
      "continuous_evaluation": {
        "description": "Continuously evaluate prompts in production",
        "pattern": "Sample production runs -> Evaluate -> Alert on degradation",
        "best_practices": [
          "Sample representative requests",
          "Set quality thresholds",
          "Alert on degradation",
          "Track trends over time"
        ]
      }
    },
    "prompt_templates": {
      "variable_templates": {
        "description": "Create reusable prompts with variables",
        "pattern": "Define template with placeholders -> Partial application -> Use",
        "example": "prompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a {role} assistant.\\n\\nContext: {context}\\n\\nInstructions: {instructions}'),\n    ('human', '{input}')\n])\n\nanalyst_prompt = prompt.partial(\n    role='data analyst',\n    output_format='JSON with analysis and confidence keys'\n)",
        "use_when": [
          "Reusable prompts",
          "Role-specific variants",
          "Context injection"
        ]
      },
      "complex_templates": {
        "description": "Build complex prompts with multiple components",
        "components": [
          "System messages",
          "Context placeholders",
          "History placeholders",
          "Instructions",
          "Output format specifications"
        ],
        "best_practices": [
          "Use MessagesPlaceholder for history",
          "Separate concerns clearly",
          "Make templates composable"
        ]
      }
    },
    "integration_patterns": {
      "chain_integration": {
        "description": "Integrate Hub prompts into LangChain chains",
        "pattern": "Pull prompt -> Combine with LLM -> Add parsers -> Create chain",
        "example": "prompt = hub.pull('my-org/agent-prompt')\nchain = prompt | llm | StrOutputParser()",
        "best_practices": [
          "Keep chains modular",
          "Use consistent patterns",
          "Test chains independently"
        ]
      },
      "tracing_integration": {
        "description": "Use LangSmith tracing with Hub prompts",
        "pattern": "Enable tracing -> Use Hub prompts -> View traces",
        "benefits": [
          "Debug prompt issues",
          "Monitor performance",
          "Analyze usage patterns"
        ]
      },
      "production_deployment": {
        "description": "Deploy Hub prompts to production",
        "workflow": [
          "Develop in dev environment",
          "Test with evaluation dataset",
          "A/B test in staging",
          "Deploy to production",
          "Monitor performance"
        ],
        "best_practices": [
          "Version all production prompts",
          "Keep dev/prod separate",
          "Monitor for degradation",
          "Have rollback plan"
        ]
      }
    }
  },
  "best_practices": [
    "Use semantic versioning for prompts (v1, v2, v3) and increment major version for breaking changes, minor for additions",
    "Include clear descriptions when pushing prompts to Hub explaining purpose, use case, and any special considerations",
    "Test prompts on evaluation datasets before deploying to production to catch regressions and quality issues",
    "Create and maintain evaluation datasets with diverse examples covering edge cases and expected behaviors",
    "Tag prompts by use case, domain, and environment (dev/staging/prod) for easy discovery and organization",
    "Keep production and dev prompts completely separate using environment-specific naming conventions",
    "Use LangSmith Hub for all prompts instead of hardcoding to enable version control, collaboration, and easy updates",
    "Run A/B tests before major prompt changes by comparing variants on production traffic with statistical significance",
    "Monitor prompt performance in production using LangSmith tracing, metrics tracking, and quality alerts",
    "Use consistent naming conventions (org/domain/prompt-name:version) across all prompts for maintainability",
    "Document prompt changes including rationale, expected impact, and evaluation results in commit messages or changelogs",
    "Review prompts with team before production deployment to ensure quality, alignment, and catch issues early",
    "Have rollback strategy ready for prompt changes by keeping previous versions accessible and tested"
  ],
  "anti_patterns": [
    {
      "name": "Hardcoded Prompts",
      "problem": "Embedding prompts directly in code prevents version control, makes updates difficult, and blocks team collaboration",
      "solution": "Use LangSmith Hub for all prompts. Pull prompts from Hub at runtime to enable versioning, easy updates, and collaboration"
    },
    {
      "name": "No Versioning",
      "problem": "Updating prompts without versioning makes it impossible to track changes, rollback, or compare versions",
      "solution": "Always version prompts using semantic versioning (v1, v2, etc.) and tag versions in Hub for easy reference"
    },
    {
      "name": "No Testing Before Deployment",
      "problem": "Deploying prompts without testing leads to production degradation, regressions, and poor user experience",
      "solution": "Create evaluation datasets, test prompts on representative examples, and run regression tests before production deployment"
    },
    {
      "name": "Shared Dev/Prod Prompts",
      "problem": "Using the same prompts for development and production means breaking changes immediately affect users",
      "solution": "Separate prompts by environment using naming conventions (prompt-dev, prompt-prod) and never share between environments"
    },
    {
      "name": "No Evaluation Metrics",
      "problem": "Without evaluation, you can't measure prompt effectiveness, detect degradation, or optimize performance",
      "solution": "Create evaluation datasets, define evaluator functions, run regular evaluations, and track metrics over time"
    },
    {
      "name": "No A/B Testing",
      "problem": "Deploying prompt changes without A/B testing prevents comparing variants and risks deploying worse prompts",
      "solution": "Run A/B tests before major changes by randomly assigning variants, tracking metrics per variant, and analyzing statistically"
    },
    {
      "name": "No Production Monitoring",
      "problem": "Prompt degradation, quality issues, and failures go unnoticed without monitoring, leading to poor user experience",
      "solution": "Enable LangSmith tracing, monitor key metrics (quality, latency, errors), set up alerts for threshold violations"
    },
    {
      "name": "Inconsistent Naming Conventions",
      "problem": "Inconsistent prompt naming makes it difficult to find, manage, and understand prompt relationships",
      "solution": "Use consistent naming conventions (org/domain/prompt-name:version) and document naming standards for the team"
    }
  ],
  "related_skills": [
    "langsmith-prompts",
    "langsmith-tracing",
    "langchain-usage",
    "prompt-engineering"
  ]
}