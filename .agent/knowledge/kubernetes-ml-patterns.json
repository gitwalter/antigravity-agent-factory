{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "kubernetes-ml-patterns",
  "name": "Kubernetes ML Patterns",
  "title": "Kubernetes ML Model Deployment Patterns",
  "description": "Patterns for deploying ML models on Kubernetes including GPU scheduling, model serving, and autoscaling",
  "version": "1.0.0",
  "category": "integration",
  "axiomAlignment": {
    "A1_verifiability": "K8s configs enable reproducible ML deployments",
    "A2_user_primacy": "GPU scheduling and autoscaling serve inference latency and throughput needs",
    "A3_transparency": "Resource definitions make ML infrastructure explicit",
    "A4_non_harm": "Resource limits and gang scheduling prevent resource starvation",
    "A5_consistency": "Unified ML patterns across Triton, Ray, and training operators"
  },
  "related_skills": [
    "ml-deployment",
    "kubernetes-deployment",
    "model-serving",
    "model-training"
  ],
  "related_knowledge": [
    "kubernetes-patterns.json",
    "kubernetes-deployment-patterns.json",
    "mlops-patterns.json",
    "model-serving-patterns.json"
  ],
  "gpu_scheduling": {
    "description": "GPU scheduling in Kubernetes",
    "device_plugins": {
      "nvidia": "NVIDIA device plugin",
      "amd": "AMD device plugin"
    },
    "resource_requests": "resources:\n  limits:\n    nvidia.com/gpu: 1",
    "best_practices": [
      "Request GPUs explicitly",
      "Use device plugins",
      "Monitor GPU usage",
      "Handle GPU failures"
    ]
  },
  "kueue": {
    "description": "Kubernetes job queueing system",
    "features": [
      "Job queueing",
      "Resource quotas",
      "Fair scheduling",
      "Preemption"
    ],
    "use_cases": [
      "Training jobs",
      "Batch inference",
      "Resource management"
    ]
  },
  "volcano": {
    "description": "Volcano batch scheduling system",
    "features": [
      "Gang scheduling",
      "Job management",
      "Resource allocation",
      "Preemption"
    ],
    "use_cases": [
      "Distributed training",
      "Batch jobs",
      "Multi-job coordination"
    ]
  },
  "mig_mps": {
    "description": "Multi-Instance GPU and Multi-Process Service",
    "mig": {
      "description": "Partition GPU into instances",
      "use_when": "Share GPU among multiple workloads"
    },
    "mps": {
      "description": "Share GPU among processes",
      "use_when": "Multiple processes need GPU"
    }
  },
  "gang_scheduling": {
    "description": "Schedule all pods together",
    "use_cases": [
      "Distributed training",
      "Multi-pod jobs",
      "Synchronized workloads"
    ],
    "implementation": "Use Volcano or Kueue for gang scheduling"
  },
  "training_operator": {
    "description": "Kubeflow Training Operator",
    "operators": {
      "pytorch_job": "PyTorch distributed training",
      "tensorflow_job": "TensorFlow distributed training",
      "mpi_job": "MPI-based training",
      "xgboost_job": "XGBoost training"
    },
    "usage": "apiVersion: kubeflow.org/v1\nkind: PyTorchJob\nmetadata:\n  name: pytorch-training\nspec:\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      template:\n        spec:\n          containers:\n          - name: pytorch\n            image: pytorch:latest",
    "best_practices": [
      "Use appropriate operator",
      "Configure resources",
      "Handle failures",
      "Monitor training"
    ]
  },
  "ray_on_k8s": {
    "description": "Ray distributed computing on Kubernetes",
    "features": [
      "Distributed training",
      "Hyperparameter tuning",
      "Distributed inference",
      "Auto-scaling"
    ],
    "deployment": "apiVersion: ray.io/v1\nkind: RayCluster\nmetadata:\n  name: ray-cluster\nspec:\n  headGroupSpec:\n    serviceType: ClusterIP\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:latest",
    "best_practices": [
      "Configure autoscaling",
      "Monitor Ray dashboard",
      "Handle node failures",
      "Optimize resource usage"
    ]
  },
  "triton_on_k8s": {
    "description": "NVIDIA Triton Inference Server on Kubernetes",
    "features": [
      "Multi-framework support",
      "Dynamic batching",
      "Model ensemble",
      "GPU optimization"
    ],
    "deployment": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: triton-server\nspec:\n  replicas: 2\n  template:\n    spec:\n      containers:\n      - name: triton\n        image: nvcr.io/nvidia/tritonserver:latest\n        resources:\n          limits:\n            nvidia.com/gpu: 1",
    "best_practices": [
      "Use dynamic batching",
      "Configure model repository",
      "Monitor performance",
      "Scale horizontally"
    ]
  },
  "model_serving_autoscaling": {
    "description": "Autoscale model serving",
    "hpa_config": "apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: model-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: model-serving\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Pods\n    pods:\n      metric:\n        name: request_latency\n      target:\n        type: AverageValue\n        averageValue: '100'",
    "custom_metrics": {
      "request_latency": "Average request latency",
      "queue_length": "Request queue length",
      "gpu_utilization": "GPU utilization"
    },
    "best_practices": [
      "Use multiple metrics",
      "Set appropriate thresholds",
      "Monitor scaling behavior",
      "Handle cold starts"
    ]
  },
  "model_storage": {
    "description": "Store models in Kubernetes",
    "options": {
      "configmaps": "Small models",
      "persistent_volumes": "Large models",
      "object_storage": "S3, GCS, Azure Blob",
      "model_registry": "MLflow, Weights & Biases"
    },
    "best_practices": [
      "Use object storage for large models",
      "Cache models locally",
      "Version models",
      "Handle model updates"
    ]
  },
  "inference_patterns": {
    "description": "Inference deployment patterns",
    "patterns": {
      "single_model": "One model per pod",
      "multi_model": "Multiple models per pod",
      "model_ensemble": "Combine multiple models",
      "pipeline": "Sequential model execution"
    },
    "best_practices": [
      "Choose appropriate pattern",
      "Optimize resource usage",
      "Handle model loading",
      "Monitor inference latency"
    ]
  },
  "patterns": {
    "gpu_sharing": {
      "description": "Share GPU among workloads",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# gpu_sharing pattern for kubernetes-ml-patterns\n# Implement based on description: Share GPU among workloads...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    },
    "distributed_training": {
      "description": "Distributed training on K8s",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# distributed_training pattern for kubernetes-ml-patterns\n# Implement based on description: Distributed training on K8s...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    },
    "model_serving": {
      "description": "Serve models on K8s ",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# model_serving pattern for kubernetes-ml-patterns\n# Implement based on description: Serve models on K8s ...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    },
    "autoscaling": {
      "description": "Auto-scale ML workloads",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# autoscaling pattern for kubernetes-ml-patterns\n# Implement based on description: Auto-scale ML workloads...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    }
  },
  "best_practices": [
    "Use device plugins for GPUs",
    "Configure resource requests",
    "Use training operators",
    "Implement autoscaling",
    "Monitor GPU usage",
    "Handle failures gracefully",
    "Use gang scheduling for distributed training",
    "Optimize model serving",
    "Cache models",
    "Version models",
    "Use persistent storage",
    "Monitor inference latency",
    "Set up alerts",
    "Test scaling behavior",
    "Document deployment"
  ],
  "anti_patterns": [
    {
      "name": "No GPU Resource Requests",
      "problem": "Can't schedule GPU workloads",
      "fix": "Request GPUs explicitly"
    },
    {
      "name": "No Autoscaling",
      "problem": "Poor resource utilization",
      "fix": "Configure HPA"
    },
    {
      "name": "Storing Models in ConfigMaps",
      "problem": "Size limits, slow updates",
      "fix": "Use object storage"
    },
    {
      "name": "No Gang Scheduling",
      "problem": "Inefficient distributed training",
      "fix": "Use Volcano or Kueue"
    }
  ]
}