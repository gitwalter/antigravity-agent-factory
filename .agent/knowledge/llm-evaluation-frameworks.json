{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "llm-evaluation-frameworks",
  "name": "LLM Evaluation Frameworks",
  "title": "LLM Evaluation and Testing Frameworks",
  "description": "Frameworks and patterns for evaluating LLM applications including RAG evaluation, retrieval metrics, and regression testing",
  "version": "1.0.0",
  "category": "ai-ml",
  "axiomAlignment": {
    "A1_verifiability": "Evaluation metrics enable LLM verification",
    "A2_user_primacy": "Evaluation criteria reflect user requirements and acceptance criteria",
    "A3_transparency": "Evaluation results make model behavior explicit",
    "A4_non_harm": "Evaluation supports continuous improvement and harm prevention",
    "A5_consistency": "Unified evaluation patterns across RAGAS, DeepEval, and LangSmith"
  },
  "related_skills": [
    "llm-evaluation",
    "agent-testing",
    "rag-patterns",
    "prompt-optimization"
  ],
  "related_knowledge": [
    "llm-evaluation-patterns.json",
    "rag-patterns.json",
    "anthropic-patterns.json",
    "guardrails-patterns.json"
  ],
  "ragas_metrics": {
    "description": "RAGAS (Retrieval Augmented Generation Assessment) metrics",
    "metrics": {
      "faithfulness": "Answer grounded in context",
      "answer_relevance": "Answer relevance to question",
      "context_precision": "Precision of retrieved context",
      "context_recall": "Recall of relevant context",
      "answer_similarity": "Semantic similarity to ground truth",
      "answer_correctness": "Factual correctness"
    },
    "usage": "from ragas import evaluate\nfrom ragas.metrics import faithfulness, answer_relevance\n\nresult = evaluate(\n    dataset=dataset,\n    metrics=[faithfulness, answer_relevance]\n)",
    "best_practices": [
      "Use multiple metrics",
      "Set appropriate thresholds",
      "Monitor metrics over time",
      "Combine with human evaluation"
    ]
  },
  "deepeval": {
    "description": "DeepEval evaluation framework",
    "features": [
      "Test case management",
      "Automated evaluation",
      "CI/CD integration",
      "Custom evaluators"
    ],
    "usage": "from deepeval import assert_test\nfrom deepeval.metrics import AnswerRelevancyMetric\n\n@assert_test(metrics=[AnswerRelevancyMetric(threshold=0.7)])\ndef test_answer_relevancy():\n    output = llm.generate('What is Python?')\n    return output",
    "best_practices": [
      "Write test cases",
      "Set appropriate thresholds",
      "Run in CI/CD",
      "Track test results"
    ]
  },
  "langsmith_evaluation": {
    "description": "LangSmith evaluation platform",
    "features": [
      "Dataset management",
      "Evaluation runs",
      "Comparison views",
      "Regression detection"
    ],
    "usage": "from langsmith import Client\nfrom langsmith.evaluation import evaluate\n\nclient = Client()\nresults = evaluate(\n    dataset='my-dataset',\n    llm=my_llm,\n    evaluators=[my_evaluator]\n)",
    "best_practices": [
      "Create evaluation datasets",
      "Define custom evaluators",
      "Run regular evaluations",
      "Monitor for regressions"
    ]
  },
  "custom_evaluators": {
    "description": "Custom evaluation functions",
    "types": {
      "factual_correctness": "Check factual accuracy",
      "relevance": "Check answer relevance",
      "completeness": "Check answer completeness",
      "safety": "Check content safety",
      "style": "Check writing style"
    },
    "implementation": "def evaluate_answer(question, answer, context):\n    # Custom evaluation logic\n    score = calculate_score(question, answer, context)\n    return {\n        'score': score,\n        'passed': score > threshold\n    }",
    "best_practices": [
      "Define clear criteria",
      "Use LLMs for evaluation",
      "Combine multiple evaluators",
      "Validate evaluators"
    ]
  },
  "retrieval_metrics": {
    "description": "Metrics for retrieval systems",
    "metrics": {
      "precision": "Relevant retrieved / Total retrieved",
      "recall": "Relevant retrieved / Total relevant",
      "f1_score": "Harmonic mean of precision and recall",
      "mrr": "Mean Reciprocal Rank",
      "ndcg": "Normalized Discounted Cumulative Gain",
      "hit_rate": "Percentage of queries with at least one relevant result"
    },
    "implementation": "from sklearn.metrics import precision_score, recall_score\n\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = 2 * (precision * recall) / (precision + recall)",
    "best_practices": [
      "Use multiple metrics",
      "Consider ranking quality",
      "Evaluate on diverse queries",
      "Monitor over time"
    ]
  },
  "regression_testing": {
    "description": "Regression testing for LLM outputs",
    "strategies": {
      "output_comparison": "Compare outputs across versions",
      "metric_tracking": "Track metrics over time",
      "test_suite": "Maintain test cases",
      "automated_alerts": "Alert on regressions"
    },
    "implementation": "def test_regression(question, expected_output):\n    actual_output = llm.generate(question)\n    similarity = calculate_similarity(expected_output, actual_output)\n    assert similarity > threshold, 'Regression detected'",
    "best_practices": [
      "Maintain test suite",
      "Set similarity thresholds",
      "Track changes over time",
      "Investigate regressions"
    ]
  },
  "evaluation_datasets": {
    "description": "Creating and managing evaluation datasets",
    "components": {
      "questions": "Test questions",
      "contexts": "Relevant contexts",
      "ground_truth": "Expected answers",
      "metadata": "Additional information"
    },
    "best_practices": [
      "Cover diverse scenarios",
      "Include edge cases",
      "Regularly update dataset",
      "Version datasets"
    ]
  },
  "human_evaluation": {
    "description": "Human evaluation of LLM outputs",
    "aspects": {
      "correctness": "Factual accuracy",
      "helpfulness": "Usefulness of answer",
      "harmlessness": "Safety of content",
      "coherence": "Logical flow"
    },
    "best_practices": [
      "Define clear criteria",
      "Train evaluators",
      "Use multiple evaluators",
      "Calculate inter-annotator agreement"
    ]
  },
  "evaluation_pipeline": {
    "description": "End-to-end evaluation pipeline",
    "steps": [
      "Prepare evaluation dataset",
      "Run model on dataset",
      "Calculate metrics",
      "Compare with baselines",
      "Generate report",
      "Track over time"
    ],
    "tools": [
      "RAGAS",
      "DeepEval",
      "LangSmith",
      "Custom pipelines"
    ]
  },
  "patterns": {
    "metric_calculation": {
      "description": "Calculate evaluation metrics",
      "use_when": "When evaluating RAG systems or LLM outputs before deployment",
      "code_example": "from ragas import evaluate\nfrom ragas.metrics import faithfulness, answer_relevancy\nresults = evaluate(dataset=dataset, metrics=[faithfulness, answer_relevancy])\nprint(f\"Faithfulness: {results['faithfulness']:.3f}\")",
      "best_practices": [
        "Use multiple evaluation metrics",
        "Evaluate on diverse datasets",
        "Combine automated and human evaluation",
        "Set appropriate thresholds",
        "Monitor metrics over time"
      ]
    },
    "threshold_validation": {
      "description": "Validate against thresholds",
      "use_when": "When evaluating RAG systems or LLM outputs before deployment",
      "code_example": "from ragas import evaluate\nfrom ragas.metrics import faithfulness, answer_relevancy\nresults = evaluate(dataset=dataset, metrics=[faithfulness, answer_relevancy])\nprint(f\"Faithfulness: {results['faithfulness']:.3f}\")",
      "best_practices": [
        "Use multiple evaluation metrics",
        "Evaluate on diverse datasets",
        "Combine automated and human evaluation",
        "Set appropriate thresholds",
        "Monitor metrics over time"
      ]
    },
    "comparison_analysis": {
      "description": "Compare model versions",
      "use_when": "When evaluating RAG systems or LLM outputs before deployment",
      "code_example": "from ragas import evaluate\nfrom ragas.metrics import faithfulness, answer_relevancy\nresults = evaluate(dataset=dataset, metrics=[faithfulness, answer_relevancy])\nprint(f\"Faithfulness: {results['faithfulness']:.3f}\")",
      "best_practices": [
        "Use multiple evaluation metrics",
        "Evaluate on diverse datasets",
        "Combine automated and human evaluation",
        "Set appropriate thresholds",
        "Monitor metrics over time"
      ]
    },
    "regression_detection": {
      "description": "Detect performance regressions",
      "use_when": "When evaluating RAG systems or LLM outputs before deployment",
      "code_example": "from ragas import evaluate\nfrom ragas.metrics import faithfulness, answer_relevancy\nresults = evaluate(dataset=dataset, metrics=[faithfulness, answer_relevancy])\nprint(f\"Faithfulness: {results['faithfulness']:.3f}\")",
      "best_practices": [
        "Use multiple evaluation metrics",
        "Evaluate on diverse datasets",
        "Combine automated and human evaluation",
        "Set appropriate thresholds",
        "Monitor metrics over time"
      ]
    }
  },
  "best_practices": [
    "Use multiple evaluation metrics",
    "Evaluate on diverse datasets",
    "Combine automated and human evaluation",
    "Set appropriate thresholds",
    "Monitor metrics over time",
    "Track regressions",
    "Version evaluation datasets",
    "Document evaluation criteria",
    "Run evaluations regularly",
    "Compare with baselines",
    "Use statistical significance",
    "Report confidence intervals",
    "Investigate failures",
    "Update evaluation criteria",
    "Share evaluation results"
  ],
  "anti_patterns": [
    {
      "name": "Single Metric Evaluation",
      "problem": "Incomplete picture",
      "solution": "Use multiple metrics"
    },
    {
      "name": "No Baseline Comparison",
      "problem": "Can't assess improvement",
      "solution": "Compare with baselines"
    },
    {
      "name": "No Regression Testing",
      "problem": "Miss performance degradation",
      "solution": "Implement regression testing"
    },
    {
      "name": "Static Evaluation Dataset",
      "problem": "Doesn't reflect real usage",
      "solution": "Regularly update dataset"
    }
  ]
}