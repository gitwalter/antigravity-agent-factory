{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "ai-guardrails-patterns",
  "name": "AI Guardrails Patterns",
  "title": "AI Guardrails and Safety Patterns",
  "description": "Patterns for implementing guardrails, content safety, and prompt injection prevention in AI applications",
  "version": "1.0.0",
  "category": "security",
  "axiomAlignment": {
    "A1_verifiability": "Guardrails enable verification of AI behavior and output validation",
    "A2_user_primacy": "Content filtering and topic control protect user wellbeing",
    "A3_transparency": "Explicit guardrails make safety measures clear",
    "A4_non_harm": "Content safety and PII detection prevent harmful outputs",
    "A5_consistency": "Unified defense layers across input, prompt, runtime, and output"
  },
  "related_skills": [
    "llm-guardrails",
    "ai-security",
    "security-sandboxing"
  ],
  "related_knowledge": [
    "ai-security-patterns.json",
    "tool-patterns.json"
  ],
  "nemo_guardrails": {
    "description": "NVIDIA NeMo Guardrails framework",
    "components": {
      "colang": "Conversation flow language",
      "flows": "Define conversation flows",
      "actions": "Python functions for guardrail actions",
      "models": "LLM models for guardrails"
    },
    "colang_examples": {
      "greeting": "define user express greeting\n  \"hello\"\n  \"hi\"\n  \"hey\"\n\ndefine bot express greeting\n  \"Hello! How can I help you?\"",
      "restriction": "define user ask about restricted topic\n  \"tell me about confidential information\"\n\ndefine bot refuse\n  \"I'm sorry, I can't discuss that topic.\"",
      "flow": "define flow main\n  user express greeting\n  bot express greeting\n  user ask question\n  bot provide answer"
    },
    "best_practices": [
      "Define clear conversation flows",
      "Handle edge cases",
      "Test guardrails thoroughly",
      "Monitor guardrail triggers"
    ],
    "use_when": "Apply when implementing nemo guardrails in security context",
    "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)"
  },
  "guardrails_ai": {
    "description": "Guardrails AI framework",
    "validators": {
      "toxic_language": "Detect toxic content",
      "pii": "Detect personally identifiable information",
      "sensitive_topics": "Filter sensitive topics",
      "prompt_injection": "Detect prompt injection attempts"
    },
    "usage": "from guardrails import Guard\nfrom guardrails.hub import DetectPII\n\nguard = Guard().use(DetectPII())\nresult = guard.validate(prompt)",
    "best_practices": [
      "Use appropriate validators",
      "Configure thresholds",
      "Handle validation failures",
      "Log guardrail events"
    ],
    "use_when": "Apply when implementing guardrails ai in security context",
    "code_example": "from guardrails import Guard\nfrom guardrails.hub import DetectPII\n\nguard = Guard().use(DetectPII())\nresult = guard.validate(prompt)"
  },
  "llamaguard": {
    "description": "Meta's LlamaGuard for content classification",
    "capabilities": [
      "Content safety classification",
      "Multi-category detection",
      "Customizable policies"
    ],
    "use_cases": [
      "Content moderation",
      "Chat safety",
      "API filtering"
    ],
    "integration": "from llama_guard import LlamaGuard\n\nguard = LlamaGuard.from_pretrained('meta-llama/LlamaGuard-7b')\nresult = guard.classify(text)",
    "use_when": "Apply when implementing llamaguard in security context",
    "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
    "best_practices": [
      "Document the pattern usage and rationale in code comments for llamaguard",
      "Validate implementation against domain requirements before deployment"
    ]
  },
  "presidio_pii": {
    "description": "Microsoft Presidio for PII detection",
    "supported_entities": [
      "EMAIL_ADDRESS",
      "PHONE_NUMBER",
      "CREDIT_CARD",
      "SSN",
      "IP_ADDRESS",
      "PERSON",
      "LOCATION"
    ],
    "usage": "from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\nanalyzer = AnalyzerEngine()\nresults = analyzer.analyze(text=text, language='en')\nanonymizer = AnonymizerEngine()\nanonymized = anonymizer.anonymize(text=text, analyzer_results=results)",
    "best_practices": [
      "Configure entity detection",
      "Anonymize sensitive data",
      "Validate anonymization",
      "Handle false positives"
    ],
    "use_when": "Apply when implementing presidio pii in security context",
    "code_example": "from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\nanalyzer = AnalyzerEngine()\nresults = analyzer.analyze(text=text, language='en')\nanonymizer = AnonymizerEngine()\nanonymized = anonymizer.anonymize(text=text, analyzer_results=results)"
  },
  "prompt_injection": {
    "description": "Prompt injection attack patterns and defenses",
    "attack_types": {
      "direct_injection": "User input contains instructions",
      "indirect_injection": "Hidden instructions in data",
      "jailbreak": "Bypass safety measures",
      "adversarial_prompts": "Crafted prompts to evade detection"
    },
    "defense_layers": {
      "input_sanitization": "Clean and validate inputs",
      "prompt_validation": "Check prompt structure",
      "output_filtering": "Filter model outputs",
      "context_isolation": "Isolate user context",
      "system_prompt_hardening": "Strengthen system prompts"
    },
    "detection_patterns": {
      "instruction_keywords": [
        "ignore",
        "forget",
        "system",
        "admin"
      ],
      "encoding_techniques": [
        "base64",
        "unicode",
        "rot13"
      ],
      "prompt_leaking": "Attempts to extract system prompt"
    },
    "mitigation": {
      "input_validation": "Validate and sanitize all inputs",
      "prompt_encoding": "Encode user inputs",
      "output_validation": "Validate model outputs",
      "rate_limiting": "Limit prompt attempts",
      "monitoring": "Monitor for injection patterns"
    },
    "use_when": "Apply when implementing prompt injection in security context",
    "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
    "best_practices": [
      "Document the pattern usage and rationale in code comments for prompt_injection",
      "Validate implementation against domain requirements before deployment"
    ]
  },
  "content_safety": {
    "description": "Content safety classification",
    "categories": {
      "hate_speech": "Hateful or discriminatory content",
      "violence": "Violent or harmful content",
      "sexual_content": "Sexual or explicit content",
      "self_harm": "Self-harm or suicide content",
      "illegal_activities": "Illegal activities"
    },
    "tools": {
      "azure_content_safety": "Azure Content Safety API",
      "perspective_api": "Google Perspective API",
      "openai_moderation": "OpenAI Moderation API"
    },
    "implementation": "from openai import OpenAI\n\nclient = OpenAI()\nmoderation = client.moderations.create(input=text)\nif moderation.results[0].flagged:\n    # Handle unsafe content\n    pass",
    "use_when": "Apply when implementing content safety in security context",
    "code_example": "from openai import OpenAI\n\nclient = OpenAI()\nmoderation = client.moderations.create(input=text)\nif moderation.results[0].flagged:\n    # Handle unsafe content\n    pass",
    "best_practices": [
      "Document the pattern usage and rationale in code comments for content_safety",
      "Validate implementation against domain requirements before deployment"
    ]
  },
  "topic_control": {
    "description": "Control allowed topics",
    "whitelist_approach": "Only allow specific topics",
    "blacklist_approach": "Block specific topics",
    "hybrid_approach": "Combine whitelist and blacklist",
    "implementation": "allowed_topics = ['general', 'technology', 'science']\nif topic not in allowed_topics:\n    return 'I can only discuss allowed topics.'",
    "use_when": "Apply when implementing topic control in security context",
    "code_example": "allowed_topics = ['general', 'technology', 'science']\nif topic not in allowed_topics:\n    return 'I can only discuss allowed topics.'",
    "best_practices": [
      "Document the pattern usage and rationale in code comments for topic_control",
      "Validate implementation against domain requirements before deployment"
    ]
  },
  "output_filtering": {
    "description": "Filter model outputs",
    "techniques": {
      "keyword_filtering": "Filter by keywords",
      "regex_patterns": "Use regex patterns",
      "ml_classification": "Use ML models",
      "rule_based": "Rule-based filtering"
    },
    "best_practices": [
      "Filter before returning to user",
      "Log filtered content",
      "Handle false positives",
      "Provide fallback responses"
    ],
    "use_when": "Apply when: keyword_filtering; regex_patterns; ml_classification",
    "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)"
  },
  "defense_layers": {
    "layer_1_input": {
      "description": "Input validation and sanitization",
      "techniques": [
        "Input length limits",
        "Character filtering",
        "Encoding validation",
        "Structure validation"
      ],
      "use_when": "Apply when: Input length limits; Character filtering; Encoding validation",
      "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
      "best_practices": [
        "Apply Input length limits",
        "Apply Character filtering",
        "Apply Encoding validation",
        "Apply Structure validation"
      ]
    },
    "layer_2_prompt": {
      "description": "Implements layer 2 prompt for reliable, maintainable code. Use when the scenario requires this pattern.",
      "techniques": [
        "System prompt isolation",
        "User input encoding",
        "Prompt structure validation",
        "Context separation"
      ],
      "use_when": "Apply when: System prompt isolation; User input encoding; Prompt structure validation",
      "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
      "best_practices": [
        "Apply System prompt isolation",
        "Apply User input encoding",
        "Apply Prompt structure validation",
        "Apply Context separation"
      ]
    },
    "layer_3_runtime": {
      "description": "Implements layer 3 runtime for reliable, maintainable code. Use when the scenario requires this pattern.",
      "techniques": [
        "Token monitoring",
        "Behavioral analysis",
        "Anomaly detection",
        "Rate limiting"
      ],
      "use_when": "Apply when: Token monitoring; Behavioral analysis; Anomaly detection",
      "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
      "best_practices": [
        "Apply Token monitoring",
        "Apply Behavioral analysis",
        "Apply Anomaly detection",
        "Apply Rate limiting"
      ]
    },
    "layer_4_output": {
      "description": "Implements layer 4 output for reliable, maintainable code. Use when the scenario requires this pattern.",
      "techniques": [
        "Content filtering",
        "PII detection",
        "Safety classification",
        "Output sanitization"
      ],
      "use_when": "Apply when: Content filtering; PII detection; Safety classification",
      "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
      "best_practices": [
        "Apply Content filtering",
        "Apply PII detection",
        "Apply Safety classification",
        "Apply Output sanitization"
      ]
    }
  },
  "patterns": {
    "input_sanitization": {
      "description": "Sanitize user inputs",
      "steps": [
        "Validate input format",
        "Remove dangerous characters",
        "Encode special characters",
        "Check for injection patterns"
      ],
      "use_when": "Apply when implementing input sanitization in security context",
      "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
      "best_practices": [
        "Document the pattern usage and rationale in code comments for input_sanitization",
        "Validate implementation against domain requirements before deployment"
      ]
    },
    "prompt_encoding": {
      "description": "Encode user inputs in prompts",
      "techniques": [
        "Base64 encoding",
        "JSON encoding",
        "XML encoding",
        "Custom delimiters"
      ],
      "use_when": "Apply when: Base64 encoding; JSON encoding; XML encoding",
      "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
      "best_practices": [
        "Apply Base64 encoding",
        "Apply JSON encoding",
        "Apply XML encoding",
        "Apply Custom delimiters"
      ]
    },
    "output_validation": {
      "description": "Validate model outputs",
      "checks": [
        "Content safety",
        "PII detection",
        "Topic compliance",
        "Format validation"
      ],
      "use_when": "Apply when implementing output validation in security context",
      "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
      "best_practices": [
        "Document the pattern usage and rationale in code comments for output_validation",
        "Validate implementation against domain requirements before deployment"
      ]
    },
    "fallback_responses": {
      "description": "Provide safe fallback responses",
      "use_when": "Guardrail triggered; Unsafe content detected; Validation failure",
      "code_example": "import re\n\ndef sanitize_input(text: str, max_len: int = 1000) -> str:\n    if len(text) > max_len:\n        raise ValueError('Input too long')\n    return ''.join(c for c in text if c.isprintable())\n\nresult = sanitize_input(user_input)",
      "best_practices": [
        "Document the pattern usage and rationale in code comments for fallback_responses",
        "Validate implementation against domain requirements before deployment"
      ]
    }
  },
  "best_practices": [
    "Implement multiple defense layers",
    "Validate all inputs",
    "Filter all outputs",
    "Monitor guardrail triggers",
    "Log security events",
    "Test guardrails regularly",
    "Update guardrails as threats evolve",
    "Use multiple detection methods",
    "Handle edge cases",
    "Provide clear error messages",
    "Document guardrail policies",
    "Review guardrail effectiveness",
    "Train models on adversarial examples",
    "Use rate limiting",
    "Implement audit logging"
  ],
  "anti_patterns": [
    {
      "name": "Single Layer Defense",
      "problem": "Easy to bypass",
      "solution": "Implement multiple defense layers"
    },
    {
      "name": "No Input Validation",
      "problem": "Vulnerable to injection",
      "solution": "Validate and sanitize all inputs"
    },
    {
      "name": "Trusting Model Outputs",
      "problem": "Unsafe content can leak",
      "solution": "Always filter outputs"
    },
    {
      "name": "Hardcoded Guardrails",
      "problem": "Not adaptable",
      "solution": "Make guardrails configurable"
    }
  ]
}