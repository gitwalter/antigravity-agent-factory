{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "human-in-the-loop-patterns",
  "name": "Human-in-the-Loop Patterns",
  "title": "Human-in-the-Loop Patterns",
  "description": "Patterns for human oversight, approval workflows, escalation, and feedback collection in agent systems",
  "version": "1.0.0",
  "category": "agent-patterns",
  "axiomAlignment": {
    "A1_verifiability": "Approval logging and audit trails enable verifiable human decisions",
    "A2_user_primacy": "Interrupt points and proposal editing preserve user control over critical actions",
    "A3_transparency": "Context and confirmation levels make approval requirements explicit",
    "A4_non_harm": "Human approval gates prevent harmful automated actions",
    "A5_consistency": "Unified HITL patterns across interrupt_before, interrupt_after, and escalation"
  },
  "related_skills": [
    "human-in-the-loop",
    "state-management",
    "langgraph-agent-building"
  ],
  "related_knowledge": [
    "state-patterns.json",
    "langgraph-workflows.json",
    "memory-patterns.json"
  ],
  "patterns": {
    "interrupt_before": {
      "description": "Pause before executing action",
      "use_case": "Review before high-stakes actions",
      "implementation": "graph.compile(interrupt_before=['node_name'])",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "graph.compile(interrupt_before=['node_name'])",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "interrupt_after": {
      "description": "Pause after generating proposal",
      "use_case": "Review generated content",
      "implementation": "graph.compile(interrupt_after=['node_name'])",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "graph.compile(interrupt_after=['node_name'])",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "confirmation_levels_INFO": {
      "description": "Notify only, no response needed",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# confirmation_levels_INFO pattern for hitl-patterns\n# Implement based on description: Notify only, no response needed...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    },
    "confirmation_levels_CONFIRM": {
      "description": "Simple yes/no approval",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# confirmation_levels_CONFIRM pattern for hitl-patterns\n# Implement based on description: Simple yes/no approval...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    },
    "confirmation_levels_VERIFY": {
      "description": "Type confirmation phrase",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# confirmation_levels_VERIFY pattern for hitl-patterns\n# Implement based on description: Type confirmation phrase...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    },
    "escalation": {
      "levels": [
        "agent",
        "senior_agent",
        "human",
        "manager"
      ],
      "triggers": [
        "low confidence",
        "error",
        "sensitive operation"
      ],
      "pattern": "Progressive escalation based on criteria",
      "description": "Agent pattern for iterative reasoning and tool use with observation-based refinement.",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "feedback_collection_types": {
      "description": "['correctness', 'helpfulness', 'safety']",
      "use_when": "See description for when to apply this pattern.",
      "code_example": "See description for when to apply this pattern.",
      "best_practices": [
        "Review and validate implementation against domain requirements",
        "Review and validate implementation against domain requirements"
      ]
    },
    "feedback_collection_storage": {
      "description": "LangSmith feedback API",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# feedback_collection_storage pattern for hitl-patterns\n# Implement based on description: LangSmith feedback API...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    },
    "feedback_collection_use": {
      "description": "Continuous improvement",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# feedback_collection_use pattern for hitl-patterns\n# Implement based on description: Continuous improvement...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    }
  },
  "workflow_patterns": {
    "single_approval": {
      "description": "One approval point before execution",
      "flow": "propose -> interrupt -> execute",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "multi_step": {
      "description": "Approval at each step",
      "flow": "step1 -> approve -> step2 -> approve -> ...",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "conditional_approval": {
      "description": "Approval only for certain actions",
      "criteria": [
        "cost > threshold",
        "irreversible",
        "external"
      ],
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    }
  },
  "implementation": {
    "checkpointing": {
      "required": true,
      "reason": "Resume after human response"
    },
    "thread_id": {
      "required": true,
      "reason": "Identify workflow instance"
    },
    "timeout": {
      "recommended": true,
      "pattern": "Expire pending approvals after duration",
      "description": "Agent pattern for iterative reasoning and tool use with observation-based refinement.",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    }
  },
  "best_practices": [
    "Use checkpointing (Redis/PostgreSQL) for resumable workflows so human approvals don't block system restarts or require re-execution",
    "Implement timeouts for human responses (e.g., 24-48 hours) with escalation or auto-rejection to prevent workflows from hanging indefinitely",
    "Log all human decisions with timestamps, user IDs, and context for audit trails and compliance requirements",
    "Provide clear context for decisions including proposed action, current state, potential impact, and alternatives to enable informed choices",
    "Allow humans to modify proposals, not just approve/reject, enabling iterative refinement and better outcomes",
    "Design for async human response using interrupt points rather than blocking calls, allowing other workflows to continue processing",
    "Use appropriate confirmation levels (INFO/CONFIRM/VERIFY) based on risk: INFO for low-risk, VERIFY with confirmation phrase for high-risk actions",
    "Implement progressive escalation (agent -> senior_agent -> human -> manager) based on confidence levels, error rates, or sensitivity of operations"
  ],
  "anti_patterns": [
    {
      "name": "Blocking sync input",
      "problem": "Synchronous input() calls block the event loop, preventing other workflows from executing and causing system-wide hangs",
      "fix": "Use async interrupt points with LangGraph's interrupt_before/interrupt_after, implement async approval handlers with timeouts"
    },
    {
      "name": "No context for decisions",
      "problem": "Humans receive approval requests without sufficient context, leading to poor decisions or unnecessary rejections",
      "fix": "Include full workflow state, proposed changes, impact analysis, and alternatives in approval requests for informed decision-making"
    },
    {
      "name": "Binary approval only",
      "problem": "Only approve/reject options prevent humans from refining proposals, leading to suboptimal outcomes or repeated iterations",
      "fix": "Allow humans to edit proposals, provide feedback, or request modifications before final approval or rejection"
    },
    {
      "name": "No timeout handling",
      "problem": "Workflows wait indefinitely for human responses, blocking resources and preventing completion",
      "fix": "Implement configurable timeouts (e.g., 24-48 hours), auto-escalate or auto-reject after timeout, notify users of pending approvals"
    },
    {
      "name": "Missing checkpointing",
      "problem": "Workflows cannot resume after human approval if system restarts, losing progress and requiring full re-execution",
      "fix": "Require checkpointing (MemorySaver minimum, Redis/PostgreSQL for production) before implementing human-in-the-loop patterns"
    }
  ]
}