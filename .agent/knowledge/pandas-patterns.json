{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "pandas-patterns",
  "name": "Pandas Patterns",
  "title": "Pandas Patterns",
  "description": "Best practices and patterns for pandas data manipulation and analysis",
  "version": "1.0.0",
  "category": "integration",
  "axiomAlignment": {
    "A1_verifiability": "Patterns include data validation and verification techniques",
    "A2_user_primacy": "Data validation and type checking protect user data integrity",
    "A3_transparency": "All patterns emphasize reproducible data transformations",
    "A4_non_harm": "Explicit handling of missing values and outliers prevents harmful outputs",
    "A5_consistency": "Unified patterns for loading, cleaning, and transforming data"
  },
  "related_skills": [
    "data-pipeline",
    "sqlalchemy-patterns",
    "fastapi-development",
    "python-async"
  ],
  "related_knowledge": [
    "numpy-patterns.json",
    "polars-patterns.json",
    "data-patterns.json",
    "sqlalchemy-advanced.json"
  ],
  "data_loading": {
    "reading_csv": {
      "description": "Load data from CSV files",
      "use_when": "Working with CSV data files",
      "code_example": "import pandas as pd\n\n# Basic CSV reading\ndf = pd.read_csv('data.csv')\n\n# With options\ndf = pd.read_csv(\n    'data.csv',\n    sep=',',\n    header=0,\n    index_col=0,\n    usecols=['col1', 'col2', 'col3'],\n    dtype={'col1': 'int64', 'col2': 'float64'},\n    parse_dates=['date_col'],\n    na_values=['NA', 'N/A', '']\n)\n\n# Reading large files\n# Read in chunks\nchunk_size = 10000\nfor chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n    process(chunk)\n\n# Or use low_memory=False for better type inference\ndf = pd.read_csv('large_file.csv', low_memory=False)",
      "best_practices": [
        "Specify dtype to avoid type inference overhead",
        "Use parse_dates for date columns",
        "Specify na_values explicitly",
        "Use chunksize for very large files",
        "Set low_memory=False for better type inference"
      ]
    },
    "reading_excel": {
      "description": "Load data from Excel files",
      "use_when": "Working with Excel spreadsheets",
      "code_example": "import pandas as pd\n\n# Read Excel file\ndf = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n\n# Read multiple sheets\nall_sheets = pd.read_excel('data.xlsx', sheet_name=None)\ndf1 = all_sheets['Sheet1']\ndf2 = all_sheets['Sheet2']\n\n# With options\ndf = pd.read_excel(\n    'data.xlsx',\n    sheet_name='Sheet1',\n    header=0,\n    usecols='A:D',\n    skiprows=[0, 1],\n    nrows=1000\n)",
      "best_practices": [
        "Specify sheet_name explicitly",
        "Use usecols to read specific columns",
        "Handle header rows properly",
        "Consider openpyxl or xlrd engines"
      ]
    },
    "reading_json": {
      "description": "Load data from JSON files",
      "use_when": "Working with JSON data",
      "code_example": "import pandas as pd\n\n# Read JSON\ndf = pd.read_json('data.json')\n\n# JSON lines (one object per line)\ndf = pd.read_json('data.jsonl', lines=True)\n\n# Nested JSON\nimport json\nwith open('nested.json') as f:\n    data = json.load(f)\ndf = pd.json_normalize(data, 'records')\n\n# From URL\ndf = pd.read_json('https://api.example.com/data.json')",
      "best_practices": [
        "Use lines=True for JSONL format",
        "Use json_normalize for nested JSON",
        "Handle date parsing for timestamp columns"
      ]
    },
    "reading_parquet": {
      "description": "Load data from Parquet files",
      "use_when": "Working with columnar data format",
      "code_example": "import pandas as pd\n\n# Read Parquet\ndf = pd.read_parquet('data.parquet')\n\n# With engine\ndf = pd.read_parquet('data.parquet', engine='pyarrow')\n\n# Read specific columns\ndf = pd.read_parquet('data.parquet', columns=['col1', 'col2'])\n\n# Read specific row groups\nimport pyarrow.parquet as pq\nparquet_file = pq.ParquetFile('data.parquet')\ndf = parquet_file.read_row_group(0).to_pandas()",
      "best_practices": [
        "Parquet is efficient for large datasets",
        "Supports column pruning",
        "Preserves data types",
        "Use for analytical workloads"
      ]
    },
    "reading_sql": {
      "description": "Load data from SQL databases",
      "use_when": "Querying relational databases",
      "code_example": "import pandas as pd\nfrom sqlalchemy import create_engine\n\n# Create connection\nengine = create_engine('postgresql://user:pass@localhost/db')\n\n# Read SQL query\ndf = pd.read_sql('SELECT * FROM table', con=engine)\n\n# Read SQL table\ndf = pd.read_sql_table('table_name', con=engine)\n\n# With parameters (safe)\ndf = pd.read_sql(\n    'SELECT * FROM table WHERE id = :id',\n    con=engine,\n    params={'id': 123}\n)\n\n# Chunked reading for large results\nfor chunk in pd.read_sql('SELECT * FROM large_table', con=engine, chunksize=10000):\n    process(chunk)",
      "best_practices": [
        "Use parameterized queries for safety",
        "Use chunksize for large queries",
        "Close connections properly",
        "Use connection pooling for multiple queries"
      ]
    }
  },
  "data_cleaning": {
    "handling_missing_values": {
      "description": "Detect and handle missing values",
      "use_when": "Data contains NaN or null values",
      "code_example": "import pandas as pd\nimport numpy as np\n\n# Check for missing values\ndf.isna().sum()\ndf.isnull().sum()\n\n# Drop rows with any missing values\ndf_clean = df.dropna()\n\n# Drop rows where all values are missing\ndf_clean = df.dropna(how='all')\n\n# Drop columns with missing values\ndf_clean = df.dropna(axis=1)\n\n# Fill missing values\ndf['col'].fillna(0, inplace=True)\ndf['col'].fillna(df['col'].mean(), inplace=True)\ndf['col'].fillna(method='ffill', inplace=True)  # Forward fill\ndf['col'].fillna(method='bfill', inplace=True)  # Backward fill\n\n# Interpolate\ndf['col'].interpolate(method='linear', inplace=True)\n\n# Fill with group mean\ndf['col'].fillna(df.groupby('category')['col'].transform('mean'), inplace=True)",
      "best_practices": [
        "Understand why data is missing",
        "Use appropriate fill strategy",
        "Consider dropping if too many missing",
        "Document missing value handling",
        "Use groupby for conditional filling"
      ]
    },
    "removing_duplicates": {
      "description": "Identify and remove duplicate rows",
      "use_when": "Dataset contains duplicate records",
      "code_example": "import pandas as pd\n\n# Check for duplicates\ndf.duplicated().sum()\n\n# Remove duplicates\ndf_unique = df.drop_duplicates()\n\n# Remove duplicates based on specific columns\ndf_unique = df.drop_duplicates(subset=['col1', 'col2'])\n\n# Keep first or last occurrence\ndf_unique = df.drop_duplicates(keep='first')\ndf_unique = df.drop_duplicates(keep='last')\n\n# Mark duplicates without removing\ndf['is_duplicate'] = df.duplicated()",
      "best_practices": [
        "Check duplicates before removing",
        "Understand which duplicates to keep",
        "Use subset parameter for specific columns",
        "Document duplicate removal strategy"
      ]
    },
    "data_type_conversion": {
      "description": "Convert data types appropriately",
      "use_when": "Data types are incorrect or need optimization",
      "code_example": "import pandas as pd\n\n# Convert to numeric\ndf['col'] = pd.to_numeric(df['col'], errors='coerce')\n\n# Convert to datetime\ndf['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n\n# Convert to category (memory efficient)\ndf['category'] = df['category'].astype('category')\n\n# Convert to string\ndf['col'] = df['col'].astype(str)\n\n# Convert multiple columns\ndf[['col1', 'col2']] = df[['col1', 'col2']].astype(float)\n\n# Downcast numeric types\ndf['int_col'] = pd.to_numeric(df['int_col'], downcast='integer')\ndf['float_col'] = pd.to_numeric(df['float_col'], downcast='float')",
      "best_practices": [
        "Use category dtype for low-cardinality strings",
        "Downcast numeric types to save memory",
        "Handle conversion errors appropriately",
        "Use pd.to_datetime for date parsing",
        "Convert types early in pipeline"
      ]
    },
    "outlier_detection": {
      "description": "Identify and handle outliers",
      "use_when": "Data contains extreme values",
      "code_example": "import pandas as pd\nimport numpy as np\n\n# Z-score method\nz_scores = np.abs((df['col'] - df['col'].mean()) / df['col'].std())\noutliers = df[z_scores > 3]\n\n# IQR method\nQ1 = df['col'].quantile(0.25)\nQ3 = df['col'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\noutliers = df[(df['col'] < lower_bound) | (df['col'] > upper_bound)]\n\n# Remove outliers\ndf_clean = df[(df['col'] >= lower_bound) & (df['col'] <= upper_bound)]\n\n# Cap outliers\ndf['col'] = df['col'].clip(lower=lower_bound, upper=upper_bound)",
      "best_practices": [
        "Understand domain before removing outliers",
        "Use IQR method for skewed distributions",
        "Consider capping instead of removing",
        "Document outlier handling strategy"
      ]
    },
    "string_cleaning": {
      "description": "Clean and normalize string data",
      "use_when": "String columns need cleaning",
      "code_example": "import pandas as pd\n\n# Strip whitespace\ndf['col'] = df['col'].str.strip()\n\n# Convert to lowercase\ndf['col'] = df['col'].str.lower()\n\n# Remove special characters\ndf['col'] = df['col'].str.replace('[^a-zA-Z0-9]', '', regex=True)\n\n# Replace values\ndf['col'] = df['col'].str.replace('old', 'new')\n\n# Extract patterns\ndf['email'] = df['text'].str.extract(r'([\\w\\.-]+@[\\w\\.-]+)')\n\n# Split strings\ndf[['first', 'last']] = df['name'].str.split(' ', expand=True)\n\n# Check contains\ndf['has_keyword'] = df['col'].str.contains('keyword', case=False)",
      "best_practices": [
        "Use vectorized string operations",
        "Chain string operations efficiently",
        "Handle NaN values in string operations",
        "Use regex for complex patterns"
      ]
    }
  },
  "transformation_patterns": {
    "filtering": {
      "description": "Filter rows based on conditions",
      "use_when": "Need to subset data",
      "code_example": "import pandas as pd\n\n# Single condition\ndf_filtered = df[df['col'] > 100]\n\n# Multiple conditions\ndf_filtered = df[(df['col1'] > 100) & (df['col2'] == 'value')]\n\n# Using query method\ndf_filtered = df.query('col1 > 100 and col2 == \"value\"')\n\n# Using isin\ndf_filtered = df[df['col'].isin(['val1', 'val2', 'val3'])]\n\n# Using between\ndf_filtered = df[df['col'].between(10, 100)]\n\n# Using str methods\ndf_filtered = df[df['col'].str.contains('pattern', case=False)]\n\n# Using notna\ndf_filtered = df[df['col'].notna()]",
      "best_practices": [
        "Use & and | with parentheses",
        "Use query() for complex conditions",
        "Use vectorized operations",
        "Avoid chained indexing"
      ]
    },
    "groupby_operations": {
      "description": "Group data and apply operations",
      "use_when": "Need aggregate operations by groups",
      "code_example": "import pandas as pd\n\n# Basic groupby\ngrouped = df.groupby('category')\n\n# Aggregate functions\ndf.groupby('category')['value'].sum()\ndf.groupby('category')['value'].mean()\ndf.groupby('category')['value'].agg(['sum', 'mean', 'std'])\n\n# Multiple columns\ndf.groupby(['category', 'subcategory'])['value'].sum()\n\n# Custom aggregation\ndf.groupby('category')['value'].agg(['sum', lambda x: x.max() - x.min()])\n\n# Named aggregations\ndf.groupby('category').agg(\n    total=('value', 'sum'),\n    average=('value', 'mean'),\n    count=('value', 'count')\n)\n\n# Transform (same shape as original)\ndf['group_mean'] = df.groupby('category')['value'].transform('mean')\n\n# Apply custom function\ndef custom_func(group):\n    return group['value'].sum() / group['count'].sum()\n\ndf.groupby('category').apply(custom_func)",
      "best_practices": [
        "Use named aggregations for clarity",
        "Use transform for same-shape results",
        "Avoid apply() when possible (use vectorized operations)",
        "Use as_index=False to keep group column"
      ]
    },
    "pivot_tables": {
      "description": "Create pivot tables for data reshaping",
      "use_when": "Need to reshape data for analysis",
      "code_example": "import pandas as pd\n\n# Basic pivot table\npivot = df.pivot_table(\n    values='value',\n    index='row_category',\n    columns='col_category',\n    aggfunc='sum'\n)\n\n# Multiple aggregation functions\npivot = df.pivot_table(\n    values='value',\n    index='row_category',\n    columns='col_category',\n    aggfunc=['sum', 'mean']\n)\n\n# With margins\npivot = df.pivot_table(\n    values='value',\n    index='row_category',\n    columns='col_category',\n    aggfunc='sum',\n    margins=True\n)\n\n# Fill missing values\npivot = df.pivot_table(\n    values='value',\n    index='row_category',\n    columns='col_category',\n    aggfunc='sum',\n    fill_value=0\n)",
      "best_practices": [
        "Use pivot_table for flexible reshaping",
        "Specify aggfunc explicitly",
        "Use fill_value for missing combinations",
        "Consider margins for totals"
      ]
    },
    "merging_dataframes": {
      "description": "Combine multiple dataframes",
      "use_when": "Need to join data from different sources",
      "code_example": "import pandas as pd\n\n# Inner join\ndf_merged = pd.merge(df1, df2, on='key', how='inner')\n\n# Left join\ndf_merged = pd.merge(df1, df2, on='key', how='left')\n\n# Right join\ndf_merged = pd.merge(df1, df2, on='key', how='right')\n\n# Outer join\ndf_merged = pd.merge(df1, df2, on='key', how='outer')\n\n# Multiple keys\ndf_merged = pd.merge(df1, df2, on=['key1', 'key2'])\n\n# Different column names\ndf_merged = pd.merge(df1, df2, left_on='key1', right_on='key2')\n\n# Using suffixes\ndf_merged = pd.merge(df1, df2, on='key', suffixes=('_left', '_right'))\n\n# Concatenate\ndf_concat = pd.concat([df1, df2], axis=0)  # Rows\ndf_concat = pd.concat([df1, df2], axis=1)  # Columns",
      "best_practices": [
        "Understand join types (inner, left, right, outer)",
        "Check for duplicate keys before merging",
        "Use suffixes for overlapping columns",
        "Use concat for simple stacking"
      ]
    },
    "applying_functions": {
      "description": "Apply functions to data",
      "use_when": "Need custom transformations",
      "code_example": "import pandas as pd\nimport numpy as np\n\n# Apply to series\ndf['col'] = df['col'].apply(lambda x: x * 2)\n\n# Apply to dataframe rows\ndf['result'] = df.apply(lambda row: row['col1'] + row['col2'], axis=1)\n\n# Apply to dataframe columns\ndf.apply(lambda col: col.sum(), axis=0)\n\n# Using map\ndf['category'] = df['code'].map({1: 'A', 2: 'B', 3: 'C'})\n\n# Using applymap (element-wise)\ndf.applymap(lambda x: x * 2)\n\n# Vectorized operations (preferred)\ndf['result'] = df['col1'] + df['col2']\ndf['result'] = np.sqrt(df['col'])",
      "best_practices": [
        "Prefer vectorized operations over apply()",
        "Use apply() only when necessary",
        "Use map() for simple value mapping",
        "Avoid apply() in loops"
      ]
    }
  },
  "aggregation_patterns": {
    "basic_aggregations": {
      "description": "Compute summary statistics",
      "use_when": "Need descriptive statistics",
      "code_example": "import pandas as pd\n\n# Basic statistics\ndf.describe()\n\n# Individual statistics\ndf['col'].sum()\ndf['col'].mean()\ndf['col'].median()\ndf['col'].std()\ndf['col'].var()\ndf['col'].min()\ndf['col'].max()\ndf['col'].quantile(0.25)\ndf['col'].count()\n\n# Multiple columns\ndf[['col1', 'col2']].mean()\n\n# All columns\ndf.mean()",
      "best_practices": [
        "Use describe() for quick overview",
        "Understand what each statistic means",
        "Consider median for skewed distributions"
      ]
    },
    "window_functions": {
      "description": "Compute rolling and expanding statistics",
      "use_when": "Need time-series or rolling calculations",
      "code_example": "import pandas as pd\n\n# Rolling mean\ndf['rolling_mean'] = df['value'].rolling(window=7).mean()\n\n# Rolling sum\ndf['rolling_sum'] = df['value'].rolling(window=7).sum()\n\n# Expanding mean\ndf['expanding_mean'] = df['value'].expanding().mean()\n\n# Custom window function\ndef custom_func(x):\n    return x.max() - x.min()\n\ndf['rolling_range'] = df['value'].rolling(window=7).apply(custom_func)\n\n# Time-based rolling\ndf['value'].rolling('7D').mean()  # 7 days",
      "best_practices": [
        "Use rolling() for fixed windows",
        "Use expanding() for cumulative calculations",
        "Handle NaN values at start",
        "Use time-based windows for time series"
      ]
    },
    "cumulative_operations": {
      "description": "Compute cumulative statistics",
      "use_when": "Need running totals or cumulative values",
      "code_example": "import pandas as pd\n\n# Cumulative sum\ndf['cumsum'] = df['value'].cumsum()\n\n# Cumulative product\ndf['cumprod'] = df['value'].cumprod()\n\n# Cumulative max/min\ndf['cummax'] = df['value'].cummax()\ndf['cummin'] = df['value'].cummin()\n\n# Grouped cumulative\ndf['group_cumsum'] = df.groupby('category')['value'].cumsum()",
      "best_practices": [
        "Use cumsum() for running totals",
        "Use cummax/cummin for running extremes",
        "Consider groupby for grouped cumulative"
      ]
    }
  },
  "memory_optimization": {
    "dtype_optimization": {
      "description": "Optimize data types to reduce memory",
      "use_when": "Working with large datasets",
      "code_example": "import pandas as pd\nimport numpy as np\n\n# Check memory usage\ndf.info(memory_usage='deep')\ndf.memory_usage(deep=True)\n\n# Downcast integers\ndf['int_col'] = pd.to_numeric(df['int_col'], downcast='integer')\n\n# Downcast floats\ndf['float_col'] = pd.to_numeric(df['float_col'], downcast='float')\n\n# Convert to category\ndf['category'] = df['category'].astype('category')\n\n# Use sparse dataframes\ndf_sparse = df.astype(pd.SparseDtype('float64'))",
      "best_practices": [
        "Use category dtype for low-cardinality strings",
        "Downcast numeric types",
        "Use SparseDtype for mostly zero data",
        "Monitor memory usage regularly"
      ]
    },
    "chunked_processing": {
      "description": "Process data in chunks",
      "use_when": "Dataset doesn't fit in memory",
      "code_example": "import pandas as pd\n\n# Read in chunks\nchunk_size = 10000\nresults = []\n\nfor chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n    # Process chunk\n    processed = chunk.groupby('category')['value'].sum()\n    results.append(processed)\n\n# Combine results\nfinal_result = pd.concat(results).groupby('category')['value'].sum()",
      "best_practices": [
        "Use chunksize parameter",
        "Process chunks independently when possible",
        "Combine results appropriately",
        "Consider dask for very large datasets"
      ]
    },
    "lazy_evaluation": {
      "description": "Use lazy evaluation with query()",
      "use_when": "Need to optimize query performance",
      "code_example": "import pandas as pd\n\n# Query with index\ndf_indexed = df.set_index('key')\nresult = df_indexed.query('key > 100')\n\n# Use eval() for complex expressions\ndf['result'] = df.eval('col1 + col2 * col3')",
      "best_practices": [
        "Set index for faster lookups",
        "Use query() for complex conditions",
        "Use eval() for complex expressions",
        "Consider dask for out-of-core processing"
      ]
    }
  },
  "anti_patterns": [
    {
      "name": "Chained assignment causing SettingWithCopyWarning",
      "problem": "Unpredictable behavior, warnings, and potential data corruption",
      "solution": "Use .loc[] or .iloc[] for assignment, or call .copy() explicitly before modifying"
    },
    {
      "name": "Iterating over DataFrame rows with for loops",
      "problem": "Extremely slow (100x slower than vectorized operations), not pandas-idiomatic",
      "solution": "Use vectorized operations, apply(), or list comprehensions instead of row iteration"
    },
    {
      "name": "Using object dtype for categorical data",
      "problem": "High memory usage (2-10x more than necessary) and slower operations",
      "solution": "Convert low-cardinality string columns to category dtype using .astype('category')"
    },
    {
      "name": "Not specifying dtypes when reading files",
      "problem": "Slow file reading and high memory usage due to type inference",
      "solution": "Specify dtype parameter when reading CSV/Excel files to avoid inference overhead"
    },
    {
      "name": "Modifying DataFrame during iteration",
      "problem": "Unpredictable results and potential data corruption",
      "solution": "Create new DataFrame with modifications or use vectorized operations"
    }
  ],
  "best_practices_summary": [
    "Use vectorized operations instead of loops",
    "Specify dtypes when reading files",
    "Use category dtype for low-cardinality strings",
    "Avoid chained assignment",
    "Use .loc[] and .iloc[] for indexing",
    "Handle missing values explicitly",
    "Use groupby for aggregations",
    "Process large files in chunks",
    "Use appropriate join types",
    "Document data transformations"
  ],
  "patterns": {
    "general_description": {
      "description": "Best practices and patterns for pandas data manipulation and analysis",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# general_description pattern for pandas-patterns\n# Implement based on description: Best practices and patterns for pandas data manipu...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    },
    "general_usage": {
      "description": "See detailed sections below",
      "use_when": "Apply when implementing this pattern in your domain context",
      "code_example": "# general_usage pattern for pandas-patterns\n# Implement based on description: See detailed sections below...",
      "best_practices": [
        "Validate implementation against domain requirements",
        "Document the pattern usage and rationale in code"
      ]
    }
  },
  "best_practices": [
    "Use vectorized operations instead of iterating with apply() for better performance - 10-100x faster",
    "Prefer query() method for filtering with complex boolean conditions - more readable and efficient",
    "Use categorical dtype for columns with limited unique values to reduce memory usage by 50-90%",
    "Specify dtypes when reading CSV/Excel files to avoid type inference overhead and memory waste",
    "Use .loc[] or .iloc[] for assignment to avoid SettingWithCopyWarning - never use chained indexing",
    "Process large files in chunks using chunksize parameter - don't load entire dataset into memory",
    "Use groupby().transform() when you need same-shape results as original DataFrame",
    "Downcast numeric types (int64->int32, float64->float32) when precision allows to save memory",
    "Use pd.to_datetime() with explicit format strings for faster date parsing",
    "Always check for duplicates with duplicated().sum() before removing - understand what you're deleting"
  ]
}