{
  "id": "speech-patterns",
  "name": "Speech Processing Patterns",
  "version": "1.0.0",
  "category": "audio-processing",
  "description": "Patterns for speech-to-text transcription, text-to-speech synthesis, real-time transcription, and audio analysis",
  "patterns": {
    "speech_to_text": {
      "whisper": {
        "description": "OpenAI Whisper - offline, high-accuracy transcription",
        "use_when": [
          "Offline processing",
          "High accuracy needed",
          "Multiple languages"
        ],
        "model_sizes": [
          "tiny",
          "base",
          "small",
          "medium",
          "large"
        ],
        "pros": [
          "Very high accuracy",
          "99+ languages",
          "Free",
          "Offline"
        ],
        "cons": [
          "Slower than cloud APIs",
          "GPU recommended for large models"
        ]
      },
      "google_cloud_speech": {
        "description": "Google Cloud Speech-to-Text API",
        "use_when": [
          "Real-time transcription",
          "Highest accuracy",
          "Cloud infrastructure"
        ],
        "pros": [
          "Very high accuracy",
          "Real-time streaming",
          "Word timestamps"
        ],
        "cons": [
          "Requires internet",
          "Paid service",
          "API rate limits"
        ]
      },
      "azure_speech": {
        "description": "Azure Cognitive Services Speech",
        "use_when": [
          "Azure ecosystem",
          "Multi-language support",
          "Custom models"
        ],
        "pros": [
          "High accuracy",
          "Custom models",
          "Speaker diarization"
        ],
        "cons": [
          "Paid service",
          "Requires internet"
        ]
      },
      "description": "Pattern speech_to_text for speech-patterns",
      "use_when": "When implementing speech_to_text",
      "code_example": "// Example for speech_to_text",
      "best_practices": [
        "Use appropriately for best results.",
        "Monitor results and optimize."
      ]
    },
    "text_to_speech": {
      "gtts": {
        "description": "Google Text-to-Speech (free, requires internet)",
        "use_when": [
          "Simple TTS",
          "Multiple languages",
          "Free solution"
        ],
        "pros": [
          "Free",
          "100+ languages",
          "Easy to use"
        ],
        "cons": [
          "Requires internet",
          "Limited voice options"
        ]
      },
      "pyttsx3": {
        "description": "Offline TTS using system voices",
        "use_when": [
          "Offline operation",
          "System integration",
          "No internet"
        ],
        "pros": [
          "Offline",
          "Free",
          "System voices"
        ],
        "cons": [
          "Lower quality",
          "Limited languages"
        ]
      },
      "azure_speech_synthesis": {
        "description": "Azure Speech Synthesis with neural voices",
        "use_when": [
          "High quality needed",
          "Natural voices",
          "Production apps"
        ],
        "pros": [
          "High quality",
          "Natural voices",
          "SSML support"
        ],
        "cons": [
          "Paid service",
          "Requires internet"
        ]
      },
      "description": "Pattern text_to_speech for speech-patterns",
      "use_when": "When implementing text_to_speech",
      "code_example": "// Example for text_to_speech",
      "best_practices": [
        "Use appropriately for best results.",
        "Monitor results and optimize."
      ]
    },
    "real_time_transcription": {
      "streaming_whisper": {
        "description": "Real-time transcription using Whisper with audio chunks",
        "pattern": "Record audio in chunks, transcribe each chunk, combine results",
        "chunk_duration": "5-10 seconds recommended"
      },
      "voice_activity_detection": {
        "description": "Detect speech segments to skip silence",
        "library": "webrtcvad",
        "pattern": "Process audio frames, detect speech, only transcribe speech segments",
        "use_when": [
          "Long audio files",
          "Reducing processing time"
        ]
      },
      "continuous_streaming": {
        "description": "Continuous audio stream transcription",
        "pattern": "Use threading for parallel recording/transcription, maintain audio buffer"
      },
      "description": "Pattern real_time_transcription for speech-patterns",
      "use_when": "When implementing real_time_transcription",
      "code_example": "// Example for real_time_transcription",
      "best_practices": [
        "Use appropriately for best results.",
        "Monitor results and optimize."
      ]
    },
    "audio_analysis": {
      "silence_detection": {
        "description": "Detect silent segments in audio",
        "library": "pydub",
        "pattern": "split_on_silence with threshold and min_silence_len",
        "use_when": [
          "Segmenting audio",
          "Removing silence"
        ]
      },
      "normalization": {
        "description": "Normalize audio levels to target dBFS",
        "pattern": "Calculate current dBFS, apply gain to reach target",
        "target_dBFS": "-20.0 dBFS recommended"
      },
      "segmentation": {
        "description": "Split audio into fixed-duration segments",
        "pattern": "Iterate through audio, extract segments of fixed duration",
        "use_when": [
          "Batch processing",
          "Chunking for models"
        ]
      },
      "audio_properties": {
        "description": "Analyze audio file properties",
        "properties": [
          "duration",
          "sample_rate",
          "channels",
          "rms",
          "dBFS"
        ]
      },
      "description": "Pattern audio_analysis for speech-patterns",
      "use_when": "When implementing audio_analysis",
      "code_example": "// Example for audio_analysis",
      "best_practices": [
        "Use appropriately for best results.",
        "Monitor results and optimize."
      ]
    },
    "word_timestamps": {
      "whisper_timestamps": {
        "description": "Get word-level timestamps from Whisper",
        "pattern": "Enable word_timestamps=True, extract from segments",
        "use_when": [
          "Precise alignment",
          "Subtitle generation"
        ]
      },
      "google_cloud_timestamps": {
        "description": "Word-level timestamps from Google Cloud Speech",
        "pattern": "Enable enable_word_time_offsets, extract from results",
        "use_when": [
          "High precision needed",
          "Cloud processing"
        ]
      },
      "description": "Pattern word_timestamps for speech-patterns",
      "use_when": "When implementing word_timestamps",
      "code_example": "// Example for word_timestamps",
      "best_practices": [
        "Use appropriately for best results.",
        "Monitor results and optimize."
      ]
    }
  },
  "tool_comparison": {
    "whisper": {
      "type": "STT",
      "accuracy": "Very High",
      "languages": "99+",
      "cost": "Free",
      "offline": true
    },
    "google_cloud_speech": {
      "type": "STT",
      "accuracy": "Very High",
      "languages": "50+",
      "cost": "Paid",
      "offline": false
    },
    "azure_speech": {
      "type": "STT/TTS",
      "accuracy": "Very High",
      "languages": "50+",
      "cost": "Paid",
      "offline": false
    },
    "gtts": {
      "type": "TTS",
      "accuracy": "High",
      "languages": "100+",
      "cost": "Free",
      "offline": false
    },
    "pyttsx3": {
      "type": "TTS",
      "accuracy": "Medium",
      "languages": "System",
      "cost": "Free",
      "offline": true
    }
  },
  "best_practices": [
    "Use Whisper for offline, high-accuracy transcription",
    "Preprocess audio (normalize, remove noise) before transcription",
    "Use voice activity detection to skip silent segments",
    "Choose appropriate model size based on accuracy vs. speed needs",
    "Cache transcriptions for repeated audio files",
    "Use word-level timestamps for precise alignment",
    "Normalize audio levels before processing",
    "Handle multiple speakers with speaker diarization when needed",
    "Convert audio to required sample rate (16kHz for Whisper)",
    "Use appropriate chunk sizes for real-time processing (5-10 seconds)"
  ],
  "anti_patterns": [
    {
      "name": "No audio preprocessing",
      "problem": "Lower transcription accuracy",
      "fix": "Normalize, denoise before transcription"
    },
    {
      "name": "Wrong sample rate",
      "problem": "Poor transcription quality",
      "fix": "Convert to model's required rate (16kHz for Whisper)"
    },
    {
      "name": "Processing entire file",
      "problem": "Slow, inefficient",
      "fix": "Use VAD to segment speech regions"
    },
    {
      "name": "Ignoring confidence scores",
      "problem": "Incorrect transcriptions included",
      "fix": "Filter low-confidence transcriptions"
    },
    {
      "name": "No language specification",
      "problem": "Lower accuracy on non-English",
      "fix": "Detect or specify language for better accuracy"
    },
    {
      "name": "Large model for simple tasks",
      "problem": "Unnecessary slow processing",
      "fix": "Use smaller models (base/tiny) when speed matters"
    },
    {
      "name": "No error handling",
      "problem": "Crashes on API failures or format errors",
      "fix": "Handle API failures and audio format errors gracefully"
    }
  ],
  "related_skills": [
    "speech-processing",
    "vision-agents",
    "applying-rag-patterns",
    "ocr-processing"
  ],
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Speech-patterns Knowledge",
  "axiomAlignment": {
    "A1_verifiability": "Patterns are verified through automated testing.",
    "A2_user_primacy": "The user maintains control over all generated output.",
    "A3_transparency": "All automated actions are logged and verifiable.",
    "A4_non_harm": "Strict safety checks prevent destructive operations.",
    "A5_consistency": "Uniform patterns ensure predictable system behavior."
  },
  "related_knowledge": [
    "manifest.json"
  ]
}
