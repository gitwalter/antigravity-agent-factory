{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "agent-memory-patterns-2026",
  "name": "Agent Memory Patterns 2026",
  "title": "Agent Memory Patterns 2026",
  "description": "Modern agent memory architectures including Mem0, EverMemOS, graph memory, and long-horizon reasoning patterns for persistent agent knowledge",
  "version": "1.0.0",
  "last_updated": "2026-02-11",
  "category": "agent-patterns",
  "axiomAlignment": {
    "A1_verifiability": "Memory retrieval and provenance enable verification",
    "A2_user_primacy": "User-scoped memory respects data ownership",
    "A3_transparency": "Memory operations are logged and inspectable",
    "A4_non_harm": "Memory sanitization prevents harmful data persistence",
    "A5_consistency": "Unified memory patterns across agent frameworks"
  },
  "related_skills": [
    "memory-management",
    "rag-patterns",
    "knowledge-graphs"
  ],
  "related_knowledge": [
    "langchain-patterns.json",
    "langgraph-workflows.json",
    "rag-patterns.json"
  ],
  "patterns": {
    "mem0_integration": {
      "description": "Mem0 - production-ready memory layer for AI agents",
      "use_when": "Agents need persistent, structured memory across sessions",
      "code_example": "from mem0 import MemoryClient\nimport os\n\n# Initialize Mem0 client\nclient = MemoryClient(api_key=os.environ['MEM0_API_KEY'])\n\n# Add memory for a user\nclient.add(\n    messages=[\n        {'role': 'user', 'content': 'My name is Alice and I work at TechCorp'},\n        {'role': 'assistant', 'content': 'Nice to meet you, Alice!'}\n    ],\n    user_id='alice_123',\n    metadata={'session': 'onboarding'}\n)\n\n# Search memories\nmemories = client.search(\n    query='What company does Alice work at?',\n    user_id='alice_123',\n    limit=5\n)\nfor mem in memories:\n    print(f'{mem.content} (score: {mem.score})')\n\n# Get all memories for user\nall_memories = client.get_all(user_id='alice_123')\n\n# Use with LangChain\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\nllm = ChatOpenAI(model='gpt-4')\n\ndef chat_with_memory(user_id: str, message: str) -> str:\n    # Retrieve relevant memories\n    memories = client.search(query=message, user_id=user_id, limit=5)\n    memory_context = '\\n'.join([m.content for m in memories])\n    \n    prompt = ChatPromptTemplate.from_messages([\n        ('system', f'You are a helpful assistant. User context:\\n{memory_context}'),\n        ('user', '{message}')\n    ])\n    \n    response = (prompt | llm).invoke({'message': message})\n    \n    # Store interaction in memory\n    client.add(\n        messages=[\n            {'role': 'user', 'content': message},\n            {'role': 'assistant', 'content': response.content}\n        ],\n        user_id=user_id\n    )\n    \n    return response.content",
      "best_practices": [
        "Use user_id for multi-tenant memory isolation",
        "Add metadata for memory categorization",
        "Search before generating to include context",
        "Store important interactions for future reference"
      ],
      "key_features": {
        "multi_level": "User, session, and agent-level memories",
        "hybrid_storage": "Combines vector and graph storage",
        "auto_extraction": "Automatically extracts facts from conversations",
        "conflict_resolution": "Handles contradictory information"
      }
    },
    "salience_extraction": {
      "description": "Extract salient (important) information from conversations",
      "use_when": "Filtering noise to retain only meaningful memories",
      "code_example": "from langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass SalientFact(BaseModel):\n    content: str = Field(description='The extracted fact')\n    category: str = Field(description='Category: preference, fact, relationship, goal')\n    importance: float = Field(ge=0, le=1, description='Importance score')\n    entities: List[str] = Field(default=[], description='Named entities involved')\n\nclass SalienceExtraction(BaseModel):\n    facts: List[SalientFact]\n    should_remember: bool = Field(description='Whether this contains memorable info')\n\nllm = ChatOpenAI(model='gpt-4').with_structured_output(SalienceExtraction)\n\ndef extract_salience(conversation: str) -> SalienceExtraction:\n    return llm.invoke(\n        f'''Extract salient facts from this conversation that should be remembered long-term.\n        Focus on:\n        - User preferences and dislikes\n        - Important facts about people, places, projects\n        - Goals and intentions\n        - Relationships between entities\n        \n        Conversation:\n        {conversation}'''\n    )\n\n# Filter by importance\ndef filter_important(extraction: SalienceExtraction, threshold: float = 0.5) -> List[SalientFact]:\n    return [f for f in extraction.facts if f.importance >= threshold]",
      "best_practices": [
        "Extract facts immediately after conversation turns",
        "Filter by importance threshold to reduce noise",
        "Categorize facts for efficient retrieval",
        "Extract entities for relationship building"
      ]
    },
    "memory_consolidation": {
      "description": "Consolidate and deduplicate memories over time",
      "use_when": "Managing memory growth and removing redundancy",
      "code_example": "from langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nimport json\n\nclass ConsolidatedMemory(BaseModel):\n    content: str\n    source_ids: List[str]\n    confidence: float\n\ndef consolidate_memories(memories: List[dict]) -> List[ConsolidatedMemory]:\n    \"\"\"Merge similar memories and remove duplicates.\"\"\"\n    llm = ChatOpenAI(model='gpt-4').with_structured_output(List[ConsolidatedMemory])\n    \n    return llm.invoke(\n        f'''Consolidate these memories by:\n        1. Merging similar or overlapping information\n        2. Removing exact duplicates\n        3. Updating outdated info with newer facts\n        4. Resolving contradictions (prefer newer info)\n        \n        Memories:\n        {json.dumps(memories, indent=2)}\n        \n        Return consolidated memories with source_ids tracking which original memories were merged.'''\n    )\n\n# Schedule periodic consolidation\nasync def periodic_consolidation(memory_store, user_id: str):\n    memories = memory_store.get_all(user_id=user_id)\n    if len(memories) > 100:  # Consolidate when too many\n        consolidated = consolidate_memories([m.to_dict() for m in memories])\n        \n        # Replace with consolidated memories\n        for mem in memories:\n            memory_store.delete(mem.id)\n        for consolidated_mem in consolidated:\n            memory_store.add(consolidated_mem)",
      "best_practices": [
        "Consolidate periodically (daily, weekly)",
        "Track source IDs for provenance",
        "Prefer newer information in conflicts",
        "Set memory limits before triggering consolidation"
      ]
    },
    "graph_memory": {
      "description": "Store memories as knowledge graph for relationship queries",
      "use_when": "Need to query relationships between entities",
      "code_example": "from neo4j import GraphDatabase\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel\nfrom typing import List, Tuple\n\nclass Entity(BaseModel):\n    name: str\n    type: str  # person, company, project, concept\n    properties: dict = {}\n\nclass Relationship(BaseModel):\n    source: str\n    relation: str  # works_at, knows, owns, part_of\n    target: str\n    properties: dict = {}\n\nclass KnowledgeGraph:\n    def __init__(self, uri: str, user: str, password: str):\n        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n    \n    def add_entity(self, user_id: str, entity: Entity):\n        with self.driver.session() as session:\n            session.run(\n                '''MERGE (e:Entity {name: $name, user_id: $user_id})\n                SET e.type = $type, e += $properties''',\n                name=entity.name, user_id=user_id,\n                type=entity.type, properties=entity.properties\n            )\n    \n    def add_relationship(self, user_id: str, rel: Relationship):\n        with self.driver.session() as session:\n            session.run(\n                f'''MATCH (a:Entity {{name: $source, user_id: $user_id}})\n                MATCH (b:Entity {{name: $target, user_id: $user_id}})\n                MERGE (a)-[r:{rel.relation}]->(b)\n                SET r += $properties''',\n                source=rel.source, target=rel.target,\n                user_id=user_id, properties=rel.properties\n            )\n    \n    def query_relationships(self, user_id: str, entity: str) -> List[dict]:\n        with self.driver.session() as session:\n            result = session.run(\n                '''MATCH (e:Entity {name: $entity, user_id: $user_id})-[r]-(connected)\n                RETURN type(r) as relation, connected.name as entity, connected.type as type''',\n                entity=entity, user_id=user_id\n            )\n            return [dict(record) for record in result]\n\n# Extract entities and relationships from text\ndef extract_knowledge(text: str) -> Tuple[List[Entity], List[Relationship]]:\n    llm = ChatOpenAI(model='gpt-4')\n    # ... extraction logic ...\n    pass",
      "best_practices": [
        "Use Neo4j or similar for graph storage",
        "Scope entities by user_id for isolation",
        "Extract relationships during salience extraction",
        "Query graph for context enrichment"
      ]
    },
    "semantic_grouping": {
      "description": "Group related memories by semantic topic",
      "use_when": "Organizing memories for efficient retrieval",
      "code_example": "from sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nclass SemanticMemoryOrganizer:\n    def __init__(self, n_clusters: int = 10):\n        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n        self.n_clusters = n_clusters\n    \n    def cluster_memories(self, memories: List[str]) -> dict:\n        # Encode all memories\n        embeddings = self.encoder.encode(memories)\n        \n        # Cluster\n        kmeans = KMeans(n_clusters=min(self.n_clusters, len(memories)))\n        labels = kmeans.fit_predict(embeddings)\n        \n        # Group by cluster\n        clusters = {}\n        for i, label in enumerate(labels):\n            if label not in clusters:\n                clusters[label] = []\n            clusters[label].append(memories[i])\n        \n        return clusters\n    \n    def get_cluster_topics(self, clusters: dict) -> dict:\n        \"\"\"Generate topic labels for clusters.\"\"\"\n        topics = {}\n        for cluster_id, memories in clusters.items():\n            # Use LLM to generate topic\n            sample = memories[:5]  # Sample for topic generation\n            topic = self.generate_topic(sample)\n            topics[cluster_id] = topic\n        return topics\n    \n    def generate_topic(self, memories: List[str]) -> str:\n        from langchain_openai import ChatOpenAI\n        llm = ChatOpenAI(model='gpt-4')\n        return llm.invoke(\n            f'Generate a short topic label (2-4 words) for these memories:\\n' +\n            '\\n'.join(memories)\n        ).content",
      "best_practices": [
        "Cluster periodically as memories grow",
        "Use cluster topics for navigation",
        "Retrieve from relevant clusters first",
        "Adjust n_clusters based on memory volume"
      ]
    },
    "reconstructive_retrieval": {
      "description": "Reconstruct context from multiple memory sources",
      "use_when": "Building comprehensive context from fragmented memories",
      "code_example": "from langchain_openai import ChatOpenAI\nfrom typing import List\n\nclass ReconstructiveMemory:\n    def __init__(self, vector_store, graph_store, llm):\n        self.vector_store = vector_store\n        self.graph_store = graph_store\n        self.llm = llm\n    \n    async def reconstruct_context(self, query: str, user_id: str) -> str:\n        \"\"\"Reconstruct comprehensive context from all memory sources.\"\"\"\n        # 1. Vector search for semantically similar memories\n        vector_memories = self.vector_store.search(\n            query=query,\n            user_id=user_id,\n            limit=10\n        )\n        \n        # 2. Extract entities from query\n        entities = await self.extract_entities(query)\n        \n        # 3. Graph query for entity relationships\n        graph_context = []\n        for entity in entities:\n            relationships = self.graph_store.query_relationships(\n                user_id=user_id,\n                entity=entity\n            )\n            graph_context.extend(relationships)\n        \n        # 4. Reconstruct coherent context\n        reconstructed = await self.llm.ainvoke(\n            f'''Reconstruct a coherent context from these memory sources.\n            \n            Semantic memories:\n            {[m.content for m in vector_memories]}\n            \n            Entity relationships:\n            {graph_context}\n            \n            Query: {query}\n            \n            Synthesize these into a coherent context paragraph.'''\n        )\n        \n        return reconstructed.content\n    \n    async def extract_entities(self, text: str) -> List[str]:\n        response = await self.llm.ainvoke(\n            f'Extract named entities from: {text}\\nReturn as comma-separated list.'\n        )\n        return [e.strip() for e in response.content.split(',')]",
      "best_practices": [
        "Combine vector and graph retrieval",
        "Extract entities for graph traversal",
        "Use LLM to synthesize coherent context",
        "Cache reconstructed contexts for performance"
      ]
    },
    "evermemos_patterns": {
      "description": "EverMemOS-inspired lifecycle memory management",
      "use_when": "Complex long-horizon agent tasks requiring extensive memory",
      "stages": {
        "experience": "Initial event capture as raw experiences",
        "consolidation": "Periodic organization and denoising",
        "retrieval": "Multi-source context reconstruction",
        "application": "Memory-guided action selection"
      },
      "code_example": "from dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Optional\nimport asyncio\n\n@dataclass\nclass Experience:\n    content: str\n    timestamp: datetime\n    context: dict\n    importance: float = 0.5\n\n@dataclass\nclass ConsolidatedMemory:\n    content: str\n    sources: List[str]\n    confidence: float\n    last_accessed: datetime\n    access_count: int = 0\n\nclass LifecycleMemory:\n    def __init__(self):\n        self.experiences = []  # Short-term\n        self.consolidated = []  # Long-term\n        self.consolidation_interval = 3600  # 1 hour\n    \n    async def record_experience(self, content: str, context: dict):\n        \"\"\"Stage 1: Record raw experience.\"\"\"\n        exp = Experience(\n            content=content,\n            timestamp=datetime.now(),\n            context=context,\n            importance=await self.estimate_importance(content)\n        )\n        self.experiences.append(exp)\n    \n    async def consolidate(self):\n        \"\"\"Stage 2: Periodic consolidation.\"\"\"\n        if not self.experiences:\n            return\n        \n        # Filter by importance\n        important = [e for e in self.experiences if e.importance > 0.5]\n        \n        # Deduplicate and merge\n        consolidated = await self.merge_experiences(important)\n        \n        self.consolidated.extend(consolidated)\n        self.experiences = []  # Clear short-term\n    \n    async def retrieve(self, query: str) -> str:\n        \"\"\"Stage 3: Reconstruct context.\"\"\"\n        # Search consolidated memories\n        relevant = await self.search_consolidated(query)\n        \n        # Update access patterns\n        for mem in relevant:\n            mem.access_count += 1\n            mem.last_accessed = datetime.now()\n        \n        # Reconstruct coherent context\n        return await self.reconstruct(query, relevant)\n    \n    async def apply(self, query: str, available_actions: List[str]) -> str:\n        \"\"\"Stage 4: Memory-guided action selection.\"\"\"\n        context = await self.retrieve(query)\n        return await self.select_action(query, context, available_actions)\n    \n    async def start_consolidation_loop(self):\n        \"\"\"Background consolidation task.\"\"\"\n        while True:\n            await asyncio.sleep(self.consolidation_interval)\n            await self.consolidate()",
      "best_practices": [
        "Separate short-term and long-term storage",
        "Consolidate periodically (not on every write)",
        "Track access patterns for importance weighting",
        "Use importance filtering to reduce noise"
      ]
    },
    "long_horizon_memory": {
      "description": "Memory patterns for tasks spanning hours or days",
      "use_when": "Agents working on extended multi-step tasks",
      "code_example": "from langchain_openai import ChatOpenAI\nfrom typing import List, Optional\nfrom datetime import datetime, timedelta\n\nclass LongHorizonMemory:\n    def __init__(self, checkpointer, llm):\n        self.checkpointer = checkpointer\n        self.llm = llm\n        self.task_start = None\n        self.milestones = []\n    \n    async def start_task(self, task_description: str, task_id: str):\n        \"\"\"Initialize long-horizon task.\"\"\"\n        self.task_start = datetime.now()\n        await self.checkpointer.save(\n            task_id,\n            {\n                'description': task_description,\n                'started': self.task_start.isoformat(),\n                'status': 'in_progress',\n                'milestones': []\n            }\n        )\n    \n    async def record_milestone(self, task_id: str, milestone: str, outputs: dict):\n        \"\"\"Record progress milestone.\"\"\"\n        state = await self.checkpointer.load(task_id)\n        state['milestones'].append({\n            'description': milestone,\n            'timestamp': datetime.now().isoformat(),\n            'outputs': outputs\n        })\n        await self.checkpointer.save(task_id, state)\n    \n    async def get_progress_summary(self, task_id: str) -> str:\n        \"\"\"Summarize progress for context.\"\"\"\n        state = await self.checkpointer.load(task_id)\n        \n        return await self.llm.ainvoke(\n            f'''Summarize the progress on this task:\n            Task: {state[\"description\"]}\n            Started: {state[\"started\"]}\n            Milestones: {state[\"milestones\"]}\n            \n            Provide a concise summary of what has been done and what remains.'''\n        ).content\n    \n    async def resume_task(self, task_id: str) -> dict:\n        \"\"\"Resume task with full context.\"\"\"\n        state = await self.checkpointer.load(task_id)\n        summary = await self.get_progress_summary(task_id)\n        \n        return {\n            'task': state['description'],\n            'progress_summary': summary,\n            'milestones': state['milestones'],\n            'elapsed_time': datetime.now() - datetime.fromisoformat(state['started'])\n        }",
      "best_practices": [
        "Checkpoint progress regularly",
        "Record milestones with outputs",
        "Generate summaries for context reconstruction",
        "Enable task resumption across sessions"
      ]
    }
  },
  "best_practices": [
    "Use Mem0 or similar for production-ready memory infrastructure",
    "Extract salient facts immediately after conversations",
    "Filter by importance threshold to reduce memory noise",
    "Consolidate memories periodically to prevent unbounded growth",
    "Combine vector and graph storage for comprehensive retrieval",
    "Scope all memories by user_id for multi-tenant isolation",
    "Use semantic clustering for organized memory navigation",
    "Reconstruct context from multiple memory sources",
    "Checkpoint long-horizon tasks for resumption",
    "Track access patterns to prioritize frequently used memories"
  ],
  "anti_patterns": [
    {
      "name": "Store Everything",
      "problem": "Memory grows unbounded, retrieval slows",
      "solution": "Filter by importance, consolidate periodically"
    },
    {
      "name": "No User Isolation",
      "problem": "User memories leak across accounts",
      "solution": "Always scope by user_id"
    },
    {
      "name": "Vector-Only Memory",
      "problem": "Cannot query relationships between entities",
      "solution": "Combine vector and graph storage"
    },
    {
      "name": "Session-Only Memory",
      "problem": "Agent forgets between sessions",
      "solution": "Use persistent memory storage (Mem0, database)"
    },
    {
      "name": "No Consolidation",
      "problem": "Duplicate and outdated memories accumulate",
      "solution": "Schedule periodic consolidation"
    }
  ],
  "sources": [
    "https://mem0.ai/",
    "https://github.com/mem0ai/mem0",
    "https://arxiv.org/abs/2403.16233",
    "https://www.langchain.com/blog/memory-agent-patterns"
  ]
}