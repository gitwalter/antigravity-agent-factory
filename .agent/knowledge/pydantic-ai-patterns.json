{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "pydantic-ai-patterns",
  "name": "Pydantic AI Patterns",
  "title": "Pydantic AI Patterns",
  "description": "Best practices and patterns for Pydantic AI - schema-safe agents with structured outputs via tool-calling and Pydantic validation",
  "version": "1.0.0",
  "last_updated": "2026-02-11",
  "category": "agent-patterns",
  "axiomAlignment": {
    "A1_verifiability": "Pydantic schemas enable verification of all agent inputs and outputs",
    "A2_user_primacy": "Type-safe interfaces ensure predictable agent behavior",
    "A3_transparency": "Schema definitions make agent contracts explicit",
    "A4_non_harm": "Validation prevents malformed or harmful data",
    "A5_consistency": "Unified patterns across all agent interactions"
  },
  "related_skills": [
    "tool-usage",
    "error-handling",
    "structured-output"
  ],
  "related_knowledge": [
    "langchain-patterns.json",
    "openai-agents-sdk-patterns.json",
    "instructor-patterns.json"
  ],
  "patterns": {
    "basic_agent": {
      "description": "Create a schema-safe agent with Pydantic AI",
      "use_when": "Building agents that need type-safe, validated outputs",
      "code_example": "from pydantic_ai import Agent\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\n# Define output schema\nclass ResearchResult(BaseModel):\n    title: str = Field(description='Title of the research')\n    summary: str = Field(description='Brief summary of findings')\n    key_points: List[str] = Field(description='Main takeaways')\n    confidence: float = Field(ge=0, le=1, description='Confidence score')\n\n# Create agent with structured output\nagent = Agent(\n    model='openai:gpt-4',\n    result_type=ResearchResult,\n    system_prompt='You are a research assistant. Analyze topics thoroughly.'\n)\n\n# Run agent - output is guaranteed to match schema\nresult = await agent.run('Analyze the impact of AI on healthcare')\n\n# Access typed result\nprint(f'Title: {result.data.title}')\nprint(f'Summary: {result.data.summary}')\nfor point in result.data.key_points:\n    print(f'- {point}')\nprint(f'Confidence: {result.data.confidence}')",
      "best_practices": [
        "Define Pydantic models for all agent outputs",
        "Use Field descriptions to guide LLM output",
        "Add validation constraints (ge, le, regex, etc.)",
        "Use typing hints for complex types (List, Dict, Optional)"
      ]
    },
    "tool_output_marker": {
      "description": "Mark tool return types for structured tool calling",
      "use_when": "Tools need to return structured data that the agent can use",
      "code_example": "from pydantic_ai import Agent, ToolOutput\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass SearchResult(BaseModel):\n    title: str\n    url: str\n    snippet: str\n    relevance: float\n\nclass SearchResults(BaseModel):\n    results: List[SearchResult]\n    total_count: int\n\n# Define tool with ToolOutput marker\ndef search_web(query: str, max_results: int = 5) -> ToolOutput[SearchResults]:\n    \"\"\"Search the web for information.\"\"\"\n    # Implementation\n    results = perform_search(query, max_results)\n    return SearchResults(\n        results=[SearchResult(**r) for r in results],\n        total_count=len(results)\n    )\n\n# Agent uses structured tool output\nagent = Agent(\n    model='openai:gpt-4',\n    tools=[search_web],\n    system_prompt='You are a research assistant with web search.'\n)\n\nresult = await agent.run('Find recent news about quantum computing')\n# Agent receives typed SearchResults from tool",
      "best_practices": [
        "Use ToolOutput[T] to specify tool return types",
        "Define Pydantic models for complex tool returns",
        "Include metadata fields (count, pagination, etc.)",
        "Validate tool outputs before returning"
      ]
    },
    "agent_dependencies": {
      "description": "Inject dependencies into agent for runtime context",
      "use_when": "Agents need access to databases, APIs, or services",
      "code_example": "from pydantic_ai import Agent, RunContext\nfrom pydantic import BaseModel\nfrom dataclasses import dataclass\nimport httpx\n\n@dataclass\nclass AppDependencies:\n    \"\"\"Dependencies available to agent and tools.\"\"\"\n    http_client: httpx.AsyncClient\n    db_connection: DatabaseConnection\n    user_id: str\n    api_key: str\n\nclass UserInfo(BaseModel):\n    name: str\n    email: str\n    plan: str\n\n# Tool accessing dependencies\ndef get_user_info(ctx: RunContext[AppDependencies]) -> ToolOutput[UserInfo]:\n    \"\"\"Get current user information.\"\"\"\n    user = ctx.deps.db_connection.get_user(ctx.deps.user_id)\n    return UserInfo(\n        name=user.name,\n        email=user.email,\n        plan=user.subscription_plan\n    )\n\nagent = Agent(\n    model='openai:gpt-4',\n    deps_type=AppDependencies,\n    tools=[get_user_info],\n    system_prompt='You are a customer support assistant.'\n)\n\n# Run with dependencies\nasync with httpx.AsyncClient() as client:\n    deps = AppDependencies(\n        http_client=client,\n        db_connection=get_db(),\n        user_id='user_123',\n        api_key=os.environ['API_KEY']\n    )\n    \n    result = await agent.run('What plan am I on?', deps=deps)",
      "best_practices": [
        "Use dataclasses for dependency containers",
        "Access dependencies via RunContext",
        "Inject async clients for I/O operations",
        "Keep dependencies immutable during runs"
      ]
    },
    "streaming_results": {
      "description": "Stream agent responses for real-time UX",
      "use_when": "Building interactive applications with real-time feedback",
      "code_example": "from pydantic_ai import Agent\nfrom pydantic import BaseModel\n\nclass Article(BaseModel):\n    title: str\n    content: str\n    word_count: int\n\nagent = Agent(\n    model='openai:gpt-4',\n    result_type=Article,\n    system_prompt='You are a content writer.'\n)\n\n# Stream response\nasync with agent.run_stream('Write an article about Python') as stream:\n    # Stream partial text as it's generated\n    async for chunk in stream.stream_text():\n        print(chunk, end='', flush=True)\n    \n    # Get final structured result\n    result = await stream.get_result()\n    print(f'\\n\\nFinal result: {result.data}')\n\n# Alternative: StreamedRunResult\nresult: StreamedRunResult[Article] = await agent.run_stream(\n    'Write an article about AI'\n)\n\n# Access partial data during streaming\nasync for partial in result.stream():\n    if partial.data:\n        print(f'Partial: {partial.data}')",
      "best_practices": [
        "Use run_stream for real-time applications",
        "Stream text for immediate feedback",
        "Call get_result() for final validated output",
        "Handle streaming errors gracefully"
      ]
    },
    "conversation_history": {
      "description": "Maintain conversation state across interactions",
      "use_when": "Building multi-turn conversational agents",
      "code_example": "from pydantic_ai import Agent\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\nclass ConversationResponse(BaseModel):\n    response: str\n    sentiment: str\n    follow_up_questions: Optional[List[str]] = None\n\nagent = Agent(\n    model='openai:gpt-4',\n    result_type=ConversationResponse,\n    system_prompt='You are a helpful assistant. Ask clarifying questions.'\n)\n\n# First message\nresult1 = await agent.run('I want to learn Python')\nprint(f'Assistant: {result1.data.response}')\n\n# Continue conversation with history\nresult2 = await agent.run(\n    'What should I start with?',\n    message_history=result1.all_messages()\n)\nprint(f'Assistant: {result2.data.response}')\n\n# Continue further\nresult3 = await agent.run(\n    'Do you have any book recommendations?',\n    message_history=result2.all_messages()\n)\nprint(f'Assistant: {result3.data.response}')\n\n# Access full history\nfor msg in result3.all_messages():\n    print(f'{msg.kind}: {msg.content}')",
      "best_practices": [
        "Pass message_history for multi-turn conversations",
        "Use all_messages() to get complete history",
        "Summarize long histories to prevent context overflow",
        "Store histories for session persistence"
      ]
    },
    "result_validation": {
      "description": "Validate and retry on invalid agent outputs",
      "use_when": "Ensuring agent outputs meet quality requirements",
      "code_example": "from pydantic_ai import Agent, ModelRetry\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List\n\nclass CodeReview(BaseModel):\n    issues: List[str] = Field(min_length=1, description='Found issues')\n    severity: str = Field(pattern='^(low|medium|high|critical)$')\n    suggestions: List[str]\n    approved: bool\n    \n    @field_validator('suggestions')\n    @classmethod\n    def suggestions_not_empty(cls, v):\n        if not v:\n            raise ValueError('Must provide at least one suggestion')\n        return v\n\nagent = Agent(\n    model='openai:gpt-4',\n    result_type=CodeReview,\n    system_prompt='You are a code reviewer. Always provide actionable feedback.',\n    retries=3  # Retry on validation failure\n)\n\ntry:\n    result = await agent.run('Review this code: def add(a, b): return a + b')\n    print(f'Issues: {result.data.issues}')\n    print(f'Severity: {result.data.severity}')\n    print(f'Approved: {result.data.approved}')\nexcept ModelRetry as e:\n    print(f'Agent failed after retries: {e}')",
      "best_practices": [
        "Use Pydantic validators for complex constraints",
        "Set appropriate retry count for production",
        "Define clear Field constraints (min_length, pattern, etc.)",
        "Handle ModelRetry exceptions gracefully"
      ]
    },
    "multiple_models": {
      "description": "Use different models for different tasks",
      "use_when": "Optimizing cost and performance across agent operations",
      "code_example": "from pydantic_ai import Agent\nfrom pydantic import BaseModel\n\nclass Summary(BaseModel):\n    summary: str\n    key_points: list[str]\n\nclass DetailedAnalysis(BaseModel):\n    analysis: str\n    recommendations: list[str]\n    confidence: float\n\n# Fast model for simple tasks\nsummary_agent = Agent(\n    model='openai:gpt-3.5-turbo',\n    result_type=Summary,\n    system_prompt='Summarize content concisely.'\n)\n\n# Capable model for complex analysis\nanalysis_agent = Agent(\n    model='openai:gpt-4',\n    result_type=DetailedAnalysis,\n    system_prompt='Provide detailed analysis with recommendations.'\n)\n\n# Anthropic for specific use cases\ncreative_agent = Agent(\n    model='anthropic:claude-3-opus',\n    result_type=str,\n    system_prompt='You are a creative writer.'\n)\n\n# Use appropriate agent based on task\ndocument = 'Long document content...'\n\n# Quick summary with fast model\nsummary = await summary_agent.run(f'Summarize: {document}')\n\n# Detailed analysis with capable model\nanalysis = await analysis_agent.run(f'Analyze: {document}')",
      "best_practices": [
        "Use faster models for simple tasks",
        "Use capable models for complex reasoning",
        "Match model to task requirements",
        "Monitor costs across model usage"
      ]
    }
  },
  "best_practices": [
    "Define Pydantic models for all agent outputs to ensure type safety",
    "Use Field descriptions to guide LLM output generation",
    "Add validation constraints (ge, le, regex, min_length) for quality control",
    "Use ToolOutput[T] marker for structured tool returns",
    "Inject dependencies via RunContext for clean architecture",
    "Use run_stream for real-time applications with immediate feedback",
    "Pass message_history for multi-turn conversations",
    "Set retries > 1 for production to handle validation failures",
    "Match model selection to task complexity for cost optimization",
    "Handle ModelRetry exceptions for graceful degradation"
  ],
  "anti_patterns": [
    {
      "name": "Untyped Agent Outputs",
      "problem": "Agent outputs are unpredictable and hard to use",
      "solution": "Always define result_type with Pydantic model"
    },
    {
      "name": "Missing Field Descriptions",
      "problem": "LLM doesn't understand expected output format",
      "solution": "Add Field(description=...) to all Pydantic fields"
    },
    {
      "name": "No Validation Constraints",
      "problem": "Invalid data passes through without detection",
      "solution": "Add constraints like min_length, pattern, ge, le"
    },
    {
      "name": "Unhandled Retries",
      "problem": "Application crashes on validation failures",
      "solution": "Set retries and handle ModelRetry exceptions"
    },
    {
      "name": "Hardcoded Dependencies",
      "problem": "Hard to test and modify agent behavior",
      "solution": "Use deps_type and RunContext for dependency injection"
    }
  ],
  "comparison_with_instructor": {
    "similarities": [
      "Both use Pydantic for structured outputs",
      "Both integrate with OpenAI and other LLM providers",
      "Both support validation and retries"
    ],
    "differences": [
      "Pydantic AI is agent-focused, Instructor is output-focused",
      "Pydantic AI has built-in tool support with ToolOutput",
      "Pydantic AI has dependency injection system",
      "Instructor is simpler for pure structured extraction"
    ],
    "when_to_use_pydantic_ai": [
      "Building agents with tools",
      "Need dependency injection",
      "Multi-turn conversations",
      "Streaming structured outputs"
    ],
    "when_to_use_instructor": [
      "Simple structured extraction",
      "Patching existing OpenAI clients",
      "Minimal setup required",
      "Focus on output only, not agent behavior"
    ]
  },
  "sources": [
    "https://ai.pydantic.dev/",
    "https://github.com/pydantic/pydantic-ai",
    "https://pypi.org/project/pydantic-ai/"
  ]
}