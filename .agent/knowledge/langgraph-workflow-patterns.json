{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "langgraph-workflow-patterns",
  "name": "LangGraph Workflow Patterns",
  "title": "LangGraph Workflow Patterns",
  "description": "Patterns for building stateful agent workflows with LangGraph including state management, checkpointing, human-in-the-loop, distributed workflows, and production patterns",
  "version": "1.1.0",
  "category": "workflow",
  "axiomAlignment": {
    "A1_verifiability": "State machines enable traceable, reproducible agent behavior",
    "A2_user_primacy": "Workflows serve user goals; human-in-the-loop supports critical decisions",
    "A3_transparency": "Graph structure makes agent logic explicit and inspectable",
    "A4_non_harm": "Error nodes and fallback paths ensure graceful failure handling",
    "A5_consistency": "Unified state and checkpoint patterns across workflows"
  },
  "related_skills": [
    "langgraph-agent-building",
    "state-management",
    "human-in-the-loop",
    "subagent-orchestration",
    "agent-testing"
  ],
  "related_knowledge": [
    "langchain-patterns.json",
    "multi-agent-patterns.json",
    "agent-handoffs.json"
  ],
  "core_concepts": {
    "graph": {
      "description": "Directed graph defining agent workflow",
      "components": [
        "Nodes (functions)",
        "Edges (transitions)",
        "State (shared data)"
      ],
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "state": {
      "description": "TypedDict or Pydantic model holding workflow state",
      "best_practices": [
        "Use immutable updates (return new state)",
        "Define clear state schema with types",
        "Include metadata for observability"
      ],
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)"
    },
    "node": {
      "description": "Function that transforms state",
      "signature": "def node(state: State) -> State | dict",
      "best_practices": [
        "Single responsibility per node",
        "Return only changed state keys",
        "Handle errors gracefully"
      ],
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)"
    },
    "edge": {
      "description": "Connection between nodes, can be conditional",
      "types": [
        "Normal edge",
        "Conditional edge",
        "Entry/exit points"
      ],
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    }
  },
  "patterns": {
    "supervisor_pattern": {
      "description": "Supervisor agent coordinates worker agents",
      "use_when": "Complex tasks requiring multiple specialized agents",
      "structure": {
        "supervisor_node": "Routes tasks to appropriate workers",
        "worker_nodes": "Specialized agents for specific tasks",
        "aggregator_node": "Combines worker outputs"
      },
      "code_example": "from langgraph.graph import StateGraph, MessagesState, START, END\n\ndef supervisor(state: State) -> State:\n    # Decide which worker to invoke\n    next_worker = decide_next_worker(state)\n    return {'next': next_worker}\n\ndef worker_a(state: State) -> State:\n    # Perform specialized task A\n    result = do_task_a(state['input'])\n    return {'results': state['results'] + [result]}\n\ngraph = StateGraph(State)\ngraph.add_node('supervisor', supervisor)\ngraph.add_node('worker_a', worker_a)\ngraph.add_node('worker_b', worker_b)\ngraph.add_conditional_edges('supervisor', route_to_worker)\ngraph.add_edge(START, 'supervisor')",
      "best_practices": [
        "Keep supervisor logic simple and focused on routing",
        "Define clear handoff protocols between workers",
        "Implement timeout handling for workers"
      ],
      "axiom_alignment": "A2 (User Primacy) - Supervisor ensures user intent is properly delegated"
    },
    "sequential_workflow": {
      "description": "Linear sequence of processing steps",
      "use_when": "Ordered pipeline of transformations",
      "structure": "step1 -> step2 -> step3 -> END",
      "code_example": "graph = StateGraph(State)\ngraph.add_node('extract', extract_info)\ngraph.add_node('transform', transform_data)\ngraph.add_node('validate', validate_output)\ngraph.add_edge(START, 'extract')\ngraph.add_edge('extract', 'transform')\ngraph.add_edge('transform', 'validate')\ngraph.add_edge('validate', END)",
      "best_practices": [
        "Each step should be independently testable",
        "Add validation between steps",
        "Log state at each transition"
      ]
    },
    "branching_workflow": {
      "description": "Conditional routing based on state",
      "use_when": "Different processing paths based on input or intermediate results",
      "code_example": "def route_by_type(state: State) -> str:\n    if state['input_type'] == 'code':\n        return 'process_code'\n    elif state['input_type'] == 'text':\n        return 'process_text'\n    return 'default_process'\n\ngraph.add_conditional_edges(\n    'classifier',\n    route_by_type,\n    {'process_code': 'code_node', 'process_text': 'text_node', 'default_process': 'default_node'}\n)",
      "best_practices": [
        "Define exhaustive routing conditions",
        "Always include default/fallback route",
        "Test all branches"
      ]
    },
    "parallel_workflow": {
      "description": "Multiple nodes execute concurrently",
      "use_when": "Independent tasks that can run in parallel",
      "implementation": "Use fanout edges to multiple nodes, fanin to aggregator",
      "best_practices": [
        "Ensure tasks are truly independent",
        "Handle partial failures gracefully",
        "Implement proper aggregation logic"
      ],
      "code_example": "Use fanout edges to multiple nodes, fanin to aggregator"
    },
    "human_in_the_loop": {
      "description": "Pause workflow for human input/approval",
      "use_when": "Critical decisions requiring human oversight",
      "implementation": "Use interrupt_before or interrupt_after with checkpointing",
      "code_example": "from langgraph.checkpoint.sqlite import SqliteSaver\n\nmemory = SqliteSaver.from_conn_string(':memory:')\ngraph = builder.compile(\n    checkpointer=memory,\n    interrupt_before=['human_review']\n)\n\n# Resume after human input\ngraph.invoke(None, config={'configurable': {'thread_id': thread_id}})",
      "best_practices": [
        "Persist state for resumption",
        "Provide clear context for human decision",
        "Set timeouts for human responses"
      ],
      "axiom_alignment": "A2 (User Primacy) - Human oversight for critical decisions"
    },
    "reflection_pattern": {
      "description": "Agent reviews and improves its own output",
      "use_when": "Quality improvement through self-critique",
      "structure": "generate -> reflect -> revise (loop until satisfactory)",
      "code_example": "def should_continue(state: State) -> str:\n    if state['iteration'] >= 3:\n        return 'end'\n    if state['quality_score'] >= 0.9:\n        return 'end'\n    return 'reflect'\n\ngraph.add_conditional_edges(\n    'revise',\n    should_continue,\n    {'end': END, 'reflect': 'reflect'}\n)",
      "best_practices": [
        "Set maximum iteration limits",
        "Define clear quality criteria",
        "Track improvement across iterations"
      ]
    },
    "tool_executor_pattern": {
      "description": "Node that executes tools and returns results",
      "use_when": "Agent needs to use external tools",
      "implementation": "ToolNode from langgraph.prebuilt",
      "code_example": "from langgraph.prebuilt import ToolNode\n\ntools = [search_tool, calculator_tool]\ntool_node = ToolNode(tools)\n\ngraph.add_node('tools', tool_node)\ngraph.add_edge('agent', 'tools')\ngraph.add_edge('tools', 'agent')",
      "best_practices": [
        "Validate tool outputs",
        "Handle tool failures gracefully",
        "Log tool invocations for debugging"
      ]
    },
    "subgraph_pattern": {
      "description": "Compose graphs within graphs for modularity",
      "use_when": "Complex workflows that can be decomposed into sub-workflows",
      "code_example": "from langgraph.graph import StateGraph, START, END\n\n# Create subgraph\nresearch_subgraph = StateGraph(State)\nresearch_subgraph.add_node('gather', gather_sources)\nresearch_subgraph.add_node('analyze', analyze_sources)\nresearch_subgraph.add_edge(START, 'gather')\nresearch_subgraph.add_edge('gather', 'analyze')\nresearch_subgraph.add_edge('analyze', END)\nresearch_compiled = research_subgraph.compile()\n\n# Use subgraph as node in main graph\nmain_graph = StateGraph(State)\nmain_graph.add_node('research', research_compiled.invoke)\nmain_graph.add_node('write', write_report)\nmain_graph.add_edge(START, 'research')\nmain_graph.add_edge('research', 'write')\nmain_graph.add_edge('write', END)",
      "best_practices": [
        "Keep subgraphs focused and reusable",
        "Define clear interfaces between graphs",
        "Test subgraphs independently",
        "Use for complex, reusable workflows"
      ]
    },
    "nested_subgraph": {
      "description": "Subgraphs that can be nested multiple levels",
      "use_when": "Very complex workflows with hierarchical structure",
      "code_example": "def create_research_subgraph():\n    graph = StateGraph(State)\n    graph.add_node('search', search_node)\n    graph.add_node('filter', filter_node)\n    graph.add_edge(START, 'search')\n    graph.add_edge('search', 'filter')\n    graph.add_edge('filter', END)\n    return graph.compile()\n\ndef create_analysis_subgraph():\n    graph = StateGraph(State)\n    graph.add_node('research', create_research_subgraph().invoke)\n    graph.add_node('analyze', analyze_node)\n    graph.add_edge(START, 'research')\n    graph.add_edge('research', 'analyze')\n    graph.add_edge('analyze', END)\n    return graph.compile()\n\n# Main graph uses nested subgraph\nmain_graph.add_node('analysis', create_analysis_subgraph().invoke)",
      "best_practices": [
        "Limit nesting depth (2-3 levels)",
        "Document subgraph interfaces",
        "Test at each level",
        "Consider flattening if too nested"
      ]
    }
  },
  "advanced_patterns": {
    "time_travel_debugging": {
      "description": "Inspect and replay workflow execution at any point",
      "use_when": "Debugging complex workflows or understanding execution flow",
      "code_example": "from langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.checkpoint.base import Checkpoint\n\nmemory = SqliteSaver.from_conn_string('checkpoints.db')\ngraph = builder.compile(checkpointer=memory)\n\n# Execute workflow\nconfig = {'configurable': {'thread_id': 'debug-123'}}\nresult = graph.invoke({'input': 'test'}, config=config)\n\n# Get all checkpoints for this thread\ncheckpoints = memory.list(config, filter={'thread_id': 'debug-123'})\n\n# Replay from specific checkpoint\nfor checkpoint in checkpoints:\n    print(f'Checkpoint at step {checkpoint[\"step\"]}')\n    print(f'State: {checkpoint[\"channel_values\"]}')\n    \n    # Replay from this checkpoint\n    replay_config = {\n        'configurable': {\n            'thread_id': 'debug-123',\n            'checkpoint_id': checkpoint['id']\n        }\n    }\n    replay_result = graph.invoke(None, config=replay_config)",
      "best_practices": [
        "Use persistent checkpointing for debugging",
        "Store checkpoint metadata",
        "Implement checkpoint inspection tools",
        "Use for post-mortem analysis"
      ]
    },
    "distributed_workflows": {
      "description": "Workflows that run across multiple machines",
      "use_when": "Large-scale or geographically distributed systems",
      "code_example": "from langgraph.checkpoint.postgres import PostgresSaver\nimport redis\n\n# Shared checkpoint store\ncheckpointer = PostgresSaver.from_conn_string(\n    'postgresql://user:pass@host/db'\n)\n\n# Distributed graph execution\nclass DistributedGraph:\n    def __init__(self, graph, checkpointer, redis_client):\n        self.graph = graph.compile(checkpointer=checkpointer)\n        self.redis = redis_client\n    \n    def invoke_distributed(self, input_state, config):\n        # Lock for distributed execution\n        lock_key = f'lock:{config[\"configurable\"][\"thread_id\"]}'\n        \n        with self.redis.lock(lock_key, timeout=30):\n            # Execute with shared checkpoint\n            result = self.graph.invoke(input_state, config)\n            return result\n\n# Use Redis for coordination\nredis_client = redis.Redis(host='localhost', port=6379)\ndistributed_graph = DistributedGraph(graph, checkpointer, redis_client)",
      "best_practices": [
        "Use shared checkpoint store (Postgres, etc.)",
        "Implement distributed locking",
        "Handle network partitions",
        "Monitor distributed execution",
        "Consider message queues for coordination"
      ]
    },
    "advanced_checkpointing": {
      "description": "Advanced checkpointing strategies",
      "snapshot_checkpointing": {
        "description": "Periodic snapshots for fast recovery",
        "code_example": "from langgraph.checkpoint.base import BaseCheckpointSaver\n\nclass SnapshotCheckpointSaver(BaseCheckpointSaver):\n    def __init__(self, base_saver, snapshot_interval=10):\n        self.base_saver = base_saver\n        self.snapshot_interval = snapshot_interval\n        self.snapshots = {}\n    \n    def put(self, config, checkpoint, metadata, new_versions):\n        # Save to base saver\n        result = self.base_saver.put(config, checkpoint, metadata, new_versions)\n        \n        # Create snapshot periodically\n        thread_id = config['configurable']['thread_id']\n        step = checkpoint.get('step', 0)\n        \n        if step % self.snapshot_interval == 0:\n            self.snapshots[thread_id] = {\n                'checkpoint': checkpoint,\n                'step': step,\n                'timestamp': time.time()\n            }\n        \n        return result\n    \n    def get_snapshot(self, thread_id, step=None):\n        if step:\n            # Find closest snapshot\n            for snap in self.snapshots.get(thread_id, []):\n                if snap['step'] <= step:\n                    return snap\n        return self.snapshots.get(thread_id)",
        "best_practices": [
          "Set appropriate snapshot interval",
          "Clean up old snapshots",
          "Use for long-running workflows",
          "Balance storage vs recovery speed"
        ],
        "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows."
      },
      "incremental_checkpointing": {
        "description": "Store only changes between checkpoints",
        "code_example": "class IncrementalCheckpointSaver:\n    def __init__(self, base_saver):\n        self.base_saver = base_saver\n        self.deltas = {}\n    \n    def put(self, config, checkpoint, metadata, new_versions):\n        thread_id = config['configurable']['thread_id']\n        \n        # Get previous checkpoint\n        prev_checkpoint = self.base_saver.get(config)\n        \n        if prev_checkpoint:\n            # Calculate delta\n            delta = self._calculate_delta(\n                prev_checkpoint['channel_values'],\n                checkpoint['channel_values']\n            )\n            \n            # Store delta\n            self.deltas[thread_id] = delta\n        \n        # Save full checkpoint periodically\n        return self.base_saver.put(config, checkpoint, metadata, new_versions)\n    \n    def _calculate_delta(self, old_state, new_state):\n        delta = {}\n        for key in new_state:\n            if key not in old_state or old_state[key] != new_state[key]:\n                delta[key] = new_state[key]\n        return delta",
        "best_practices": [
          "Use for large state objects",
          "Periodically save full checkpoints",
          "Handle delta reconstruction",
          "Monitor storage savings"
        ],
        "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows."
      },
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "state_reducers": {
      "description": "Advanced state reduction patterns",
      "custom_reducer": {
        "description": "Create custom reducers for state updates",
        "code_example": "from typing import Annotated\nfrom langgraph.graph import StateGraph\nfrom operator import add\n\n# Custom reducer for merging dictionaries\ndef merge_dicts(left: dict, right: dict) -> dict:\n    result = left.copy()\n    result.update(right)\n    return result\n\n# Custom reducer for appending with deduplication\ndef append_unique(left: list, right: list) -> list:\n    result = left.copy()\n    for item in right:\n        if item not in result:\n            result.append(item)\n    return result\n\nclass State(TypedDict):\n    # Use built-in reducer\n    messages: Annotated[list, add_messages]\n    \n    # Use operator reducer\n    counts: Annotated[dict, add]\n    \n    # Use custom reducer\n    metadata: Annotated[dict, merge_dicts]\n    unique_items: Annotated[list, append_unique]",
        "best_practices": [
          "Use appropriate reducers for data types",
          "Ensure reducers are commutative and associative",
          "Test reducer behavior",
          "Document reducer semantics"
        ],
        "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows."
      },
      "conditional_reducer": {
        "description": "Reducer that applies conditionally",
        "code_example": "def conditional_merge(left: dict, right: dict) -> dict:\n    result = left.copy()\n    \n    # Only merge non-None values\n    for key, value in right.items():\n        if value is not None:\n            result[key] = value\n    \n    return result\n\nclass State(TypedDict):\n    config: Annotated[dict, conditional_merge]",
        "best_practices": [
          "Document conditional logic",
          "Ensure idempotency",
          "Handle edge cases"
        ],
        "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows."
      },
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "command_pattern": {
      "description": "Encapsulate operations as commands for undo/redo",
      "use_when": "Need to support undo/redo or audit operations",
      "code_example": "from typing import Protocol\nfrom abc import ABC, abstractmethod\n\nclass Command(Protocol):\n    def execute(self, state: State) -> State:\n        ...\n    \n    def undo(self, state: State) -> State:\n        ...\n\nclass UpdateStateCommand:\n    def __init__(self, key: str, value: any):\n        self.key = key\n        self.value = value\n        self.previous_value = None\n    \n    def execute(self, state: State) -> State:\n        self.previous_value = state.get(self.key)\n        return {**state, self.key: self.value}\n    \n    def undo(self, state: State) -> State:\n        if self.previous_value is None:\n            return {k: v for k, v in state.items() if k != self.key}\n        return {**state, self.key: self.previous_value}\n\nclass CommandHistory:\n    def __init__(self):\n        self.history = []\n        self.current_index = -1\n    \n    def execute_command(self, command: Command, state: State) -> State:\n        # Remove any commands after current index (for redo)\n        self.history = self.history[:self.current_index + 1]\n        \n        # Execute command\n        new_state = command.execute(state)\n        \n        # Add to history\n        self.history.append(command)\n        self.current_index += 1\n        \n        return new_state\n    \n    def undo(self, state: State) -> State:\n        if self.current_index >= 0:\n            command = self.history[self.current_index]\n            self.current_index -= 1\n            return command.undo(state)\n        return state\n    \n    def redo(self, state: State) -> State:\n        if self.current_index < len(self.history) - 1:\n            self.current_index += 1\n            command = self.history[self.current_index]\n            return command.execute(state)\n        return state\n\n# Use in node\ndef command_node(state: State, history: CommandHistory) -> State:\n    command = UpdateStateCommand('status', 'processing')\n    return history.execute_command(command, state)",
      "best_practices": [
        "Implement execute and undo for all commands",
        "Store command history in checkpoints",
        "Limit history size",
        "Use for audit trails"
      ]
    }
  },
  "state_management": {
    "typed_state": {
      "description": "Define state with TypedDict for type safety",
      "code_example": "from typing import TypedDict, Annotated\nfrom langgraph.graph import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n    context: str\n    iteration: int",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "reducers": {
      "description": "Functions that combine state updates",
      "use_case": "When multiple nodes update the same key",
      "built_in": [
        "add_messages (for chat)",
        "operator.add (for lists)"
      ],
      "code_example": "from operator import add\nfrom langgraph.graph import add_messages\n\nclass State(TypedDict):\n    # Built-in message reducer\n    messages: Annotated[list, add_messages]\n    \n    # Operator reducer for lists\n    results: Annotated[list, add]\n    \n    # Custom reducer\n    metadata: Annotated[dict, lambda left, right: {**left, **right}]",
      "advanced_reducers": {
        "description": "Advanced reducer patterns",
        "merge_dicts": "from typing import Annotated\n\ndef merge_dicts(left: dict, right: dict) -> dict:\n    result = left.copy()\n    result.update(right)\n    return result\n\nclass State(TypedDict):\n    config: Annotated[dict, merge_dicts]",
        "append_unique": "def append_unique(left: list, right: list) -> list:\n    result = left.copy()\n    for item in right:\n        if item not in result:\n            result.append(item)\n    return result\n\nclass State(TypedDict):\n    unique_items: Annotated[list, append_unique]",
        "best_practices": [
          "Reducers must be commutative: reducer(a, b) == reducer(b, a)",
          "Reducers should be associative: reducer(reducer(a, b), c) == reducer(a, reducer(b, c))",
          "Test reducer with edge cases (empty inputs, None values)",
          "Document reducer behavior clearly"
        ],
        "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
        "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)"
      },
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "checkpointing": {
      "description": "Persist state for resumption and debugging",
      "implementations": [
        "MemorySaver",
        "SqliteSaver",
        "PostgresSaver"
      ],
      "use_when": "Human-in-the-loop; Long-running workflows; Fault tolerance",
      "basic_checkpointing": {
        "code_example": "from langgraph.checkpoint.sqlite import SqliteSaver\n\nmemory = SqliteSaver.from_conn_string('checkpoints.db')\ngraph = builder.compile(checkpointer=memory)\n\nconfig = {'configurable': {'thread_id': 'conversation-123'}}\nresult = graph.invoke({'input': 'Hello'}, config=config)",
        "best_practices": [
          "Use persistent storage for production",
          "Include thread_id for conversation tracking",
          "Implement cleanup for old checkpoints"
        ],
        "description": "Agent pattern for iterative reasoning and tool use with observation-based refinement.",
        "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows."
      },
      "advanced_checkpointing": {
        "description": "Advanced checkpointing strategies",
        "checkpoint_metadata": {
          "code_example": "from langgraph.checkpoint.base import CheckpointMetadata\n\n# Add metadata to checkpoints\nmetadata = CheckpointMetadata(\n    source='user_input',\n    user_id='user-123',\n    session_id='session-456'\n)\n\nconfig = {\n    'configurable': {\n        'thread_id': 'thread-789',\n        'metadata': metadata\n    }\n}\n\nresult = graph.invoke({'input': 'test'}, config=config)",
          "best_practices": [
            "Add metadata for filtering and debugging",
            "Include user/session identifiers",
            "Store operation context"
          ],
          "description": "Agent pattern for iterative reasoning and tool use with observation-based refinement.",
          "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows."
        },
        "checkpoint_filtering": {
          "code_example": "from langgraph.checkpoint.sqlite import SqliteSaver\n\nmemory = SqliteSaver.from_conn_string('checkpoints.db')\n\n# List checkpoints with filter\ncheckpoints = memory.list(\n    {'configurable': {}},\n    filter={'metadata.user_id': 'user-123'}\n)\n\n# Get specific checkpoint\ncheckpoint = memory.get(\n    {'configurable': {'thread_id': 'thread-789'}},\n    {'checkpoint_id': 'checkpoint-id'}\n)",
          "best_practices": [
            "Use filters for efficient querying",
            "Index frequently filtered fields",
            "Clean up old checkpoints"
          ],
          "description": "Agent pattern for iterative reasoning and tool use with observation-based refinement.",
          "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows."
        },
        "checkpoint_compression": {
          "code_example": "import gzip\nimport json\nfrom langgraph.checkpoint.base import BaseCheckpointSaver\n\nclass CompressedCheckpointSaver(BaseCheckpointSaver):\n    def __init__(self, base_saver):\n        self.base_saver = base_saver\n    \n    def put(self, config, checkpoint, metadata, new_versions):\n        # Compress checkpoint data\n        compressed_checkpoint = {\n            **checkpoint,\n            'channel_values': self._compress(checkpoint['channel_values'])\n        }\n        \n        return self.base_saver.put(config, compressed_checkpoint, metadata, new_versions)\n    \n    def get(self, config, checkpoint_id=None):\n        checkpoint = self.base_saver.get(config, checkpoint_id)\n        if checkpoint:\n            checkpoint['channel_values'] = self._decompress(checkpoint['channel_values'])\n        return checkpoint\n    \n    def _compress(self, data):\n        return gzip.compress(json.dumps(data).encode())\n    \n    def _decompress(self, compressed_data):\n        return json.loads(gzip.decompress(compressed_data).decode())",
          "best_practices": [
            "Use for large state objects",
            "Monitor compression ratio",
            "Consider compression overhead"
          ],
          "description": "Agent pattern for iterative reasoning and tool use with observation-based refinement.",
          "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows."
        },
        "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
        "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
        "best_practices": [
          "Set max_iterations to prevent infinite agent loops",
          "Use structured output for reliable parsing of agent responses"
        ]
      },
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    }
  },
  "prebuilt_components": {
    "create_react_agent": {
      "description": "Pre-built ReAct agent graph",
      "use_when": "Standard agent with tools",
      "code": "from langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(model, tools, checkpointer=memory)",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "ToolNode": {
      "description": "Pre-built node for tool execution",
      "use_when": "Need to execute tools in a graph",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    }
  },
  "testing_langgraph": {
    "unit_testing": {
      "description": "Test individual nodes",
      "approach": "Test node functions with mock state",
      "code_example": "def test_extract_node():\n    input_state = {'input': 'test data', 'results': []}\n    output_state = extract_node(input_state)\n    assert 'extracted' in output_state",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "integration_testing": {
      "description": "Test complete graph execution",
      "approach": "Compile graph and run with test inputs",
      "code_example": "def test_workflow():\n    graph = create_workflow()\n    result = graph.invoke({'input': 'test'})\n    assert result['status'] == 'complete'",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    },
    "visualization": {
      "description": "Visualize graph structure for debugging",
      "code": "graph.get_graph().draw_mermaid_png()",
      "use_when": "When building agents that need tool calling, multi-step reasoning, or structured workflows.",
      "code_example": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef example_tool(query: str) -> str:\n    '''Example tool for agent use.'''\n    return f\"Result for: {query}\"\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\ntools = [example_tool]\nagent = llm.bind_tools(tools)",
      "best_practices": [
        "Set max_iterations to prevent infinite agent loops",
        "Use structured output for reliable parsing of agent responses"
      ]
    }
  },
  "production_patterns": {
    "distributed_workflows": {
      "description": "Run workflows across multiple machines with shared state",
      "use_when": "Large-scale, geographically distributed, or fault-tolerant systems",
      "code_example": "from langgraph.checkpoint.postgres import PostgresSaver\nimport redis\nfrom contextlib import contextmanager\n\n# Shared checkpoint store for all workers\ncheckpointer = PostgresSaver.from_conn_string(\n    'postgresql://user:pass@host/db'\n)\n\n# Redis for distributed coordination and locking\nredis_client = redis.Redis(host='localhost', port=6379)\n\nclass DistributedWorkflowRunner:\n    def __init__(self, graph_builder, checkpointer, redis_client):\n        self.graph = graph_builder.compile(checkpointer=checkpointer)\n        self.redis = redis_client\n    \n    @contextmanager\n    def distributed_lock(self, thread_id: str, timeout: int = 30):\n        lock = self.redis.lock(f'workflow:{thread_id}', timeout=timeout)\n        try:\n            acquired = lock.acquire(blocking=True, blocking_timeout=10)\n            if not acquired:\n                raise TimeoutError(f'Could not acquire lock for {thread_id}')\n            yield\n        finally:\n            if lock.owned():\n                lock.release()\n    \n    def invoke_distributed(self, input_state: dict, config: dict):\n        thread_id = config['configurable']['thread_id']\n        with self.distributed_lock(thread_id):\n            return self.graph.invoke(input_state, config)",
      "best_practices": [
        "Use PostgresSaver for shared checkpoint store across workers",
        "Implement distributed locking with Redis to prevent race conditions",
        "Handle network partitions with timeout and retry logic",
        "Use message queues (RabbitMQ, Kafka) for async workflow triggers",
        "Monitor distributed execution with centralized logging"
      ]
    },
    "multi_worker_deployment": {
      "description": "Deploy workflow executors across multiple workers",
      "use_when": "High-throughput workflow processing with horizontal scaling",
      "code_example": "from langgraph.checkpoint.postgres import PostgresSaver\nfrom celery import Celery\n\n# Celery for task distribution\napp = Celery('workflows', broker='redis://localhost:6379/0')\n\n# Shared checkpointer\ncheckpointer = PostgresSaver.from_conn_string('postgresql://...')\n\n@app.task(bind=True, max_retries=3)\ndef execute_workflow(self, workflow_id: str, input_data: dict):\n    try:\n        graph = create_workflow().compile(checkpointer=checkpointer)\n        config = {'configurable': {'thread_id': workflow_id}}\n        return graph.invoke(input_data, config)\n    except Exception as exc:\n        self.retry(exc=exc, countdown=2 ** self.request.retries)",
      "best_practices": [
        "Use Celery or similar task queues for work distribution",
        "Configure retry logic with exponential backoff",
        "Monitor worker health and queue depth",
        "Use separate queues for different workflow priorities"
      ]
    },
    "enhanced_time_travel": {
      "description": "Advanced time-travel debugging with checkpoint inspection and replay",
      "use_when": "Debugging complex workflows or investigating failures",
      "code_example": "from langgraph.checkpoint.postgres import PostgresSaver\nfrom datetime import datetime\n\ncheckpointer = PostgresSaver.from_conn_string('postgresql://...')\ngraph = builder.compile(checkpointer=checkpointer)\n\nclass WorkflowDebugger:\n    def __init__(self, checkpointer, graph):\n        self.checkpointer = checkpointer\n        self.graph = graph\n    \n    def list_checkpoints(self, thread_id: str):\n        config = {'configurable': {'thread_id': thread_id}}\n        return list(self.checkpointer.list(config))\n    \n    def inspect_state_at(self, thread_id: str, checkpoint_id: str):\n        config = {\n            'configurable': {\n                'thread_id': thread_id,\n                'checkpoint_id': checkpoint_id\n            }\n        }\n        checkpoint = self.checkpointer.get(config)\n        return {\n            'state': checkpoint['channel_values'],\n            'metadata': checkpoint.get('metadata', {}),\n            'step': checkpoint.get('step', 0)\n        }\n    \n    def replay_from(self, thread_id: str, checkpoint_id: str, new_input: dict = None):\n        config = {\n            'configurable': {\n                'thread_id': f'{thread_id}-replay-{datetime.now().isoformat()}',\n                'checkpoint_id': checkpoint_id\n            }\n        }\n        return self.graph.invoke(new_input, config)\n    \n    def compare_checkpoints(self, thread_id: str, cp_id_1: str, cp_id_2: str):\n        state_1 = self.inspect_state_at(thread_id, cp_id_1)['state']\n        state_2 = self.inspect_state_at(thread_id, cp_id_2)['state']\n        diff = {}\n        all_keys = set(state_1.keys()) | set(state_2.keys())\n        for key in all_keys:\n            if state_1.get(key) != state_2.get(key):\n                diff[key] = {'before': state_1.get(key), 'after': state_2.get(key)}\n        return diff",
      "best_practices": [
        "Store checkpoints in persistent storage for post-mortem debugging",
        "Add metadata to checkpoints (user_id, session_id, timestamp)",
        "Create replay threads with unique IDs to avoid conflicts",
        "Use checkpoint comparison to identify where workflows diverged",
        "Implement checkpoint cleanup policies to manage storage"
      ]
    },
    "supervisor_routing": {
      "description": "Advanced supervisor patterns for multi-agent routing",
      "use_when": "Complex task delegation across specialized agents",
      "code_example": "from langgraph.graph import StateGraph, MessagesState, START, END\nfrom pydantic import BaseModel\nfrom typing import Literal\n\nclass RouteDecision(BaseModel):\n    next_agent: Literal['researcher', 'writer', 'reviewer', 'done']\n    reasoning: str\n\ndef create_supervisor_graph(llm, agents: dict):\n    builder = StateGraph(MessagesState)\n    \n    # Supervisor node that routes to workers\n    def supervisor(state: MessagesState):\n        structured_llm = llm.with_structured_output(RouteDecision)\n        decision = structured_llm.invoke([\n            ('system', '''You are a supervisor routing tasks to specialists:\n            - researcher: gathers information\n            - writer: creates content\n            - reviewer: checks quality\n            - done: task is complete'''),\n            *state['messages']\n        ])\n        return {'next': decision.next_agent, 'reasoning': decision.reasoning}\n    \n    # Add supervisor and worker nodes\n    builder.add_node('supervisor', supervisor)\n    for name, agent_fn in agents.items():\n        builder.add_node(name, agent_fn)\n    \n    # Conditional routing from supervisor\n    def route_supervisor(state):\n        return state.get('next', 'done')\n    \n    builder.add_conditional_edges(\n        'supervisor',\n        route_supervisor,\n        {name: name for name in agents.keys()} | {'done': END}\n    )\n    \n    # Workers return to supervisor\n    for name in agents.keys():\n        builder.add_edge(name, 'supervisor')\n    \n    builder.add_edge(START, 'supervisor')\n    return builder",
      "best_practices": [
        "Use structured output for routing decisions",
        "Include reasoning in routing for observability",
        "Implement max_iterations to prevent infinite routing loops",
        "Add fallback routing for unknown or error cases",
        "Log routing decisions for debugging and optimization"
      ]
    },
    "durable_execution": {
      "description": "Ensure workflows complete even with failures",
      "use_when": "Mission-critical workflows that must not lose progress",
      "code_example": "from langgraph.checkpoint.postgres import PostgresSaver\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\ncheckpointer = PostgresSaver.from_conn_string('postgresql://...')\n\nclass DurableWorkflowExecutor:\n    def __init__(self, graph_builder, checkpointer):\n        self.graph = graph_builder.compile(checkpointer=checkpointer)\n        self.checkpointer = checkpointer\n    \n    @retry(\n        stop=stop_after_attempt(5),\n        wait=wait_exponential(multiplier=1, min=1, max=60)\n    )\n    def execute_durable(self, input_data: dict, thread_id: str):\n        config = {'configurable': {'thread_id': thread_id}}\n        \n        # Check if workflow already started\n        existing = self.checkpointer.get(config)\n        if existing:\n            # Resume from last checkpoint\n            return self.graph.invoke(None, config)\n        else:\n            # Start fresh\n            return self.graph.invoke(input_data, config)\n    \n    def recover_failed_workflows(self, max_age_hours: int = 24):\n        # Query checkpointer for incomplete workflows\n        # Implementation depends on checkpointer capabilities\n        pass",
      "best_practices": [
        "Use PostgresSaver for production durability",
        "Implement automatic retry with exponential backoff",
        "Check for existing checkpoints before starting",
        "Monitor and alert on stuck workflows",
        "Implement recovery procedures for failed workflows"
      ]
    }
  },
  "anti_patterns": [
    {
      "name": "State with too many fields",
      "problem": "Hard to track, prone to bugs",
      "fix": "Keep state minimal, use nested structures for organization"
    },
    {
      "name": "Nodes that modify external state",
      "problem": "Hard to test, unpredictable",
      "fix": "Make nodes pure when possible, isolate side effects"
    },
    {
      "name": "No handling for node failures",
      "problem": "Silent failures, stuck workflows",
      "fix": "Add error nodes, implement fallback paths"
    },
    {
      "name": "Single-instance checkpoint store",
      "problem": "Cannot scale horizontally, single point of failure",
      "fix": "Use PostgresSaver or other distributed checkpoint stores for production"
    },
    {
      "name": "Missing distributed locks",
      "problem": "Race conditions when multiple workers process same workflow",
      "fix": "Implement distributed locking with Redis or similar"
    }
  ],
  "best_practices": [
    "Use TypedDict with Annotated reducers for state management to ensure type safety and proper state merging",
    "Always include a default branch in conditional edges to handle unexpected routing scenarios",
    "Set max_iterations or max_rounds on all agent nodes to prevent infinite loops",
    "Use persistent checkpointing (SqliteSaver or PostgresSaver) for production workflows to enable resumption and debugging",
    "Implement error nodes and fallback paths for graceful failure handling instead of letting workflows crash",
    "Use subgraphs to decompose complex workflows into reusable, testable components",
    "Add interrupt_before or interrupt_after for human-in-the-loop approval on critical decisions",
    "Visualize graph structure with graph.get_graph().draw_mermaid_png() during development for debugging",
    "Use PostgresSaver with Redis locking for distributed multi-worker deployments",
    "Implement durable execution patterns with automatic retry for mission-critical workflows",
    "Add checkpoint metadata (user_id, session_id) to enable filtering and debugging",
    "Use structured output in supervisor nodes for reliable routing decisions"
  ],
  "sources": [
    "https://langchain-ai.github.io/langgraph/concepts/multi_agent/",
    "https://pypi.org/project/langgraph/",
    "https://jangwook.net/en/blog/en/langgraph-multi-agent/"
  ],
  "last_updated": "2026-02-11"
}