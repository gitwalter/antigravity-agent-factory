{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "id": "vector-database-patterns",
  "name": "Vector Database Patterns",
  "title": "Vector Database Patterns",
  "description": "Patterns for vector databases used in RAG, semantic search, and AI applications",
  "version": "1.0.0",
  "category": "rag",
  "axiomAlignment": {
    "A1_verifiability": "Includes retrieval quality metrics and evaluation patterns",
    "A2_user_primacy": "Database selection respects user scale and latency needs",
    "A3_transparency": "Search parameters and indexing strategies are explicit",
    "A4_non_harm": "Filtering and access control protect data and outputs",
    "A5_consistency": "Unified vector database patterns across RAG applications"
  },
  "related_skills": [
    "rag-patterns",
    "advanced-retrieval",
    "ai-system-design"
  ],
  "related_knowledge": [
    "rag-patterns.json",
    "embedding-models.json",
    "advanced-rag-patterns.json"
  ],
  "database_comparison": {
    "overview": {
      "description": "Comparison of popular vector databases",
      "databases": {
        "chromadb": {
          "type": "Embedded / Client-Server",
          "best_for": "Prototyping, small-medium datasets, local development",
          "hosting": "Self-hosted, embedded",
          "max_vectors": "~1M (embedded), more with server",
          "features": [
            "Simple API",
            "Persistent storage",
            "Metadata filtering"
          ],
          "pros": [
            "Easy setup",
            "No infrastructure needed",
            "Good for RAG prototypes"
          ],
          "cons": [
            "Limited scale",
            "No managed option",
            "Basic search features"
          ]
        },
        "qdrant": {
          "type": "Client-Server",
          "best_for": "Production RAG, hybrid search, complex filtering",
          "hosting": "Self-hosted, Qdrant Cloud",
          "max_vectors": "Billions",
          "features": [
            "Hybrid search",
            "Rich filtering",
            "Quantization",
            "Multi-vector"
          ],
          "pros": [
            "Excellent performance",
            "Rich features",
            "Good documentation"
          ],
          "cons": [
            "More complex setup than Chroma"
          ]
        },
        "pinecone": {
          "type": "Managed SaaS",
          "best_for": "Enterprise, fully managed, minimal ops",
          "hosting": "Managed cloud only",
          "max_vectors": "Billions",
          "features": [
            "Managed infrastructure",
            "Namespaces",
            "Metadata filtering"
          ],
          "pros": [
            "Zero ops",
            "Enterprise support",
            "Easy scaling"
          ],
          "cons": [
            "Vendor lock-in",
            "Can be expensive",
            "Less control"
          ]
        },
        "weaviate": {
          "type": "Client-Server",
          "best_for": "GraphQL fans, multi-modal, complex schemas",
          "hosting": "Self-hosted, Weaviate Cloud",
          "max_vectors": "Billions",
          "features": [
            "GraphQL API",
            "Multi-modal",
            "Modules",
            "Hybrid search"
          ],
          "pros": [
            "Flexible schema",
            "Multi-modal support",
            "Active community"
          ],
          "cons": [
            "Steeper learning curve",
            "Resource intensive"
          ]
        },
        "milvus": {
          "type": "Client-Server",
          "best_for": "Large scale, enterprise, Kubernetes native",
          "hosting": "Self-hosted, Zilliz Cloud",
          "max_vectors": "Billions+",
          "features": [
            "Distributed",
            "GPU acceleration",
            "Multiple indexes"
          ],
          "pros": [
            "Massive scale",
            "Enterprise features",
            "Open source"
          ],
          "cons": [
            "Complex setup",
            "Heavy resource requirements"
          ]
        },
        "pgvector": {
          "type": "PostgreSQL Extension",
          "best_for": "Existing PostgreSQL users, simple use cases",
          "hosting": "Any PostgreSQL host",
          "max_vectors": "Millions",
          "features": [
            "SQL interface",
            "ACID transactions",
            "Joins with other data"
          ],
          "pros": [
            "Leverage existing Postgres",
            "SQL familiarity",
            "Transactions"
          ],
          "cons": [
            "Performance limits at scale",
            "Limited vector features"
          ]
        },
        "faiss": {
          "type": "Library (In-Memory / File-Based)",
          "best_for": "Research, prototyping, small-medium datasets",
          "hosting": "Local, embedded",
          "max_vectors": "Millions (depends on RAM)",
          "features": [
            "Multiple index types",
            "GPU support",
            "Fast search",
            "No server needed"
          ],
          "pros": [
            "Very fast",
            "No infrastructure",
            "GPU acceleration",
            "Good for research"
          ],
          "cons": [
            "In-memory limitations",
            "No distributed support",
            "Manual persistence"
          ]
        },
        "elasticsearch": {
          "type": "Search Engine with Vector Support",
          "best_for": "Existing Elasticsearch users, hybrid search",
          "hosting": "Self-hosted, Elastic Cloud",
          "max_vectors": "Billions",
          "features": [
            "Hybrid BM25+vector",
            "Full-text search",
            "Rich filtering",
            "Distributed"
          ],
          "pros": [
            "Excellent hybrid search",
            "Mature ecosystem",
            "Rich query DSL",
            "Production-ready"
          ],
          "cons": [
            "More complex setup",
            "Resource intensive",
            "Steeper learning curve"
          ]
        },
        "redis": {
          "type": "In-Memory Database with Vector Search",
          "best_for": "Real-time applications, caching, hybrid queries",
          "hosting": "Self-hosted, Redis Cloud",
          "max_vectors": "Millions",
          "features": [
            "Vector search (RediSearch)",
            "HNSW/FLAT indexes",
            "Hybrid queries",
            "JSON documents"
          ],
          "pros": [
            "Very fast",
            "Real-time updates",
            "Good for caching",
            "Hybrid search support"
          ],
          "cons": [
            "Memory limitations",
            "Cost at scale",
            "Less specialized than dedicated vector DBs"
          ]
        }
      },
      "use_when": "RAG applications, semantic search, or vector similarity retrieval",
      "code_example": "from langchain_qdrant import QdrantVectorStore\nstore = QdrantVectorStore.from_documents(docs, embeddings, path='./data/rag/qdrant_db', collection_name='docs')",
      "best_practices": [
        "Validate inputs and handle errors appropriately",
        "Document the pattern and when to use it"
      ]
    },
    "selection_guide": {
      "prototyping": "Qdrant (Local) - minimal setup, production-ready, good for development",
      "production_rag": "Qdrant (Server/Cloud) - balance of features and reliability",
      "enterprise_scale": "Milvus or Qdrant - proven at large scale",
      "existing_postgres": "pgvector - leverage existing infrastructure",
      "multimodal": "Weaviate - built-in multimodal support",
      "hybrid_search": "Qdrant - excellent BM25+vector support",
      "real_time": "Redis - very fast, good for real-time applications",
      "research": "Qdrant - fast, flexible, no infrastructure needed in local mode"
    }
  },
  "chromadb_patterns": {
    "basic_usage": {
      "description": "Basic ChromaDB setup and operations",
      "use_when": "Local development, prototyping RAG",
      "code_example": "import chromadb\nfrom chromadb.utils import embedding_functions\n\n# Create client (persistent storage)\nclient = chromadb.PersistentClient(path='./chroma_db')\n\n# Use OpenAI embeddings\nopenai_ef = embedding_functions.OpenAIEmbeddingFunction(\n    api_key='your-api-key',\n    model_name='text-embedding-3-small'\n)\n\n# Create or get collection\ncollection = client.get_or_create_collection(\n    name='documents',\n    embedding_function=openai_ef,\n    metadata={'hnsw:space': 'cosine'}  # Distance metric\n)\n\n# Add documents\ncollection.add(\n    documents=['Document 1 content', 'Document 2 content'],\n    metadatas=[{'source': 'file1.pdf'}, {'source': 'file2.pdf'}],\n    ids=['doc1', 'doc2']\n)\n\n# Query\nresults = collection.query(\n    query_texts=['What is the main topic?'],\n    n_results=5,\n    where={'source': 'file1.pdf'},  # Metadata filter\n    include=['documents', 'metadatas', 'distances']\n)\n\nprint(results['documents'][0])  # Top results\nprint(results['distances'][0])  # Similarity scores",
      "best_practices": [
        "Use PersistentClient for data durability",
        "Choose appropriate distance metric (cosine for normalized)",
        "Use metadata filters to narrow search space",
        "Batch adds for better performance"
      ]
    },
    "with_langchain": {
      "description": "ChromaDB with LangChain integration",
      "use_when": "Building RAG with LangChain",
      "code_example": "from langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\n\n# Load and split documents\nloader = PyPDFLoader('document.pdf')\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\nsplits = text_splitter.split_documents(documents)\n\n# Create vector store\nembeddings = OpenAIEmbeddings(model='text-embedding-3-small')\nvectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    persist_directory='./chroma_db',\n    collection_name='my_collection'\n)\n\n# Create retriever\nretriever = vectorstore.as_retriever(\n    search_type='similarity',\n    search_kwargs={'k': 5}\n)\n\n# Query\ndocs = retriever.invoke('What is the main topic?')\nfor doc in docs:\n    print(doc.page_content[:200])",
      "best_practices": [
        "Validate inputs and handle errors appropriately",
        "Document the pattern and when to use it"
      ]
    }
  },
  "qdrant_patterns": {
    "basic_usage": {
      "description": "Basic Qdrant setup and operations",
      "use_when": "Production RAG, need advanced features",
      "code_example": "from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\nimport openai\n\n# Connect to Qdrant\nclient = QdrantClient(path='./qdrant_db')  # Local\n# client = QdrantClient(url='http://localhost:6333')  # Server\n# client = QdrantClient(url='https://xxx.qdrant.io', api_key='...')  # Cloud\n\n# Create collection\nclient.create_collection(\n    collection_name='documents',\n    vectors_config=VectorParams(\n        size=1536,  # OpenAI embedding dimension\n        distance=Distance.COSINE\n    )\n)\n\n# Generate embeddings\ndef get_embedding(text):\n    response = openai.embeddings.create(\n        input=text,\n        model='text-embedding-3-small'\n    )\n    return response.data[0].embedding\n\n# Upsert points\npoints = [\n    PointStruct(\n        id=1,\n        vector=get_embedding('Document 1 content'),\n        payload={'source': 'file1.pdf', 'page': 1, 'text': 'Document 1 content'}\n    ),\n    PointStruct(\n        id=2,\n        vector=get_embedding('Document 2 content'),\n        payload={'source': 'file2.pdf', 'page': 1, 'text': 'Document 2 content'}\n    )\n]\n\nclient.upsert(collection_name='documents', points=points)\n\n# Search\nresults = client.search(\n    collection_name='documents',\n    query_vector=get_embedding('What is the main topic?'),\n    limit=5,\n    query_filter={\n        'must': [{'key': 'source', 'match': {'value': 'file1.pdf'}}]\n    }\n)\n\nfor result in results:\n    print(f'Score: {result.score:.4f}')\n    print(f'Text: {result.payload[\"text\"]}')",
      "best_practices": [
        "Use payload for metadata storage",
        "Implement proper error handling",
        "Use batch operations for bulk inserts",
        "Consider quantization for large datasets"
      ]
    },
    "hybrid_search": {
      "description": "Combine vector and keyword search",
      "use_when": "Need both semantic and keyword matching",
      "code_example": "from qdrant_client import QdrantClient, models\nfrom qdrant_client.models import SparseVector, Distance, VectorParams\n\n# Create collection with dense and sparse vectors\nclient.create_collection(\n    collection_name='hybrid_docs',\n    vectors_config={\n        'dense': VectorParams(\n            size=1536,  # OpenAI embedding dimension\n            distance=Distance.COSINE\n        )\n    },\n    sparse_vectors_config={\n        'sparse': models.SparseVectorParams()\n    }\n)\n\n# Generate dense embedding\ndef get_embedding(text):\n    response = openai.embeddings.create(\n        input=text,\n        model='text-embedding-3-small'\n    )\n    return response.data[0].embedding\n\n# Generate sparse vector (BM25-like)\ndef create_sparse_vector(text, token_to_index):\n    from collections import Counter\n    tokens = text.lower().split()\n    tf = Counter(tokens)\n    indices = []\n    values = []\n    for token in tokens:\n        if token in token_to_index:\n            idx = token_to_index[token]\n            if idx not in indices:\n                indices.append(idx)\n                values.append(tf[token])\n    return SparseVector(indices=indices, values=values)\n\n# Hybrid search with RRF (Reciprocal Rank Fusion)\nquery = 'machine learning algorithms'\ndense_vector = get_embedding(query)\nsparse_vector = create_sparse_vector(query, token_vocab)\n\nresults = client.query_points(\n    collection_name='hybrid_docs',\n    prefetch=[\n        models.Prefetch(\n            query=dense_vector,\n            using='dense',\n            limit=20\n        ),\n        models.Prefetch(\n            query=sparse_vector,\n            using='sparse',\n            limit=20\n        )\n    ],\n    query=models.FusionQuery(fusion=models.Fusion.RRF),\n    limit=10,\n    with_payload=True\n)\n\n# Alternative: Weighted hybrid search\nresults = client.query_points(\n    collection_name='hybrid_docs',\n    prefetch=[\n        models.Prefetch(\n            query=dense_vector,\n            using='dense',\n            limit=20\n        ),\n        models.Prefetch(\n            query=sparse_vector,\n            using='sparse',\n            limit=20\n        )\n    ],\n    query=models.FusionQuery(\n        fusion=models.Fusion.RRF,\n        rrf_k=60  # RRF parameter, higher = more balanced\n    ),\n    limit=10\n)",
      "best_practices": [
        "Hybrid search improves recall significantly",
        "Use RRF (Reciprocal Rank Fusion) to combine dense and sparse results",
        "Tune rrf_k parameter: lower (20-40) favors top results, higher (60-100) more balanced",
        "Dense vectors capture semantic meaning, sparse capture exact keywords",
        "Use sparse vectors for technical terms, proper nouns, and exact matches",
        "Consider collection-specific hybrid configurations"
      ]
    },
    "collection_management": {
      "description": "Collection lifecycle management patterns",
      "use_when": "Managing collections, updates, and maintenance",
      "code_example": "from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, OptimizersConfigDiff\n\n# Collection creation with optimizers\nclient.create_collection(\n    collection_name='documents',\n    vectors_config=VectorParams(\n        size=1536,\n        distance=Distance.COSINE\n    ),\n    optimizers_config=OptimizersConfigDiff(\n        indexing_threshold=20000,  # Index after 20k vectors\n        memmap_threshold=50000  # Use memmap after 50k vectors\n    )\n)\n\n# Check collection info\ncollection_info = client.get_collection('documents')\nprint(f'Vectors count: {collection_info.vectors_count}')\nprint(f'Indexed: {collection_info.indexed_vectors_count}')\n\n# Update collection parameters\nclient.update_collection(\n    collection_name='documents',\n    optimizers_config=OptimizersConfigDiff(\n        indexing_threshold=10000\n    )\n)\n\n# Create collection snapshot\nsnapshot_info = client.create_snapshot('documents')\nprint(f'Snapshot: {snapshot_info.name}')\n\n# List collections\ncollections = client.get_collections()\nfor collection in collections.collections:\n    info = client.get_collection(collection.name)\n    print(f'{collection.name}: {info.vectors_count} vectors')\n\n# Delete collection\nclient.delete_collection('old_collection')\n\n# Collection aliases (for zero-downtime updates)\nclient.create_alias(\n    collection_name='documents_v2',\n    alias_name='documents'  # Point alias to new collection\n)\n\n# Update alias to new collection\nclient.update_collection_aliases(\n    change_aliases=[\n        models.CreateAlias(\n            collection_name='documents_v3',\n            alias_name='documents'\n        ),\n        models.DeleteAlias(\n            collection_name='documents_v2',\n            alias_name='documents'\n        )\n    ]\n)",
      "best_practices": [
        "Use aliases for zero-downtime collection updates",
        "Create snapshots before major updates",
        "Monitor collection size and indexing status",
        "Tune optimizer thresholds based on collection size",
        "Use separate collections for different data types or versions",
        "Implement collection versioning strategy"
      ]
    },
    "multi_vector": {
      "description": "Store multiple vectors per document (ColBERT style)",
      "use_when": "Need fine-grained matching within documents",
      "code_example": "from qdrant_client.models import MultiVectorConfig, VectorParams, Distance, PointStruct\n\n# Create collection with multi-vector support\nclient.create_collection(\n    collection_name='multi_vector_docs',\n    vectors_config={\n        'colbert': VectorParams(\n            size=128,\n            distance=Distance.COSINE,\n            multivector_config=MultiVectorConfig(\n                comparator=models.MultiVectorComparator.MAX_SIM\n            )\n        )\n    }\n)\n\n# Each document has multiple token vectors\ntoken_vectors = [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]  # Per-token embeddings\n\nclient.upsert(\n    collection_name='multi_vector_docs',\n    points=[\n        PointStruct(\n            id=1,\n            vector={'colbert': token_vectors},\n            payload={'text': 'Document content'}\n        )\n    ]\n)\n\n# Search with multi-vector\nquery_token_vectors = [[0.2, 0.3, ...], [0.4, 0.5, ...]]\n\nresults = client.search(\n    collection_name='multi_vector_docs',\n    query_vector=models.NamedVector(\n        name='colbert',\n        vector=query_token_vectors\n    ),\n    limit=10\n)",
      "best_practices": [
        "Validate inputs and handle errors appropriately",
        "Document the pattern and when to use it"
      ]
    },
    "metadata_filtering": {
      "description": "Advanced metadata filtering patterns",
      "use_when": "Need to filter search results by metadata attributes",
      "code_example": "from qdrant_client.models import Filter, FieldCondition, MatchValue, MatchText, Range, GeoRadius\n\n# Simple filter: exact match\nfilter_exact = Filter(\n    must=[\n        FieldCondition(\n            key='source',\n            match=MatchValue(value='annual_report.pdf')\n        )\n    ]\n)\n\n# Multiple conditions (AND)\nfilter_and = Filter(\n    must=[\n        FieldCondition(key='source', match=MatchValue(value='report.pdf')),\n        FieldCondition(key='year', range=Range(gte=2023, lte=2024)),\n        FieldCondition(key='category', match=MatchValue(value='financial'))\n    ]\n)\n\n# OR conditions\nfilter_or = Filter(\n    should=[\n        FieldCondition(key='category', match=MatchValue(value='technical')),\n        FieldCondition(key='category', match=MatchValue(value='financial'))\n    ]\n)\n\n# Complex: (A AND B) OR (C AND D)\nfilter_complex = Filter(\n    should=[\n        Filter(\n            must=[\n                FieldCondition(key='source', match=MatchValue(value='doc1.pdf')),\n                FieldCondition(key='year', range=Range(gte=2023))\n            ]\n        ),\n        Filter(\n            must=[\n                FieldCondition(key='source', match=MatchValue(value='doc2.pdf')),\n                FieldCondition(key='year', range=Range(gte=2024))\n            ]\n        )\n    ]\n)\n\n# Text search in metadata\nfilter_text = Filter(\n    must=[\n        FieldCondition(\n            key='description',\n            match=MatchText(text='machine learning')\n        )\n    ]\n)\n\n# Array contains\nfilter_array = Filter(\n    must=[\n        FieldCondition(\n            key='tags',\n            match=MatchValue(value='python')\n        )\n    ]\n)\n\n# Geo-spatial filtering\nfilter_geo = Filter(\n    must=[\n        FieldCondition(\n            key='location',\n            geo_radius=GeoRadius(\n                center={'lat': 40.7128, 'lon': -74.0060},\n                radius=1000  # meters\n            )\n        )\n    ]\n)\n\n# Search with filter\nresults = client.search(\n    collection_name='documents',\n    query_vector=query_vector,\n    query_filter=filter_and,\n    limit=10\n)\n\n# Filter with LangChain\nfrom langchain_community.vectorstores import Qdrant\n\nvectorstore = Qdrant.from_documents(docs, embeddings)\n\nretriever = vectorstore.as_retriever(\n    search_kwargs={\n        'k': 5,\n        'filter': {\n            'source': 'report.pdf',\n            'year': {'$gte': 2023}\n        }\n    }\n)",
      "best_practices": [
        "Use filters to narrow search space before vector search",
        "Index frequently filtered metadata fields",
        "Combine multiple filters with AND/OR logic",
        "Use range filters for numeric metadata",
        "Text matching in metadata requires proper indexing",
        "Filters are applied before vector search for efficiency",
        "Store commonly filtered attributes as metadata"
      ]
    }
  },
  "pinecone_patterns": {
    "basic_usage": {
      "description": "Basic Pinecone setup and operations",
      "use_when": "Managed vector database, enterprise use",
      "code_example": "from pinecone import Pinecone, ServerlessSpec\nimport openai\n\n# Initialize Pinecone\npc = Pinecone(api_key='your-api-key')\n\n# Create index\npc.create_index(\n    name='documents',\n    dimension=1536,\n    metric='cosine',\n    spec=ServerlessSpec(\n        cloud='aws',\n        region='us-east-1'\n    )\n)\n\n# Connect to index\nindex = pc.Index('documents')\n\n# Generate embeddings\ndef get_embedding(text):\n    response = openai.embeddings.create(\n        input=text,\n        model='text-embedding-3-small'\n    )\n    return response.data[0].embedding\n\n# Upsert vectors\nvectors = [\n    {\n        'id': 'doc1',\n        'values': get_embedding('Document 1 content'),\n        'metadata': {'source': 'file1.pdf', 'page': 1}\n    },\n    {\n        'id': 'doc2',\n        'values': get_embedding('Document 2 content'),\n        'metadata': {'source': 'file2.pdf', 'page': 1}\n    }\n]\n\nindex.upsert(vectors=vectors, namespace='default')\n\n# Query\nresults = index.query(\n    vector=get_embedding('What is the main topic?'),\n    top_k=5,\n    include_metadata=True,\n    filter={'source': {'$eq': 'file1.pdf'}}\n)\n\nfor match in results.matches:\n    print(f'Score: {match.score:.4f}')\n    print(f'Metadata: {match.metadata}')",
      "best_practices": [
        "Use namespaces to organize data",
        "Batch upserts for efficiency (up to 100 vectors)",
        "Use metadata filtering to narrow search",
        "Monitor usage for cost management"
      ]
    }
  },
  "milvus_patterns": {
    "basic_usage": {
      "description": "Basic Milvus setup and operations",
      "use_when": "Large scale production, need distributed architecture",
      "code_example": "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility\nimport numpy as np\n\n# Connect to Milvus\nconnections.connect(\n    alias='default',\n    host='localhost',\n    port='19530'\n)\n\n# Define schema\nfields = [\n    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n    FieldSchema(name='text', dtype=DataType.VARCHAR, max_length=1000),\n    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=1536),\n    FieldSchema(name='source', dtype=DataType.VARCHAR, max_length=200),\n    FieldSchema(name='page', dtype=DataType.INT64)\n]\n\nschema = CollectionSchema(\n    fields=fields,\n    description='Document collection'\n)\n\n# Create collection\ncollection = Collection(\n    name='documents',\n    schema=schema\n)\n\n# Create index\nindex_params = {\n    'metric_type': 'COSINE',\n    'index_type': 'IVF_FLAT',\n    'params': {'nlist': 1024}\n}\n\ncollection.create_index(\n    field_name='embedding',\n    index_params=index_params\n)\n\n# Load collection\ncollection.load()\n\n# Insert data\nentities = [\n    ['Document 1', 'Document 2'],  # text\n    [[0.1]*1536, [0.2]*1536],  # embeddings\n    ['file1.pdf', 'file2.pdf'],  # source\n    [1, 2]  # page\n]\n\ncollection.insert(entities)\ncollection.flush()  # Ensure data is written\n\n# Search\nsearch_params = {\n    'metric_type': 'COSINE',\n    'params': {'nprobe': 10}\n}\n\nquery_vector = [[0.15]*1536]  # Query embedding\n\nresults = collection.search(\n    data=query_vector,\n    anns_field='embedding',\n    param=search_params,\n    limit=5,\n    expr='source == \"file1.pdf\"',  # Metadata filter\n    output_fields=['text', 'source', 'page']\n)\n\nfor hits in results:\n    for hit in hits:\n        print(f'ID: {hit.id}, Score: {hit.score:.4f}')\n        print(f'Text: {hit.entity.get(\"text\")}')",
      "best_practices": [
        "Use appropriate index type: IVF_FLAT for small, HNSW for large datasets",
        "Tune nprobe parameter for search quality vs speed tradeoff",
        "Use partitions for logical data separation",
        "Load collection before searching",
        "Flush after inserts to ensure persistence",
        "Use expr parameter for metadata filtering"
      ]
    },
    "collection_management": {
      "description": "Milvus collection and partition management",
      "code_example": "from pymilvus import Collection, utility\n\n# List collections\ncollections = utility.list_collections()\nprint(f'Collections: {collections}')\n\n# Check if collection exists\nif utility.has_collection('documents'):\n    collection = Collection('documents')\n\n# Get collection stats\nstats = collection.num_entities\nprint(f'Total entities: {stats}')\n\n# Create partition\ncollection.create_partition('partition_2024')\n\n# Insert into partition\ncollection.insert(\n    entities=entities,\n    partition_name='partition_2024'\n)\n\n# Search in partition\nresults = collection.search(\n    data=query_vector,\n    anns_field='embedding',\n    param=search_params,\n    limit=5,\n    partition_names=['partition_2024']\n)\n\n# Release collection from memory\ncollection.release()\n\n# Drop collection\nutility.drop_collection('old_collection')",
      "best_practices": [
        "Use partitions for time-based or category-based data separation",
        "Release collections when not in use to free memory",
        "Monitor collection statistics regularly",
        "Use aliases for collection versioning"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    },
    "hybrid_search": {
      "description": "Hybrid search in Milvus",
      "code_example": "from pymilvus import Collection, connections\n\n# Connect\nconnections.connect(alias='default', host='localhost', port='19530')\ncollection = Collection('hybrid_docs')\ncollection.load()\n\n# Define search with both vector and scalar fields\nsearch_params = {\n    'metric_type': 'COSINE',\n    'params': {'nprobe': 10}\n}\n\n# Hybrid search: vector similarity + metadata filtering\nresults = collection.search(\n    data=query_vector,\n    anns_field='embedding',\n    param=search_params,\n    limit=10,\n    expr='source == \"report.pdf\" && year >= 2023',  # Metadata filter\n    output_fields=['text', 'source', 'year']\n)\n\n# For true hybrid (dense + sparse), use multiple collections\n# or implement RRF (Reciprocal Rank Fusion) at application level",
      "best_practices": [
        "Use expr parameter for efficient metadata filtering",
        "Combine vector search with scalar field filters",
        "For dense+sparse hybrid, use application-level RRF",
        "Index scalar fields used in filters for performance"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    }
  },
  "weaviate_patterns": {
    "basic_usage": {
      "description": "Basic Weaviate setup and operations",
      "use_when": "Need GraphQL API, multi-modal support, or flexible schema",
      "code_example": "import weaviate\nfrom weaviate.classes.config import Configure, Property, DataType\nfrom weaviate.classes.query import QueryReturn, MetadataQuery\n\n# Connect to Weaviate\nclient = weaviate.connect_to_local()\n# client = weaviate.connect_to_wcs(\n#     cluster_url='https://xxx.weaviate.network',\n#     auth_credentials=weaviate.auth.AuthApiKey('your-api-key')\n# )\n\n# Define schema\nclient.collections.create(\n    name='Document',\n    properties=[\n        Property(name='text', data_type=DataType.TEXT),\n        Property(name='source', data_type=DataType.TEXT),\n        Property(name='page', data_type=DataType.INT),\n        Property(name='category', data_type=DataType.TEXT)\n    ],\n    vectorizer_config=Configure.Vectorizer.text2vec_openai(\n        model='text-embedding-3-small',\n        api_key='your-openai-key'\n    )\n)\n\n# Get collection\ndocuments = client.collections.get('Document')\n\n# Insert data\nwith documents.batch.dynamic() as batch:\n    batch.add_object(\n        properties={\n            'text': 'Document 1 content',\n            'source': 'file1.pdf',\n            'page': 1,\n            'category': 'technical'\n        }\n    )\n    batch.add_object(\n        properties={\n            'text': 'Document 2 content',\n            'source': 'file2.pdf',\n            'page': 1,\n            'category': 'financial'\n        }\n    )\n\n# Query with GraphQL-like syntax\nresponse = documents.query.near_text(\n    query='machine learning algorithms',\n    limit=5,\n    return_metadata=MetadataQuery(distance=True),\n    where={\n        'path': ['category'],\n        'operator': 'Equal',\n        'valueText': 'technical'\n    }\n)\n\nfor obj in response.objects:\n    print(f'Distance: {obj.metadata.distance:.4f}')\n    print(f'Text: {obj.properties[\"text\"]}')\n    print(f'Source: {obj.properties[\"source\"]}')",
      "best_practices": [
        "Use batch operations for bulk inserts",
        "Leverage Weaviate's built-in vectorizers",
        "Use where filters for metadata filtering",
        "Monitor distance scores for quality assessment",
        "Use GraphQL API for complex queries"
      ]
    },
    "hybrid_search": {
      "description": "Hybrid search in Weaviate combining vector and keyword",
      "code_example": "import weaviate\nfrom weaviate.classes.query import HybridFusion\n\nclient = weaviate.connect_to_local()\ndocuments = client.collections.get('Document')\n\n# Hybrid search: combines vector and keyword search\nresponse = documents.query.hybrid(\n    query='machine learning algorithms',\n    alpha=0.7,  # 0=keyword only, 1=vector only, 0.5=balanced\n    limit=10,\n    fusion_type=HybridFusion.RELATIVE_SCORE,  # or RANKED\n    where={\n        'path': ['category'],\n        'operator': 'Equal',\n        'valueText': 'technical'\n    }\n)\n\nfor obj in response.objects:\n    print(f'Score: {obj.metadata.score:.4f}')\n    print(f'Text: {obj.properties[\"text\"]}')\n\n# Alternative: BM25 + vector with RRF\nresponse = documents.query.hybrid(\n    query='machine learning',\n    alpha=0.5,\n    limit=10,\n    fusion_type=HybridFusion.RANKED,  # Reciprocal Rank Fusion\n    return_metadata=MetadataQuery(score=True, explain_score=True)\n)",
      "best_practices": [
        "Tune alpha parameter: 0.7-0.8 for semantic-heavy, 0.3-0.4 for keyword-heavy",
        "Use RELATIVE_SCORE fusion for balanced results",
        "Use RANKED fusion (RRF) for better diversity",
        "Combine with where filters for metadata filtering",
        "Monitor explain_score for debugging"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    },
    "collection_management": {
      "description": "Weaviate collection and schema management",
      "code_example": "import weaviate\nfrom weaviate.classes.config import Configure\n\nclient = weaviate.connect_to_local()\n\n# List collections\ncollections = client.collections.list_all()\nprint(f'Collections: {collections}')\n\n# Check if collection exists\nif client.collections.exists('Document'):\n    documents = client.collections.get('Document')\n\n# Get collection configuration\nconfig = documents.config.get()\nprint(f'Vectorizer: {config.vectorizer}')\nprint(f'Properties: {config.properties}')\n\n# Update collection (add property)\n# Note: Schema updates may require re-indexing\n\n# Delete collection\nclient.collections.delete('old_collection')\n\n# Create collection with custom vectorizer\nclient.collections.create(\n    name='CustomDocument',\n    vectorizer_config=Configure.Vectorizer.none(),  # Manual vectors\n    properties=[\n        Property(name='text', data_type=DataType.TEXT)\n    ]\n)\n\n# Add vectors manually\ncustom_docs = client.collections.get('CustomDocument')\nwith custom_docs.batch.dynamic() as batch:\n    batch.add_object(\n        properties={'text': 'Content'},\n        vector=[0.1] * 1536  # Custom vector\n    )",
      "best_practices": [
        "Use built-in vectorizers when possible",
        "Schema changes may require data migration",
        "Use collection namespaces for multi-tenancy",
        "Monitor collection statistics",
        "Backup collections before major changes"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    },
    "metadata_filtering": {
      "description": "Advanced metadata filtering in Weaviate",
      "code_example": "import weaviate\n\nclient = weaviate.connect_to_local()\ndocuments = client.collections.get('Document')\n\n# Simple filter\nresponse = documents.query.fetch_objects(\n    where={\n        'path': ['source'],\n        'operator': 'Equal',\n        'valueText': 'report.pdf'\n    },\n    limit=10\n)\n\n# Multiple conditions (AND)\nresponse = documents.query.near_text(\n    query='machine learning',\n    limit=10,\n    where={\n        'operator': 'And',\n        'operands': [\n            {'path': ['category'], 'operator': 'Equal', 'valueText': 'technical'},\n            {'path': ['page'], 'operator': 'GreaterThanEqual', 'valueInt': 10}\n        ]\n    }\n)\n\n# OR conditions\nresponse = documents.query.near_text(\n    query='algorithms',\n    limit=10,\n    where={\n        'operator': 'Or',\n        'operands': [\n            {'path': ['category'], 'operator': 'Equal', 'valueText': 'technical'},\n            {'path': ['category'], 'operator': 'Equal', 'valueText': 'research'}\n        ]\n    }\n)\n\n# Range filter\nresponse = documents.query.near_text(\n    query='neural networks',\n    limit=10,\n    where={\n        'path': ['page'],\n        'operator': 'GreaterThanEqual',\n        'valueInt': 1\n    }\n)\n\n# Text contains\nresponse = documents.query.fetch_objects(\n    where={\n        'path': ['text'],\n        'operator': 'ContainsAny',\n        'valueText': ['machine learning', 'neural network']\n    }\n)",
      "best_practices": [
        "Use where filters to narrow search space",
        "Combine filters with vector search for precision",
        "Index frequently filtered properties",
        "Use And/Or operators for complex conditions",
        "Filters are applied before vector search"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    }
  },
  "faiss_patterns": {
    "basic_usage": {
      "description": "Basic FAISS setup and operations",
      "use_when": "Local development, research, prototyping",
      "code_example": "import faiss\nimport numpy as np\n\n# Create index (Flat index for exact search)\ndimension = 1536  # OpenAI embedding dimension\nindex = faiss.IndexFlatL2(dimension)  # L2 distance\n# index = faiss.IndexFlatIP(dimension)  # Inner product (for normalized vectors)\n\n# Generate sample vectors\nvectors = np.random.random((1000, dimension)).astype('float32')\n\n# Add vectors to index\nindex.add(vectors)\n\n# Search\nquery_vector = np.random.random((1, dimension)).astype('float32')\nk = 5  # Number of nearest neighbors\n\n# Search returns distances and indices\ndistances, indices = index.search(query_vector, k)\n\nprint(f'Nearest neighbors: {indices[0]}')\nprint(f'Distances: {distances[0]}')\n\n# Save index to disk\nfaiss.write_index(index, 'faiss_index.bin')\n\n# Load index from disk\nindex = faiss.read_index('faiss_index.bin')",
      "best_practices": [
        "Use IndexFlatL2 for unnormalized embeddings",
        "Use IndexFlatIP for normalized embeddings (cosine similarity)",
        "Convert vectors to float32 for FAISS",
        "Save index to disk for persistence"
      ]
    },
    "index_types": {
      "description": "Different FAISS index types for various use cases",
      "code_example": "import faiss\nimport numpy as np\n\ndimension = 1536\nnum_vectors = 100000\n\n# 1. Flat index (exact search, slow for large datasets)\nflat_index = faiss.IndexFlatL2(dimension)\n\n# 2. IVF (Inverted File Index) - approximate search\nnlist = 100  # Number of clusters\nquantizer = faiss.IndexFlatL2(dimension)\nivf_index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n\n# Train IVF index (required before adding vectors)\nvectors = np.random.random((num_vectors, dimension)).astype('float32')\nivf_index.train(vectors)\nivf_index.add(vectors)\nivf_index.nprobe = 10  # Search in 10 nearest clusters\n\n# 3. HNSW (Hierarchical Navigable Small World) - fast approximate search\nhnsw_index = faiss.IndexHNSWFlat(dimension, 32)  # 32 = M parameter\nhnsw_index.add(vectors)\n\n# 4. Product Quantization (PQ) - memory efficient\nm = 64  # Number of subquantizers\nbits = 8  # Bits per subquantizer\npq_index = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, bits)\npq_index.train(vectors)\npq_index.add(vectors)\n\n# Search\nquery_vector = np.random.random((1, dimension)).astype('float32')\ndistances, indices = ivf_index.search(query_vector, k=5)",
      "best_practices": [
        "Use Flat index for small datasets (<100K vectors)",
        "Use IVF for medium datasets (100K-10M vectors)",
        "Use HNSW for fast approximate search",
        "Use PQ for memory-constrained scenarios",
        "Train IVF and PQ indexes before adding vectors",
        "Tune nprobe for IVF (higher = more accurate but slower)"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    },
    "gpu_support": {
      "description": "Use GPU acceleration with FAISS",
      "code_example": "import faiss\nimport numpy as np\n\n# Check GPU availability\nres = faiss.StandardGpuResources()\n\n# Create index on CPU\ncpu_index = faiss.IndexFlatL2(1536)\n\n# Move to GPU\ngpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU 0\n\n# Add vectors\ndimension = 1536\nvectors = np.random.random((10000, dimension)).astype('float32')\ngpu_index.add(vectors)\n\n# Search on GPU\nquery_vector = np.random.random((1, dimension)).astype('float32')\ndistances, indices = gpu_index.search(query_vector, k=5)\n\n# Move back to CPU if needed\ncpu_index = faiss.index_gpu_to_cpu(gpu_index)",
      "best_practices": [
        "GPU significantly faster for large datasets",
        "Use GPU for batch operations",
        "Move to CPU for persistence",
        "Check GPU memory availability"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    },
    "with_langchain": {
      "description": "FAISS with LangChain integration",
      "code_example": "from langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Create FAISS vector store\nembeddings = OpenAIEmbeddings(model='text-embedding-3-small')\nvectorstore = FAISS.from_texts(\n    texts=['Document 1', 'Document 2', 'Document 3'],\n    embedding=embeddings\n)\n\n# Save to disk\nvectorstore.save_local('./faiss_index')\n\n# Load from disk\nvectorstore = FAISS.load_local('./faiss_index', embeddings)\n\n# Similarity search\nresults = vectorstore.similarity_search('query', k=5)\n\n# With metadata\nvectorstore = FAISS.from_documents(\n    documents=documents,\n    embedding=embeddings\n)\n\n# Filtered search\nresults = vectorstore.similarity_search(\n    'query',\n    k=5,\n    filter={'source': 'doc1.pdf'}\n)",
      "best_practices": [
        "Use FAISS for local development",
        "Save index regularly",
        "Consider Chroma or Qdrant for production",
        "FAISS doesn't support advanced metadata filtering"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    }
  },
  "elasticsearch_patterns": {
    "basic_usage": {
      "description": "Basic Elasticsearch vector search setup",
      "use_when": "Need hybrid search, existing Elasticsearch infrastructure",
      "code_example": "from elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk\nimport numpy as np\n\n# Connect to Elasticsearch\nclient = Elasticsearch(['http://localhost:9200'])\n\n# Create index with dense_vector mapping\nindex_name = 'documents'\n\nmapping = {\n    'properties': {\n        'text': {'type': 'text'},\n        'embedding': {\n            'type': 'dense_vector',\n            'dims': 1536,\n            'index': True,\n            'similarity': 'cosine'\n        },\n        'source': {'type': 'keyword'},\n        'page': {'type': 'integer'}\n    }\n}\n\nclient.indices.create(index=index_name, mappings=mapping)\n\n# Index documents with embeddings\ndef get_embedding(text):\n    # Your embedding function\n    return np.random.random(1536).tolist()\n\ndocuments = [\n    {'text': 'Document 1', 'source': 'file1.pdf', 'page': 1},\n    {'text': 'Document 2', 'source': 'file2.pdf', 'page': 1}\n]\n\nfor doc in documents:\n    doc['embedding'] = get_embedding(doc['text'])\n    client.index(index=index_name, document=doc)\n\n# Vector search\nquery_vector = get_embedding('What is machine learning?')\n\nresponse = client.search(\n    index=index_name,\n    query={\n        'script_score': {\n            'query': {'match_all': {}},\n            'script': {\n                'source': \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n                'params': {'query_vector': query_vector}\n            }\n        }\n    },\n    size=5\n)\n\nfor hit in response['hits']['hits']:\n    print(f\"Score: {hit['_score']}, Text: {hit['_source']['text']}\")",
      "best_practices": [
        "Use dense_vector type for embeddings",
        "Set similarity metric (cosine, dot_product, l2_norm)",
        "Enable index: true for faster search",
        "Use script_score for vector similarity"
      ]
    },
    "hybrid_search": {
      "description": "Combine BM25 keyword search with vector search",
      "code_example": "from elasticsearch import Elasticsearch\n\nclient = Elasticsearch(['http://localhost:9200'])\n\nquery_text = 'machine learning'\nquery_vector = get_embedding(query_text)\n\n# Hybrid search: BM25 + vector\nresponse = client.search(\n    index='documents',\n    query={\n        'bool': {\n            'should': [\n                # BM25 keyword search\n                {\n                    'match': {\n                        'text': {\n                            'query': query_text,\n                            'boost': 0.3\n                        }\n                    }\n                },\n                # Vector search\n                {\n                    'script_score': {\n                        'query': {'match_all': {}},\n                        'script': {\n                            'source': \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n                            'params': {'query_vector': query_vector}\n                        },\n                        'boost': 0.7\n                    }\n                }\n            ]\n        }\n    },\n    size=10\n)\n\n# Alternative: RRF (Reciprocal Rank Fusion)\nresponse = client.search(\n    index='documents',\n    query={\n        'match': {'text': query_text}\n    },\n    knn={\n        'field': 'embedding',\n        'query_vector': query_vector,\n        'k': 10,\n        'num_candidates': 100\n    },\n    rank={'rrf': {}},\n    size=10\n)",
      "best_practices": [
        "Use RRF for better hybrid results",
        "Tune boost values for BM25 vs vector",
        "Use knn query for native vector search (Elasticsearch 8.0+)",
        "Combine with filters for metadata filtering"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    },
    "knn_search": {
      "description": "Native kNN search in Elasticsearch 8.0+",
      "code_example": "from elasticsearch import Elasticsearch\n\nclient = Elasticsearch(['http://localhost:9200'])\n\nquery_vector = get_embedding('machine learning')\n\n# Native kNN search\nresponse = client.search(\n    index='documents',\n    knn={\n        'field': 'embedding',\n        'query_vector': query_vector,\n        'k': 10,\n        'num_candidates': 100  # How many candidates to consider\n    },\n    size=10\n)\n\n# kNN with filters\nresponse = client.search(\n    index='documents',\n    knn={\n        'field': 'embedding',\n        'query_vector': query_vector,\n        'k': 10,\n        'num_candidates': 100,\n        'filter': {\n            'term': {'source': 'file1.pdf'}\n        }\n    },\n    size=10\n)",
      "best_practices": [
        "Use knn query for better performance",
        "Tune num_candidates (higher = more accurate)",
        "Combine with filters for metadata filtering",
        "Requires Elasticsearch 8.0+"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    },
    "index_mapping": {
      "description": "Proper index mapping for vector search",
      "code_example": "from elasticsearch import Elasticsearch\n\nclient = Elasticsearch(['http://localhost:9200'])\n\n# Complete mapping with vector and metadata fields\nmapping = {\n    'properties': {\n        'text': {\n            'type': 'text',\n            'analyzer': 'standard'\n        },\n        'embedding': {\n            'type': 'dense_vector',\n            'dims': 1536,\n            'index': True,\n            'similarity': 'cosine'\n        },\n        'source': {'type': 'keyword'},\n        'page': {'type': 'integer'},\n        'date': {'type': 'date'},\n        'category': {'type': 'keyword'}\n    }\n}\n\nclient.indices.create(\n    index='documents',\n    mappings=mapping,\n    settings={\n        'number_of_shards': 1,\n        'number_of_replicas': 0\n    }\n)",
      "best_practices": [
        "Set dims to match embedding dimension",
        "Choose similarity metric (cosine, dot_product, l2_norm)",
        "Use keyword type for metadata filtering",
        "Index embeddings for faster search"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    }
  },
  "redis_patterns": {
    "basic_usage": {
      "description": "Basic Redis vector search with RediSearch",
      "use_when": "Real-time applications, caching, need fast vector search",
      "code_example": "import redis\nfrom redis.commands.search.field import VectorField, TextField, TagField\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\nimport numpy as np\n\n# Connect to Redis\nr = redis.Redis(host='localhost', port=6379, db=0)\n\n# Create index schema\nschema = (\n    VectorField(\n        'embedding',\n        'HNSW',\n        {\n            'TYPE': 'FLOAT32',\n            'DIM': 1536,\n            'DISTANCE_METRIC': 'COSINE'\n        }\n    ),\n    TextField('text'),\n    TagField('source'),\n    TagField('category')\n)\n\n# Create index\nindex_name = 'documents'\nr.ft(index_name).create_index(\n    schema,\n    definition=IndexDefinition(prefix=['doc:'], index_type=IndexType.HASH)\n)\n\n# Add documents with embeddings\ndef get_embedding(text):\n    return np.random.random(1536).astype('float32').tobytes()\n\n# Store document\nr.hset(\n    'doc:1',\n    mapping={\n        'text': 'Document 1 content',\n        'embedding': get_embedding('Document 1 content'),\n        'source': 'file1.pdf',\n        'category': 'technical'\n    }\n)\n\n# Vector search\nquery_vector = get_embedding('What is machine learning?')\n\nresults = r.ft(index_name).search(\n    f\"*=>[KNN 5 @embedding $query_vector]\",\n    {'query_vector': query_vector}\n)\n\nfor doc in results.docs:\n    print(f\"ID: {doc.id}, Text: {doc.text}, Score: {doc.score}\")",
      "best_practices": [
        "Use HNSW index for fast approximate search",
        "Use FLAT index for exact search (slower)",
        "Store embeddings as bytes (float32)",
        "Use TagField for metadata filtering"
      ]
    },
    "hybrid_search": {
      "description": "Combine vector search with text search in Redis",
      "code_example": "import redis\nfrom redis.commands.search.field import VectorField, TextField\n\nr = redis.Redis(host='localhost', port=6379)\n\nquery_text = 'machine learning'\nquery_vector = get_embedding(query_text)\n\n# Hybrid search: vector + text\nresults = r.ft('documents').search(\n    f\"(@text:{query_text})=>[KNN 10 @embedding $query_vector]\",\n    {'query_vector': query_vector}\n)\n\n# With metadata filters\nresults = r.ft('documents').search(\n    f\"@source:{{file1.pdf}} =>[KNN 5 @embedding $query_vector]\",\n    {'query_vector': query_vector}\n)\n\n# Complex hybrid query\nresults = r.ft('documents').search(\n    f\"(@text:{query_text} | @category:{{technical}})=>[KNN 10 @embedding $query_vector]\",\n    {'query_vector': query_vector}\n)",
      "best_practices": [
        "Combine vector search with text filters",
        "Use TagField for exact match filtering",
        "Use TextField for full-text search",
        "Combine multiple conditions with AND/OR"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    },
    "json_documents": {
      "description": "Use Redis JSON with vector search",
      "code_example": "import redis\nimport json\n\nr = redis.Redis(host='localhost', port=6379)\n\n# Store JSON document\nr.json().set(\n    'doc:1',\n    '$',\n    {\n        'text': 'Document 1',\n        'embedding': get_embedding('Document 1').tolist(),\n        'source': 'file1.pdf',\n        'metadata': {\n            'page': 1,\n            'category': 'technical'\n        }\n    }\n)\n\n# Create index on JSON\nschema = (\n    VectorField(\n        '$.embedding',\n        'HNSW',\n        {'TYPE': 'FLOAT32', 'DIM': 1536, 'DISTANCE_METRIC': 'COSINE'}\n    ),\n    TextField('$.text'),\n    TagField('$.source')\n)\n\nr.ft('documents').create_index(schema)\n\n# Search JSON documents\nquery_vector = get_embedding('query')\nresults = r.ft('documents').search(\n    f\"*=>[KNN 5 @$.embedding $query_vector]\",\n    {'query_vector': query_vector}\n)",
      "best_practices": [
        "Use JSON for structured documents",
        "Access nested fields with JSONPath ($.field)",
        "Combine JSON with vector search",
        "Good for complex document structures"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    },
    "index_types": {
      "description": "Choose between HNSW and FLAT indexes",
      "code_example": "# HNSW index (fast approximate search)\nschema_hnsw = (\n    VectorField(\n        'embedding',\n        'HNSW',\n        {\n            'TYPE': 'FLOAT32',\n            'DIM': 1536,\n            'DISTANCE_METRIC': 'COSINE',\n            'M': 16,  # Number of connections\n            'EF_CONSTRUCTION': 200  # Construction parameter\n        }\n    )\n)\n\n# FLAT index (exact search, slower)\nschema_flat = (\n    VectorField(\n        'embedding',\n        'FLAT',\n        {\n            'TYPE': 'FLOAT32',\n            'DIM': 1536,\n            'DISTANCE_METRIC': 'COSINE'\n        }\n    )\n)",
      "best_practices": [
        "Use HNSW for large datasets (>100K vectors)",
        "Use FLAT for small datasets or exact search",
        "Tune HNSW parameters (M, EF_CONSTRUCTION)",
        "HNSW is faster but approximate"
      ],
      "use_when": "RAG applications, semantic search, or vector similarity retrieval"
    }
  },
  "embedding_patterns": {
    "embedding_selection": {
      "description": "Choose appropriate embedding model",
      "recommendations": {
        "openai_small": {
          "model": "text-embedding-3-small",
          "dimensions": 1536,
          "use_for": "General purpose, cost-effective",
          "cost": "$0.02 / 1M tokens"
        },
        "openai_large": {
          "model": "text-embedding-3-large",
          "dimensions": 3072,
          "use_for": "Highest quality, complex retrieval",
          "cost": "$0.13 / 1M tokens"
        },
        "cohere": {
          "model": "embed-english-v3.0",
          "dimensions": 1024,
          "use_for": "Multilingual, good quality",
          "cost": "$0.10 / 1M tokens"
        },
        "local": {
          "model": "sentence-transformers/all-MiniLM-L6-v2",
          "dimensions": 384,
          "use_for": "Free, local, lower quality",
          "cost": "Free (compute cost only)"
        }
      },
      "use_when": "RAG applications, semantic search, or vector similarity retrieval",
      "code_example": "from langchain_community.vectorstores import Chroma\nstore = Chroma.from_documents(docs, embeddings, persist_directory='./chroma')",
      "best_practices": [
        "Validate inputs and handle errors appropriately",
        "Document the pattern and when to use it"
      ]
    },
    "local_embeddings": {
      "description": "Generate embeddings locally",
      "use_when": "Cost-sensitive or privacy requirements",
      "code_example": "from sentence_transformers import SentenceTransformer\n\n# Load model (downloads on first use)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings\ntexts = ['Document 1', 'Document 2', 'Document 3']\nembeddings = model.encode(texts, show_progress_bar=True)\n\nprint(f'Embedding shape: {embeddings.shape}')  # (3, 384)\n\n# Batch processing for large datasets\nfrom tqdm import tqdm\n\ndef batch_embed(texts, batch_size=32):\n    all_embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch = texts[i:i+batch_size]\n        embeddings = model.encode(batch)\n        all_embeddings.extend(embeddings)\n    return all_embeddings",
      "best_practices": [
        "Use GPU for faster encoding",
        "Batch texts for efficiency",
        "Consider model size vs quality tradeoff",
        "Normalize embeddings for cosine similarity"
      ]
    }
  },
  "chunking_patterns": {
    "recursive_character": {
      "description": "Split by characters with hierarchy of separators",
      "use_when": "General text documents",
      "code_example": "from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len,\n    separators=['\\n\\n', '\\n', ' ', '']\n)\n\nchunks = splitter.split_text(long_document)\nprint(f'Created {len(chunks)} chunks')",
      "best_practices": [
        "chunk_size 500-1500 tokens typical",
        "chunk_overlap 10-20% of chunk_size",
        "Adjust based on embedding model context"
      ]
    },
    "semantic_chunking": {
      "description": "Split based on semantic similarity",
      "use_when": "Want semantically coherent chunks",
      "code_example": "from langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\n\nchunker = SemanticChunker(\n    embeddings,\n    breakpoint_threshold_type='percentile',\n    breakpoint_threshold_amount=95\n)\n\nchunks = chunker.split_text(document)\n\n# Each chunk should be semantically coherent\nfor i, chunk in enumerate(chunks[:3]):\n    print(f'Chunk {i}: {chunk[:100]}...')",
      "best_practices": [
        "More expensive (requires embeddings)",
        "Better semantic coherence",
        "Tune threshold for chunk size balance"
      ]
    }
  },
  "retrieval_patterns": {
    "similarity_search": {
      "description": "Basic vector similarity search",
      "code_example": "# Simple similarity search\ndocs = vectorstore.similarity_search(\n    query='What is machine learning?',\n    k=5\n)\n\n# With scores\ndocs_with_scores = vectorstore.similarity_search_with_score(\n    query='What is machine learning?',\n    k=5\n)\n\nfor doc, score in docs_with_scores:\n    print(f'Score: {score:.4f}')\n    print(f'Content: {doc.page_content[:100]}...')",
      "use_when": "RAG applications, semantic search, or vector similarity retrieval",
      "best_practices": [
        "Validate inputs and handle errors appropriately",
        "Document the pattern and when to use it"
      ]
    },
    "mmr_search": {
      "description": "Maximum Marginal Relevance for diversity",
      "use_when": "Want diverse results, not just most similar",
      "code_example": "# MMR balances relevance and diversity\ndocs = vectorstore.max_marginal_relevance_search(\n    query='What is machine learning?',\n    k=5,\n    fetch_k=20,  # Fetch more, then diversify\n    lambda_mult=0.5  # 0=max diversity, 1=max relevance\n)",
      "best_practices": [
        "fetch_k should be 3-4x of k",
        "lambda_mult=0.5 is good default",
        "Use when results are too similar"
      ]
    },
    "filtered_search": {
      "description": "Combine vector search with metadata filters",
      "use_when": "Need to constrain search by attributes",
      "code_example": "# LangChain with filter\nretriever = vectorstore.as_retriever(\n    search_type='similarity',\n    search_kwargs={\n        'k': 5,\n        'filter': {'source': 'annual_report.pdf'}\n    }\n)\n\n# Qdrant filter syntax\nfrom qdrant_client.models import Filter, FieldCondition, MatchValue\n\nfilter_condition = Filter(\n    must=[\n        FieldCondition(\n            key='source',\n            match=MatchValue(value='annual_report.pdf')\n        ),\n        FieldCondition(\n            key='year',\n            range=Range(gte=2023)\n        )\n    ]\n)\n\nresults = client.search(\n    collection_name='documents',\n    query_vector=query_vector,\n    query_filter=filter_condition,\n    limit=5\n)",
      "best_practices": [
        "Validate inputs and handle errors appropriately",
        "Document the pattern and when to use it"
      ]
    },
    "reranking": {
      "description": "Re-score results with cross-encoder",
      "use_when": "Need higher precision, can afford latency",
      "code_example": "from sentence_transformers import CrossEncoder\n\n# Load reranker\nreranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n# Initial retrieval\ninitial_results = vectorstore.similarity_search(query, k=20)\n\n# Rerank\npairs = [[query, doc.page_content] for doc in initial_results]\nscores = reranker.predict(pairs)\n\n# Sort by reranker scores\nreranked = sorted(\n    zip(initial_results, scores),\n    key=lambda x: x[1],\n    reverse=True\n)[:5]\n\nfor doc, score in reranked:\n    print(f'Rerank score: {score:.4f}')\n    print(f'Content: {doc.page_content[:100]}')",
      "best_practices": [
        "Retrieve more (20-50), then rerank to top k",
        "Cross-encoders are more accurate but slower",
        "Consider Cohere Rerank API for production"
      ]
    }
  },
  "anti_patterns": [
    {
      "name": "Wrong distance metric for embedding type",
      "problem": "Using L2/Euclidean distance with normalized embeddings or cosine with unnormalized embeddings results in poor search quality and incorrect similarity scores",
      "fix": "Use cosine similarity for normalized embeddings (OpenAI, Cohere) and L2/Euclidean distance for unnormalized embeddings"
    },
    {
      "name": "Chunks exceeding embedding context window",
      "problem": "Chunks larger than embedding model's max token limit (e.g., 8191 for OpenAI) cause truncation, lost information, or errors",
      "fix": "Keep chunks under embedding model's max token limit - typically 500-1500 tokens with 10-20% overlap"
    },
    {
      "name": "Storing vectors without metadata",
      "problem": "Cannot filter search results, trace sources, or provide citations - violates transparency and verifiability",
      "fix": "Always store rich metadata with vectors: source file, page number, date, category, and any domain-specific attributes"
    },
    {
      "name": "Mismatched embedding models",
      "problem": "Using different embedding models for indexing documents and querying causes semantic mismatch and garbage search results",
      "fix": "Use the same embedding model for both indexing and querying - never mix models even if they have same dimensions"
    },
    {
      "name": "No hybrid search for production",
      "problem": "Relying only on dense vector search misses documents with exact keyword matches, especially technical terms and proper nouns",
      "fix": "Implement hybrid search combining dense embeddings with sparse keyword vectors (BM25) using native support (Qdrant, Weaviate) or ensemble retrievers"
    }
  ],
  "best_practices_summary": [
    "Use same embedding model for indexing and querying",
    "Store rich metadata with vectors",
    "Chunk documents appropriately (500-1500 tokens)",
    "Use overlap between chunks",
    "Consider hybrid search for production",
    "Implement reranking for high-precision needs",
    "Monitor and evaluate retrieval quality",
    "Batch operations for efficiency"
  ],
  "patterns": {
    "retrieval_patterns_similarity_search": {
      "description": "Basic vector similarity search",
      "code_example": "# Simple similarity search\ndocs = vectorstore.similarity_search(\n    query='What is machine learning?',\n    k=5\n)\n\n# With scores\ndocs_with_scores = vectorstore.similarity_search_with_score(\n    query='What is machine learning?',\n    k=5\n)\n\nfor doc, score in docs_with_scores:\n    print(f'Score: {score:.4f}')\n    print(f'Content: {doc.page_content[:100]}...')",
      "use_when": "RAG applications, semantic search, or vector similarity retrieval",
      "best_practices": [
        "Validate inputs and handle errors appropriately",
        "Document the pattern and when to use it"
      ]
    },
    "retrieval_patterns_mmr_search": {
      "description": "Maximum Marginal Relevance for diversity",
      "use_when": "Want diverse results, not just most similar",
      "code_example": "# MMR balances relevance and diversity\ndocs = vectorstore.max_marginal_relevance_search(\n    query='What is machine learning?',\n    k=5,\n    fetch_k=20,  # Fetch more, then diversify\n    lambda_mult=0.5  # 0=max diversity, 1=max relevance\n)",
      "best_practices": [
        "fetch_k should be 3-4x of k",
        "lambda_mult=0.5 is good default",
        "Use when results are too similar"
      ]
    },
    "retrieval_patterns_filtered_search": {
      "description": "Combine vector search with metadata filters",
      "use_when": "Need to constrain search by attributes",
      "code_example": "# LangChain with filter\nretriever = vectorstore.as_retriever(\n    search_type='similarity',\n    search_kwargs={\n        'k': 5,\n        'filter': {'source': 'annual_report.pdf'}\n    }\n)\n\n# Qdrant filter syntax\nfrom qdrant_client.models import Filter, FieldCondition, MatchValue\n\nfilter_condition = Filter(\n    must=[\n        FieldCondition(\n            key='source',\n            match=MatchValue(value='annual_report.pdf')\n        ),\n        FieldCondition(\n            key='year',\n            range=Range(gte=2023)\n        )\n    ]\n)\n\nresults = client.search(\n    collection_name='documents',\n    query_vector=query_vector,\n    query_filter=filter_condition,\n    limit=5\n)",
      "best_practices": [
        "Validate inputs and handle errors appropriately",
        "Document the pattern and when to use it"
      ]
    },
    "retrieval_patterns_reranking": {
      "description": "Re-score results with cross-encoder",
      "use_when": "Need higher precision, can afford latency",
      "code_example": "from sentence_transformers import CrossEncoder\n\n# Load reranker\nreranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n# Initial retrieval\ninitial_results = vectorstore.similarity_search(query, k=20)\n\n# Rerank\npairs = [[query, doc.page_content] for doc in initial_results]\nscores = reranker.predict(pairs)\n\n# Sort by reranker scores\nreranked = sorted(\n    zip(initial_results, scores),\n    key=lambda x: x[1],\n    reverse=True\n)[:5]\n\nfor doc, score in reranked:\n    print(f'Rerank score: {score:.4f}')\n    print(f'Content: {doc.page_content[:100]}')",
      "best_practices": [
        "Retrieve more (20-50), then rerank to top k",
        "Cross-encoders are more accurate but slower",
        "Consider Cohere Rerank API for production"
      ]
    },
    "embedding_patterns_embedding_selection": {
      "description": "Choose appropriate embedding model",
      "recommendations": {
        "openai_small": {
          "model": "text-embedding-3-small",
          "dimensions": 1536,
          "use_for": "General purpose, cost-effective",
          "cost": "$0.02 / 1M tokens"
        },
        "openai_large": {
          "model": "text-embedding-3-large",
          "dimensions": 3072,
          "use_for": "Highest quality, complex retrieval",
          "cost": "$0.13 / 1M tokens"
        },
        "cohere": {
          "model": "embed-english-v3.0",
          "dimensions": 1024,
          "use_for": "Multilingual, good quality",
          "cost": "$0.10 / 1M tokens"
        },
        "local": {
          "model": "sentence-transformers/all-MiniLM-L6-v2",
          "dimensions": 384,
          "use_for": "Free, local, lower quality",
          "cost": "Free (compute cost only)"
        }
      },
      "use_when": "RAG applications, semantic search, or vector similarity retrieval",
      "code_example": "from langchain_community.vectorstores import Chroma\nstore = Chroma.from_documents(docs, embeddings, persist_directory='./chroma')",
      "best_practices": [
        "Validate inputs and handle errors appropriately",
        "Document the pattern and when to use it"
      ]
    },
    "embedding_patterns_local_embeddings": {
      "description": "Generate embeddings locally",
      "use_when": "Cost-sensitive or privacy requirements",
      "code_example": "from sentence_transformers import SentenceTransformer\n\n# Load model (downloads on first use)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings\ntexts = ['Document 1', 'Document 2', 'Document 3']\nembeddings = model.encode(texts, show_progress_bar=True)\n\nprint(f'Embedding shape: {embeddings.shape}')  # (3, 384)\n\n# Batch processing for large datasets\nfrom tqdm import tqdm\n\ndef batch_embed(texts, batch_size=32):\n    all_embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch = texts[i:i+batch_size]\n        embeddings = model.encode(batch)\n        all_embeddings.extend(embeddings)\n    return all_embeddings",
      "best_practices": [
        "Use GPU for faster encoding",
        "Batch texts for efficiency",
        "Consider model size vs quality tradeoff",
        "Normalize embeddings for cosine similarity"
      ]
    }
  },
  "best_practices": [
    "Use the same embedding model for indexing and querying - never mix different models as it causes semantic mismatch",
    "Store rich metadata with vectors (source, page, date, category) to enable filtering and source attribution",
    "Choose appropriate distance metric: cosine for normalized embeddings, L2/Euclidean for unnormalized embeddings",
    "Implement hybrid search combining dense vectors with sparse keyword vectors for production RAG systems",
    "Use collection aliases for zero-downtime updates when rebuilding indices or migrating collections",
    "Batch vector operations (upserts, queries) for better performance - typically 50-100 vectors per batch",
    "Monitor collection statistics and indexing status regularly to ensure optimal performance",
    "Tune index parameters (HNSW m/ef_construct, IVF nlist/nprobe) based on collection size and query patterns"
  ],
  "use_when": "RAG applications, semantic search, or vector similarity retrieval",
  "code_example": "from langchain_community.vectorstores import Chroma\nstore = Chroma.from_documents(docs, embeddings, persist_directory='./chroma')"
}
