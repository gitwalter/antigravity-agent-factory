{
  "metadata": {
    "blueprintId": "multimodal-ai",
    "blueprintName": "Multimodal AI Blueprint",
    "description": "Multimodal AI development with vision, audio, and text processing using transformers, CLIP, and vision-capable LLMs",
    "version": "1.0.0",
    "author": "Cursor Agent Factory",
    "tags": [
      "python",
      "multimodal",
      "vision",
      "audio",
      "text",
      "transformers",
      "clip",
      "torchvision",
      "pillow",
      "llm"
    ],
    "purpose": "Enable development of multimodal AI applications combining vision, audio, and text processing with modern frameworks and vision-capable LLMs"
  },
  "stack": {
    "primaryLanguage": "python",
    "frameworks": [
      {
        "name": "Transformers",
        "version": "5.1.0+",
        "purpose": "Multimodal transformers and vision models"
      },
      {
        "name": "Pillow",
        "version": "10.0+",
        "purpose": "Image processing and manipulation"
      },
      {
        "name": "torchvision",
        "version": "0.15+",
        "purpose": "Computer vision utilities and pre-trained models"
      },
      {
        "name": "CLIP",
        "version": "1.0+",
        "purpose": "Contrastive Language-Image Pre-training models",
        "optional": true
      },
      {
        "name": "PyTorch",
        "version": "2.10+",
        "purpose": "Deep learning framework"
      },
      {
        "name": "FastAPI",
        "version": "0.133+",
        "purpose": "API serving",
        "optional": true
      },
      {
        "name": "Streamlit",
        "version": "1.54+",
        "purpose": "Demo UI",
        "optional": true
      }
    ],
    "llmProviders": [
      {
        "name": "OpenAI",
        "models": [
          "gpt-5.2",
          "gpt-5.3-codex",
          "gpt-4o",
          "gpt-4o-mini",
          "o1",
          "o1-mini"
        ],
        "purpose": "Vision-capable LLM and text generation"
      },
      {
        "name": "Anthropic",
        "models": [
          "claude-opus-4.6",
          "claude-3-5-sonnet-20241022",
          "claude-3-5-haiku-20241022"
        ],
        "purpose": "Vision-capable LLM alternative",
        "optional": true
      },
      {
        "name": "Google",
        "models": [
          "gemini-3-pro",
          "gemini-3-flash",
          "gemini-2.5-flash",
          "gemini-2.0-flash-exp"
        ],
        "purpose": "Vision-capable LLM alternative",
        "optional": true
      }
    ],
    "tools": [
      {
        "name": "pytest",
        "purpose": "Testing"
      },
      {
        "name": "ruff",
        "purpose": "Linting"
      },
      {
        "name": "black",
        "purpose": "Formatting"
      },
      {
        "name": "mypy",
        "purpose": "Type checking"
      },
      {
        "name": "opencv-python",
        "purpose": "Computer vision operations",
        "optional": true
      },
      {
        "name": "librosa",
        "purpose": "Audio processing",
        "optional": true
      }
    ],
    "styleGuides": [
      {
        "name": "pep8",
        "description": "PEP 8 Style Guide"
      },
      {
        "name": "google",
        "description": "Google Python Style"
      }
    ]
  },
  "agents": [
    {
      "patternId": "code-reviewer",
      "required": true,
      "customizations": {
        "skills": [
          "clean-code-review",
          "grounding"
        ]
      }
    },
    {
      "patternId": "test-generator",
      "required": true,
      "customizations": {
        "skills": [
          "tdd"
        ]
      }
    },
    {
      "patternId": "documentation-agent",
      "required": true
    },
    {
      "patternId": "knowledge-extender",
      "required": true,
      "description": "Extend knowledge base during development"
    },
    {
      "patternId": "knowledge-evolution",
      "required": true,
      "description": "Manage and evolve project knowledge independently"
    },
    {
      "patternId": "factory-updates",
      "required": true,
      "description": "Receive updates from the Cursor Agent Factory"
    },
    {
      "patternId": "debug-conductor-project",
      "required": false,
      "description": "Autonomous debugging agent for CI/CD and test failures"
    }
  ],
  "skills": [
    {
      "patternId": "bugfix-workflow",
      "required": true
    },
    {
      "patternId": "feature-workflow",
      "required": true
    },
    {
      "patternId": "grounding",
      "required": true
    },
    {
      "patternId": "grounding-verification",
      "required": true,
      "default": true,
      "description": "Two-pass verification for LLM grounding and factual claims"
    },
    {
      "patternId": "alignment-check",
      "required": true,
      "default": true,
      "description": "Verify understanding before major implementations"
    },
    {
      "patternId": "research-first-project",
      "required": true,
      "default": true,
      "description": "Research existing solutions before building"
    },
    {
      "patternId": "tdd",
      "required": true
    },
    {
      "patternId": "extend-knowledge",
      "required": true,
      "description": "Extend knowledge base with new topics"
    },
    {
      "patternId": "receive-updates",
      "required": true,
      "description": "Receive updates from Factory"
    },
    {
      "patternId": "ci-monitor-project",
      "required": false,
      "default": true,
      "description": "Monitor CI/CD pipelines with automatic error detection"
    },
    {
      "patternId": "pipeline-error-fix-project",
      "required": false,
      "default": true,
      "description": "Systematic pipeline error detection and fixing"
    }
  ],
  "knowledge": [
    {
      "filename": "huggingface-patterns.json",
      "description": "Transformers patterns for multimodal models, vision transformers, and model loading"
    },
    {
      "filename": "pytorch-patterns.json",
      "description": "PyTorch patterns for computer vision, model training, and optimization"
    },
    {
      "filename": "deep-learning-patterns.json",
      "description": "Deep learning training patterns, optimization, and checkpointing"
    },
    {
      "filename": "llm-provider-comparison.json",
      "description": "LLM provider comparison including vision-capable models"
    }
  ],
  "templates": {
    "codeTemplates": [
      {
        "category": "vision",
        "directory": "templates/python/vision/"
      },
      {
        "category": "multimodal",
        "directory": "templates/python/multimodal/"
      },
      {
        "category": "inference",
        "directory": "templates/python/inference/"
      }
    ],
    "documentTemplates": [
      {
        "name": "model_card.md",
        "directory": "templates/docs/"
      },
      {
        "name": "multimodal_architecture.md",
        "directory": "templates/docs/"
      }
    ]
  },
  "projectStructure": {
    "directories": [
      {
        "path": ".cursor/agents/",
        "purpose": "AI agent definitions"
      },
      {
        "path": ".cursor/skills/",
        "purpose": "Reusable skills"
      },
      {
        "path": "src/",
        "purpose": "Source code"
      },
      {
        "path": "src/vision/",
        "purpose": "Vision processing modules"
      },
      {
        "path": "src/audio/",
        "purpose": "Audio processing modules",
        "optional": true
      },
      {
        "path": "src/multimodal/",
        "purpose": "Multimodal fusion and processing"
      },
      {
        "path": "src/models/",
        "purpose": "Model architectures and loading"
      },
      {
        "path": "src/inference/",
        "purpose": "Inference pipelines"
      },
      {
        "path": "src/utils/",
        "purpose": "Utilities and helpers"
      },
      {
        "path": "data/",
        "purpose": "Data files and datasets"
      },
      {
        "path": "data/images/",
        "purpose": "Image datasets"
      },
      {
        "path": "data/audio/",
        "purpose": "Audio datasets",
        "optional": true
      },
      {
        "path": "checkpoints/",
        "purpose": "Model checkpoints"
      },
      {
        "path": "outputs/",
        "purpose": "Processing outputs"
      },
      {
        "path": "tests/",
        "purpose": "Test files"
      },
      {
        "path": "notebooks/",
        "purpose": "Exploration notebooks"
      }
    ],
    "files": [
      {
        "path": ".cursorrules",
        "purpose": "Agent behavior rules"
      },
      {
        "path": "README.md",
        "purpose": "Project documentation"
      },
      {
        "path": "pyproject.toml",
        "purpose": "Python project config"
      },
      {
        "path": "requirements.txt",
        "purpose": "Dependencies"
      },
      {
        "path": "inference.py",
        "purpose": "Inference script"
      },
      {
        "path": "Dockerfile",
        "purpose": "Container definition"
      }
    ]
  },
  "cursorrules": {
    "variables": [
      {
        "name": "PYTHON_PATH",
        "description": "Python executable",
        "default": "python"
      },
      {
        "name": "OPENAI_API_KEY",
        "description": "OpenAI API key",
        "default": ""
      },
      {
        "name": "ANTHROPIC_API_KEY",
        "description": "Anthropic API key",
        "default": ""
      },
      {
        "name": "GOOGLE_API_KEY",
        "description": "Google API key",
        "default": ""
      },
      {
        "name": "CUDA_VISIBLE_DEVICES",
        "description": "GPU devices",
        "default": "0"
      },
      {
        "name": "IMAGE_SIZE",
        "description": "Default image size for processing",
        "default": "224"
      }
    ],
    "rules": [
      {
        "name": "Image Preprocessing",
        "description": "Always normalize images using appropriate mean/std for the model"
      },
      {
        "name": "Model Loading",
        "description": "Use transformers Auto classes for consistent model loading"
      },
      {
        "name": "Memory Efficiency",
        "description": "Use gradient checkpointing and mixed precision for large models"
      },
      {
        "name": "Vision LLM Integration",
        "description": "Properly encode images as base64 or use vision model APIs correctly"
      },
      {
        "name": "Multimodal Fusion",
        "description": "Ensure proper alignment between vision and text embeddings"
      },
      {
        "name": "Error Handling",
        "description": "Handle image format errors and API rate limits gracefully"
      },
      {
        "name": "Reproducibility",
        "description": "Set random seeds for deterministic results"
      },
      {
        "name": "Logging",
        "description": "Log model predictions, confidence scores, and processing times"
      }
    ]
  },
  "pmIntegration": {
    "enabled": false,
    "description": "Optional project management integration with agile workflows",
    "agents": [
      {
        "patternId": "product-owner",
        "description": "Creates and refines stories, prioritizes backlog"
      },
      {
        "patternId": "sprint-master",
        "description": "Facilitates sprint ceremonies and removes blockers"
      },
      {
        "patternId": "task-manager",
        "description": "Breaks stories into tasks, manages issues"
      },
      {
        "patternId": "reporting-agent",
        "description": "Generates metrics, burndowns, dashboards"
      }
    ],
    "skills": [
      {
        "patternId": "create-story",
        "description": "Create user stories with acceptance criteria"
      },
      {
        "patternId": "create-epic",
        "description": "Create epics with child story structure"
      },
      {
        "patternId": "create-task",
        "description": "Create implementation tasks"
      },
      {
        "patternId": "plan-sprint",
        "description": "Sprint planning workflow"
      },
      {
        "patternId": "run-standup",
        "description": "Daily standup facilitation"
      },
      {
        "patternId": "close-sprint",
        "description": "Sprint closure and velocity calculation"
      }
    ],
    "backends": [
      "github",
      "jira",
      "linear",
      "azure-devops"
    ]
  },
  "workflows": [
    "factory-standard-workflow"
  ]
}
