{
  "metadata": {
    "blueprintId": "llm-inference-optimization",
    "blueprintName": "LLM Inference Optimization Blueprint",
    "description": "Optimize LLM inference performance with vLLM, text-generation-inference, TensorRT-LLM, and llama.cpp",
    "version": "1.0.0",
    "author": "Cursor Agent Factory",
    "tags": [
      "python",
      "llm",
      "inference",
      "optimization",
      "vllm",
      "text-generation-inference",
      "tensorrt-llm",
      "llama.cpp",
      "performance"
    ],
    "purpose": "Enable high-performance LLM inference with optimized serving engines, quantization, and GPU acceleration"
  },
  "stack": {
    "primaryLanguage": "python",
    "frameworks": [
      {
        "name": "vLLM",
        "version": "0.6+",
        "purpose": "High-throughput LLM serving with PagedAttention"
      },
      {
        "name": "text-generation-inference",
        "version": "1.4+",
        "purpose": "HuggingFace production inference server"
      },
      {
        "name": "TensorRT-LLM",
        "version": "0.10+",
        "purpose": "NVIDIA optimized inference engine",
        "optional": true
      },
      {
        "name": "llama.cpp",
        "version": "latest",
        "purpose": "CPU-optimized inference with quantization",
        "optional": true
      }
    ],
    "tools": [
      {
        "name": "Triton",
        "version": "2.40+",
        "purpose": "NVIDIA inference server framework"
      },
      {
        "name": "PyTorch",
        "version": "2.10+",
        "purpose": "Deep learning framework"
      },
      {
        "name": "pytest",
        "purpose": "Testing"
      },
      {
        "name": "ruff",
        "purpose": "Linting"
      },
      {
        "name": "black",
        "purpose": "Formatting"
      },
      {
        "name": "mypy",
        "purpose": "Type checking"
      },
      {
        "name": "nvidia-smi",
        "purpose": "GPU monitoring"
      }
    ],
    "styleGuides": [
      {
        "name": "pep8",
        "description": "PEP 8 Style Guide"
      },
      {
        "name": "google",
        "description": "Google Python Style"
      }
    ]
  },
  "agents": [
    {
      "patternId": "code-reviewer",
      "required": true,
      "customizations": {
        "skills": [
          "clean-code-review",
          "grounding"
        ]
      }
    },
    {
      "patternId": "test-generator",
      "required": true,
      "customizations": {
        "skills": [
          "tdd"
        ]
      }
    },
    {
      "patternId": "documentation-agent",
      "required": true
    },
    {
      "patternId": "knowledge-extender",
      "required": true,
      "description": "Extend knowledge base during development"
    },
    {
      "patternId": "knowledge-evolution",
      "required": true,
      "description": "Manage and evolve project knowledge independently"
    },
    {
      "patternId": "factory-updates",
      "required": true,
      "description": "Receive updates from the Cursor Agent Factory"
    },
    {
      "patternId": "debug-conductor-project",
      "required": false,
      "description": "Autonomous debugging agent for CI/CD and test failures"
    }
  ],
  "skills": [
    {
      "patternId": "bugfix-workflow",
      "required": true
    },
    {
      "patternId": "feature-workflow",
      "required": true
    },
    {
      "patternId": "grounding",
      "required": true
    },
    {
      "patternId": "grounding-verification",
      "required": true,
      "default": true,
      "description": "Two-pass verification for LLM grounding and factual claims"
    },
    {
      "patternId": "alignment-check",
      "required": true,
      "default": true,
      "description": "Verify understanding before major implementations"
    },
    {
      "patternId": "research-first-project",
      "required": true,
      "default": true,
      "description": "Research existing solutions before building"
    },
    {
      "patternId": "tdd",
      "required": true
    },
    {
      "patternId": "extend-knowledge",
      "required": true,
      "description": "Extend knowledge base with new topics"
    },
    {
      "patternId": "receive-updates",
      "required": true,
      "description": "Receive updates from Factory"
    },
    {
      "patternId": "ci-monitor-project",
      "required": false,
      "default": true,
      "description": "Monitor CI/CD pipelines with automatic error detection"
    },
    {
      "patternId": "pipeline-error-fix-project",
      "required": false,
      "default": true,
      "description": "Systematic pipeline error detection and fixing"
    }
  ],
  "testingApproach": {
    "modes": [
      "tdd-only"
    ],
    "default": "tdd-only",
    "frameworks": {
      "tdd": "pytest"
    },
    "modeDescriptions": {
      "tdd-only": "Unit tests for inference optimization code"
    }
  },
  "knowledge": [
    {
      "filename": "model-serving-patterns.json",
      "description": "LLM serving patterns including vLLM, TGI, and Triton"
    },
    {
      "filename": "pytorch-patterns.json",
      "description": "PyTorch optimization patterns for inference"
    },
    {
      "filename": "huggingface-patterns.json",
      "description": "HuggingFace inference patterns and text-generation-inference"
    },
    {
      "filename": "deep-learning-patterns.json",
      "description": "Deep learning inference optimization techniques"
    },
    {
      "filename": "mlops-patterns.json",
      "description": "Model serving and deployment patterns"
    }
  ],
  "templates": {
    "codeTemplates": [
      {
        "category": "inference-server",
        "directory": "templates/python/inference/"
      },
      {
        "category": "optimization",
        "directory": "templates/python/inference/"
      },
      {
        "category": "benchmark",
        "directory": "templates/python/inference/"
      },
      {
        "category": "quantization",
        "directory": "templates/python/inference/"
      }
    ],
    "documentTemplates": [
      {
        "name": "benchmark_report.md",
        "directory": "templates/docs/"
      },
      {
        "name": "optimization_guide.md",
        "directory": "templates/docs/"
      }
    ]
  },
  "projectStructure": {
    "directories": [
      {
        "path": ".cursor/agents/",
        "purpose": "AI agent definitions"
      },
      {
        "path": ".cursor/skills/",
        "purpose": "Reusable skills"
      },
      {
        "path": "src/",
        "purpose": "Source code"
      },
      {
        "path": "src/inference/",
        "purpose": "Inference server implementations"
      },
      {
        "path": "src/optimization/",
        "purpose": "Optimization utilities"
      },
      {
        "path": "src/benchmarking/",
        "purpose": "Benchmarking scripts"
      },
      {
        "path": "src/quantization/",
        "purpose": "Quantization tools"
      },
      {
        "path": "configs/",
        "purpose": "Inference configurations"
      },
      {
        "path": "scripts/",
        "purpose": "Utility scripts"
      },
      {
        "path": "models/",
        "purpose": "Model checkpoints and weights"
      },
      {
        "path": "benchmarks/",
        "purpose": "Benchmark results"
      },
      {
        "path": "tests/",
        "purpose": "Test files"
      },
      {
        "path": "docs/",
        "purpose": "Documentation"
      }
    ],
    "files": [
      {
        "path": ".cursorrules",
        "purpose": "Agent behavior rules"
      },
      {
        "path": "README.md",
        "purpose": "Project documentation"
      },
      {
        "path": "pyproject.toml",
        "purpose": "Python project config"
      },
      {
        "path": "requirements.txt",
        "purpose": "Dependencies"
      },
      {
        "path": "benchmark.py",
        "purpose": "Benchmarking script"
      },
      {
        "path": "serve.py",
        "purpose": "Inference server entry point"
      },
      {
        "path": "Dockerfile",
        "purpose": "Container definition"
      },
      {
        "path": "docker-compose.yml",
        "purpose": "Multi-container orchestration"
      }
    ]
  },
  "cursorrules": {
    "variables": [
      {
        "name": "PYTHON_PATH",
        "description": "Python executable",
        "default": "python"
      },
      {
        "name": "CUDA_VISIBLE_DEVICES",
        "description": "GPU devices",
        "default": "0"
      },
      {
        "name": "GPU_MEMORY_UTILIZATION",
        "description": "GPU memory utilization (0.0-1.0)",
        "default": "0.9"
      },
      {
        "name": "TENSOR_PARALLEL_SIZE",
        "description": "Tensor parallelism size",
        "default": "1"
      }
    ],
    "rules": [
      {
        "name": "Performance First",
        "description": "Always profile before and after optimizations. Use nvidia-smi, PyTorch profiler, or vLLM metrics"
      },
      {
        "name": "Memory Efficiency",
        "description": "Use quantization (AWQ, GPTQ, INT8) for memory-constrained deployments. Monitor GPU memory usage"
      },
      {
        "name": "Batch Processing",
        "description": "Leverage continuous batching in vLLM and TGI for high throughput"
      },
      {
        "name": "KVCache Optimization",
        "description": "Use PagedAttention in vLLM for efficient KV cache management"
      },
      {
        "name": "Type Hints",
        "description": "Always use type hints for function parameters and returns"
      },
      {
        "name": "Docstrings",
        "description": "Use Google-style docstrings with performance notes"
      },
      {
        "name": "Testing",
        "description": "Write pytest tests for inference correctness and performance regression"
      },
      {
        "name": "Benchmarking",
        "description": "Include latency (p50, p95, p99), throughput (tokens/sec), and memory metrics in benchmarks"
      },
      {
        "name": "GPU Utilization",
        "description": "Monitor GPU utilization and ensure efficient use. Use tensor parallelism for large models"
      },
      {
        "name": "Quantization",
        "description": "Test quantization impact on quality before deployment. Use AWQ/GPTQ for 4-bit, INT8 for 8-bit"
      }
    ]
  },
  "pmIntegration": {
    "enabled": false,
    "description": "Optional project management integration with agile workflows",
    "agents": [
      {
        "patternId": "product-owner",
        "description": "Creates and refines stories, prioritizes backlog"
      },
      {
        "patternId": "sprint-master",
        "description": "Facilitates sprint ceremonies and removes blockers"
      },
      {
        "patternId": "task-manager",
        "description": "Breaks stories into tasks, manages issues"
      },
      {
        "patternId": "reporting-agent",
        "description": "Generates metrics, burndowns, dashboards"
      }
    ],
    "skills": [
      {
        "patternId": "create-story",
        "description": "Create user stories with acceptance criteria"
      },
      {
        "patternId": "create-epic",
        "description": "Create epics with child story structure"
      },
      {
        "patternId": "create-task",
        "description": "Create implementation tasks"
      },
      {
        "patternId": "plan-sprint",
        "description": "Sprint planning workflow"
      },
      {
        "patternId": "run-standup",
        "description": "Daily standup facilitation"
      },
      {
        "patternId": "close-sprint",
        "description": "Sprint closure and velocity calculation"
      }
    ],
    "backends": [
      "github",
      "jira",
      "linear",
      "azure-devops"
    ]
  },
  "workflows": [
    "factory-standard-workflow"
  ]
}
