"""
{{ retriever_name | default('RAG Retriever') }} - RAG Retriever Template

Purpose: {{ retriever_purpose | default('Vector store integration with hybrid retrieval (semantic + keyword)') }}
Author: {{ author | default('Cursor Agent Factory') }}
Date: {{ date | default('2026-02-08') }}

Axiom Alignment:
- A1 (Verifiability): Retrieval sources are tracked and citable
- A3 (Transparency): Retrieval process is explicit and explainable
- A2 (Correctness): Hybrid retrieval improves accuracy

This retriever provides:
- Vector store integration (Chroma, FAISS, etc.)
- Hybrid retrieval combining semantic and keyword search
- Configurable retrieval strategies
- Result ranking and filtering
"""

from typing import List, Dict, Any, Optional, Union
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma, FAISS, VectorStore
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from pydantic import BaseModel, Field
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RetrievalConfig(BaseModel):
    """Configuration for retrieval."""
    k: int = Field(default={{ k | default(4) }}, ge=1, description="Number of documents to retrieve")
    dense_weight: float = Field(default={{ dense_weight | default(0.7) }}, ge=0, le=1, description="Weight for dense vector search")
    sparse_weight: float = Field(default={{ sparse_weight | default(0.3) }}, ge=0, le=1, description="Weight for sparse keyword search")
    search_type: str = Field(default="{{ search_type | default('hybrid') }}", description="Search type: hybrid, dense, sparse, mmr")
    score_threshold: Optional[float] = Field(default=None, description="Minimum relevance score")


class RetrievalResult(BaseModel):
    """Result from retrieval operation."""
    documents: List[Document] = Field(description="Retrieved documents")
    scores: Optional[List[float]] = Field(description="Relevance scores")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Retrieval metadata")


class {{ retriever_class_name | default('RAGRetriever') }}(BaseRetriever):
    """
    {{ retriever_name | default('RAG Retriever') }} - Hybrid Search Retriever

    Combines dense vector search (semantic) with sparse keyword search (BM25)
    for improved retrieval quality. Supports multiple retrieval strategies.

    Example:
        >>> retriever = {{ retriever_class_name | default('RAGRetriever') }}(
        ...     vector_store=vectorstore,
        ...     documents=documents,
        ...     dense_weight=0.7,
        ...     sparse_weight=0.3
        ... )
        >>> results = retriever.invoke("machine learning algorithms")
        >>> print(f"Retrieved {len(results)} documents")
    """

    def __init__(
        self,
        vector_store: Optional[VectorStore] = None,
        documents: Optional[List[Document]] = None,
        embedding_model: str = "{{ embedding_model | default('text-embedding-ada-002') }}",
        config: Optional[RetrievalConfig] = None,
        **kwargs
    ):
        """
        Initialize hybrid retriever.

        Args:
            vector_store: Vector store for dense search
            documents: Documents for BM25 sparse search
            embedding_model: Embedding model name
            config: Retrieval configuration
            **kwargs: Additional arguments
        """
        super().__init__()

        self.config = config or RetrievalConfig()
        self.embedding_model = embedding_model

        # Normalize weights
        total_weight = self.config.dense_weight + self.config.sparse_weight
        if total_weight > 0:
            self.dense_weight = self.config.dense_weight / total_weight
            self.sparse_weight = self.config.sparse_weight / total_weight
        else:
            self.dense_weight = 0.5
            self.sparse_weight = 0.5

        # Initialize embeddings
        self.embeddings = OpenAIEmbeddings(model=embedding_model)

        # Setup vector store
        if vector_store is None and documents:
            logger.info("Creating vector store from documents...")
            self.vector_store = Chroma.from_documents(documents, self.embeddings)
        else:
            self.vector_store = vector_store

        # Setup BM25 retriever for sparse search
        self.bm25_retriever: Optional[BM25Retriever] = None
        if self.config.search_type in ["hybrid", "sparse"] and documents:
            logger.info("Initializing BM25 retriever...")
            self.bm25_retriever = BM25Retriever.from_documents(documents)
            self.bm25_retriever.k = self.config.k

        # Setup dense retriever
        self.dense_retriever: Optional[Any] = None
        if self.config.search_type in ["hybrid", "dense", "mmr"] and self.vector_store:
            search_kwargs = {"k": self.config.k}
            if self.config.search_type == "mmr":
                search_kwargs["fetch_k"] = self.config.k * 2
                search_kwargs["lambda_mult"] = 0.5

            self.dense_retriever = self.vector_store.as_retriever(
                search_type="similarity" if self.config.search_type != "mmr" else "mmr",
                search_kwargs=search_kwargs
            )

        # Setup ensemble retriever for hybrid search
        self.ensemble_retriever: Optional[EnsembleRetriever] = None
        if self.config.search_type == "hybrid" and self.dense_retriever and self.bm25_retriever:
            self.ensemble_retriever = EnsembleRetriever(
                retrievers=[self.bm25_retriever, self.dense_retriever],
                weights=[self.sparse_weight, self.dense_weight]
            )

        logger.info(
            f"Initialized {{ retriever_class_name | default('RAGRetriever') }} "
            f"with {self.config.search_type} search, k={self.config.k}"
        )

    @property
    def _get_lc_namespace(self) -> List[str]:
        """Return namespace for LangChain."""
        return ["langchain", "retrievers", "{{ retriever_class_name | default('RAGRetriever') }}"]

    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
    ) -> List[Document]:
        """
        Retrieve relevant documents for a query.

        Args:
            query: Query string
            run_manager: Callback manager for tracing

        Returns:
            List of relevant documents
        """
        logger.info(f"Retrieving documents for query: {query[:100]}...")

        try:
            if self.config.search_type == "hybrid":
                if not self.ensemble_retriever:
                    raise ValueError("Ensemble retriever not initialized")
                documents = self.ensemble_retriever.invoke(query)
            elif self.config.search_type == "dense":
                if not self.dense_retriever:
                    raise ValueError("Dense retriever not initialized")
                documents = self.dense_retriever.invoke(query)
            elif self.config.search_type == "sparse":
                if not self.bm25_retriever:
                    raise ValueError("BM25 retriever not initialized")
                documents = self.bm25_retriever.invoke(query)
            elif self.config.search_type == "mmr":
                if not self.dense_retriever:
                    raise ValueError("Dense retriever not initialized")
                documents = self.dense_retriever.invoke(query)
            else:
                raise ValueError(f"Unknown search type: {self.config.search_type}")

            # Apply score threshold if specified
            if self.config.score_threshold:
                filtered = []
                for doc in documents:
                    score = doc.metadata.get("score", 1.0)
                    if score >= self.config.score_threshold:
                        filtered.append(doc)
                documents = filtered

            # Limit to k documents
            documents = documents[:self.config.k]

            logger.info(f"Retrieved {len(documents)} documents")
            return documents

        except Exception as e:
            logger.error(f"Error retrieving documents: {e}")
            return []

    async def _aget_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
    ) -> List[Document]:
        """Async version of retrieval."""
        logger.info(f"Async retrieving documents for query: {query[:100]}...")

        try:
            if self.config.search_type == "hybrid":
                if not self.ensemble_retriever:
                    raise ValueError("Ensemble retriever not initialized")
                documents = await self.ensemble_retriever.ainvoke(query)
            elif self.config.search_type == "dense":
                if not self.dense_retriever:
                    raise ValueError("Dense retriever not initialized")
                documents = await self.dense_retriever.ainvoke(query)
            elif self.config.search_type == "sparse":
                if not self.bm25_retriever:
                    raise ValueError("BM25 retriever not initialized")
                documents = await self.bm25_retriever.ainvoke(query)
            elif self.config.search_type == "mmr":
                if not self.dense_retriever:
                    raise ValueError("Dense retriever not initialized")
                documents = await self.dense_retriever.ainvoke(query)
            else:
                raise ValueError(f"Unknown search type: {self.config.search_type}")

            # Apply score threshold
            if self.config.score_threshold:
                filtered = []
                for doc in documents:
                    score = doc.metadata.get("score", 1.0)
                    if score >= self.config.score_threshold:
                        filtered.append(doc)
                documents = filtered

            documents = documents[:self.config.k]

            logger.info(f"Retrieved {len(documents)} documents (async)")
            return documents

        except Exception as e:
            logger.error(f"Error in async retrieval: {e}")
            return []

    def retrieve_with_scores(self, query: str) -> RetrievalResult:
        """
        Retrieve documents with relevance scores.

        Args:
            query: Query string

        Returns:
            RetrievalResult with documents and scores
        """
        logger.info(f"Retrieving with scores for query: {query[:100]}...")

        try:
            documents = self.invoke(query)

            # Extract scores from metadata or compute similarity
            scores = []
            if self.vector_store and self.config.search_type in ["dense", "hybrid", "mmr"]:
                if hasattr(self.vector_store, "similarity_search_with_score"):
                    scored_docs = self.vector_store.similarity_search_with_score(
                        query, k=self.config.k
                    )
                    scores = [score for _, score in scored_docs]
                else:
                    scores = [1.0 - (i * 0.1) for i in range(len(documents))]
            else:
                scores = [1.0 - (i * 0.1) for i in range(len(documents))]

            return RetrievalResult(
                documents=documents,
                scores=scores[:len(documents)],
                metadata={
                    "search_type": self.config.search_type,
                    "k": len(documents),
                    "query": query
                }
            )

        except Exception as e:
            logger.error(f"Error retrieving with scores: {e}")
            return RetrievalResult(
                documents=[],
                scores=[],
                metadata={"error": str(e)}
            )

    def update_documents(self, documents: List[Document]) -> None:
        """
        Update retriever with new documents.

        Args:
            documents: New documents to add
        """
        logger.info(f"Updating retriever with {len(documents)} documents...")

        try:
            # Update vector store
            if self.vector_store:
                self.vector_store.add_documents(documents)

            # Update BM25 retriever
            if self.bm25_retriever:
                self.bm25_retriever.add_documents(documents)

            # Recreate ensemble retriever if needed
            if self.config.search_type == "hybrid" and self.dense_retriever and self.bm25_retriever:
                self.ensemble_retriever = EnsembleRetriever(
                    retrievers=[self.bm25_retriever, self.dense_retriever],
                    weights=[self.sparse_weight, self.dense_weight]
                )

            logger.info("Retriever updated successfully")

        except Exception as e:
            logger.error(f"Error updating retriever: {e}")
            raise


# Example usage
if __name__ == "__main__":
    from langchain_core.documents import Document

    # Create sample documents
    documents = [
        Document(
            page_content="Machine learning is a subset of artificial intelligence.",
            metadata={"source": "doc1"}
        ),
        Document(
            page_content="Deep learning uses neural networks with multiple layers.",
            metadata={"source": "doc2"}
        ),
        Document(
            page_content="Natural language processing enables computers to understand text.",
            metadata={"source": "doc3"}
        )
    ]

    # Create hybrid retriever
    retriever = {{ retriever_class_name | default('RAGRetriever') }}(
        documents=documents,
        dense_weight={{ dense_weight | default(0.7) }},
        sparse_weight={{ sparse_weight | default(0.3) }},
        k={{ k | default(4) }},
        search_type="{{ search_type | default('hybrid') }}"
    )

    # Retrieve documents
    query = "{{ example_query | default('machine learning') }}"
    results = retriever.invoke(query)
    print(f"Retrieved {len(results)} documents for query: {query}")
    for i, doc in enumerate(results):
        print(f"\nDocument {i+1}:")
        print(f"  Content: {doc.page_content[:100]}...")
        print(f"  Source: {doc.metadata.get('source', 'unknown')}")

    # Retrieve with scores
    result_with_scores = retriever.retrieve_with_scores(query)
    print(f"\nRetrieval with scores:")
    for doc, score in zip(result_with_scores.documents, result_with_scores.scores):
        print(f"  Score: {score:.3f} - {doc.page_content[:50]}...")
