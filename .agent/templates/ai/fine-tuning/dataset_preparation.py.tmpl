"""
{{PREPARATION_NAME}} - Dataset Preparation for Fine-Tuning

Purpose: {{PREPARATION_PURPOSE}}
Author: {{AUTHOR}}
Date: {{DATE}}

Axiom Alignment:
- A1 (Verifiability): Dataset preparation is reproducible and logged
- A3 (Transparency): Data transformations are explicit and documented
"""

from typing import List, Dict, Any, Optional, Union
from pathlib import Path
from datasets import Dataset, DatasetDict, load_dataset
from transformers import AutoTokenizer
import pandas as pd
import json
import logging
from pydantic import BaseModel, Field

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatasetConfig(BaseModel):
    """Configuration for dataset preparation."""
    tokenizer_name: str = Field(description="Tokenizer name or path")
    max_length: int = Field(default={{MAX_LENGTH}}, description="Maximum sequence length")
    text_column: str = Field(default="{{TEXT_COLUMN}}", description="Text column name")
    train_split: float = Field(default={{TRAIN_SPLIT}}, description="Training split ratio")
    val_split: float = Field(default={{VAL_SPLIT}}, description="Validation split ratio")
    test_split: float = Field(default={{TEST_SPLIT}}, description="Test split ratio")


class {{PREPARATION_CLASS_NAME}}:
    """
    {{PREPARATION_NAME}} - Dataset Preparation for Fine-Tuning

    Prepares and preprocesses datasets for language model fine-tuning.
    Supports multiple input formats (JSON, CSV, text files) and provides
    utilities for formatting, tokenization, and splitting.

    Example:
        >>> preparer = {{PREPARATION_CLASS_NAME}}(
        ...     tokenizer_name="gpt2",
        ...     max_length=512
        ... )
        >>> dataset = preparer.load_from_json("data/training_data.json")
        >>> tokenized = preparer.prepare_dataset(dataset)
    """

    def __init__(
        self,
        tokenizer_name: str = "{{TOKENIZER_NAME}}",
        max_length: int = {{MAX_LENGTH}},
        text_column: str = "{{TEXT_COLUMN}}",
    ):
        """
        Initialize dataset preparer.

        Args:
            tokenizer_name: HuggingFace tokenizer identifier
            max_length: Maximum sequence length for tokenization
            text_column: Default column name for text data
        """
        self.tokenizer_name = tokenizer_name
        self.max_length = max_length
        self.text_column = text_column

        logger.info(f"Loading tokenizer: {tokenizer_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

        # Set padding token if not present
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def load_from_json(
        self,
        file_path: str,
        text_key: Optional[str] = None,
    ) -> Dataset:
        """
        Load dataset from JSON file.

        Args:
            file_path: Path to JSON file
            text_key: Key for text data (if None, uses text_column)

        Returns:
            Dataset object

        Example:
            >>> dataset = preparer.load_from_json("data/train.json", text_key="text")
        """
        logger.info(f"Loading dataset from JSON: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        # Handle different JSON formats
        if isinstance(data, list):
            records = data
        elif isinstance(data, dict) and "data" in data:
            records = data["data"]
        else:
            raise ValueError("Unsupported JSON format")

        # Extract text column
        text_key = text_key or self.text_column
        if text_key not in records[0]:
            raise ValueError(f"Text key '{text_key}' not found in data")

        # Convert to dataset
        dataset = Dataset.from_list(records)

        logger.info(f"Loaded {len(dataset)} examples from JSON")
        return dataset

    def load_from_csv(
        self,
        file_path: str,
        text_column: Optional[str] = None,
    ) -> Dataset:
        """
        Load dataset from CSV file.

        Args:
            file_path: Path to CSV file
            text_column: Column name for text (if None, uses text_column)

        Returns:
            Dataset object
        """
        logger.info(f"Loading dataset from CSV: {file_path}")

        df = pd.read_csv(file_path)
        text_column = text_column or self.text_column

        if text_column not in df.columns:
            raise ValueError(f"Text column '{text_column}' not found in CSV")

        dataset = Dataset.from_pandas(df)

        logger.info(f"Loaded {len(dataset)} examples from CSV")
        return dataset

    def load_from_text(
        self,
        file_path: str,
        separator: str = "\n\n",
    ) -> Dataset:
        """
        Load dataset from text file (one example per separator).

        Args:
            file_path: Path to text file
            separator: Separator between examples

        Returns:
            Dataset object
        """
        logger.info(f"Loading dataset from text file: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        texts = [text.strip() for text in content.split(separator) if text.strip()]
        dataset = Dataset.from_dict({self.text_column: texts})

        logger.info(f"Loaded {len(dataset)} examples from text file")
        return dataset

    def load_from_huggingface(
        self,
        dataset_name: str,
        subset: Optional[str] = None,
        split: Optional[str] = None,
        text_column: Optional[str] = None,
    ) -> Dataset:
        """
        Load dataset from HuggingFace Hub.

        Args:
            dataset_name: HuggingFace dataset name
            subset: Dataset subset (optional)
            split: Dataset split (e.g., "train", "validation")
            text_column: Column name for text

        Returns:
            Dataset object
        """
        logger.info(f"Loading dataset from HuggingFace: {dataset_name}")

        dataset_kwargs = {}
        if subset:
            dataset_kwargs["name"] = subset

        if split:
            dataset = load_dataset(dataset_name, split=split, **dataset_kwargs)
        else:
            dataset_dict = load_dataset(dataset_name, **dataset_kwargs)
            split_name = list(dataset_dict.keys())[0]
            dataset = dataset_dict[split_name]

        if text_column and text_column in dataset.column_names:
            dataset = dataset.rename_column(text_column, self.text_column)

        logger.info(f"Loaded {len(dataset)} examples from HuggingFace")
        return dataset

    def format_for_causal_lm(
        self,
        dataset: Dataset,
        text_column: Optional[str] = None,
        instruction_column: Optional[str] = None,
        response_column: Optional[str] = None,
        format_template: Optional[str] = None,
    ) -> Dataset:
        """
        Format dataset for causal language modeling.

        Args:
            dataset: Input dataset
            text_column: Column with text (for simple format)
            instruction_column: Column with instructions (for instruction format)
            response_column: Column with responses (for instruction format)
            format_template: Custom format template

        Returns:
            Formatted dataset
        """
        logger.info("Formatting dataset for causal LM...")

        text_col = text_column or self.text_column

        def format_example(example):
            """Format a single example."""
            if instruction_column and response_column:
                if format_template:
                    formatted = format_template.format(
                        instruction=example[instruction_column],
                        response=example[response_column]
                    )
                else:
                    formatted = f"### Instruction:\n{example[instruction_column]}\n\n### Response:\n{example[response_column]}"
            elif text_col in example:
                formatted = example[text_col]
            else:
                raise ValueError("No valid text column found")

            return {self.text_column: formatted}

        formatted_dataset = dataset.map(format_example)

        logger.info("Dataset formatted successfully")
        return formatted_dataset

    def tokenize_dataset(
        self,
        dataset: Dataset,
        text_column: Optional[str] = None,
        max_length: Optional[int] = None,
        truncation: bool = True,
        padding: str = "max_length",
    ) -> Dataset:
        """
        Tokenize dataset.

        Args:
            dataset: Input dataset
            text_column: Column name for text
            max_length: Maximum sequence length (uses self.max_length if None)
            truncation: Whether to truncate sequences
            padding: Padding strategy

        Returns:
            Tokenized dataset
        """
        logger.info("Tokenizing dataset...")

        text_col = text_column or self.text_column
        max_len = max_length or self.max_length

        def tokenize_function(examples):
            """Tokenize examples."""
            texts = examples[text_col]
            tokenized = self.tokenizer(
                texts,
                truncation=truncation,
                padding=padding,
                max_length=max_len,
                return_tensors=None,
            )

            tokenized["labels"] = tokenized["input_ids"].copy()

            return tokenized

        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names,
        )

        logger.info(f"Tokenized {len(tokenized_dataset)} examples")
        return tokenized_dataset

    def split_dataset(
        self,
        dataset: Dataset,
        train_split: float = {{TRAIN_SPLIT}},
        val_split: float = {{VAL_SPLIT}},
        test_split: Optional[float] = None,
        seed: int = {{RANDOM_SEED}},
    ) -> DatasetDict:
        """
        Split dataset into train/validation/test sets.

        Args:
            dataset: Input dataset
            train_split: Training split ratio
            val_split: Validation split ratio
            test_split: Test split ratio (optional)
            seed: Random seed for reproducibility

        Returns:
            DatasetDict with train/val/test splits
        """
        logger.info("Splitting dataset...")

        total = train_split + val_split + (test_split or 0)
        if abs(total - 1.0) > 1e-6:
            logger.warning(f"Splits sum to {total}, normalizing...")
            train_split /= total
            val_split /= total
            if test_split:
                test_split /= total

        if test_split:
            splits = dataset.train_test_split(
                test_size=1 - train_split,
                seed=seed
            )
            val_test_size = test_split / (val_split + test_split)
            val_test_splits = splits["test"].train_test_split(
                test_size=val_test_size,
                seed=seed
            )

            result = DatasetDict({
                "train": splits["train"],
                "validation": val_test_splits["train"],
                "test": val_test_splits["test"],
            })
        else:
            splits = dataset.train_test_split(
                test_size=val_split,
                seed=seed
            )

            result = DatasetDict({
                "train": splits["train"],
                "validation": splits["test"],
            })

        logger.info(
            f"Split dataset: train={len(result['train'])}, "
            f"val={len(result['validation'])}, "
            f"{f'test={len(result[\"test\"])}, ' if 'test' in result else ''}"
            f"total={len(dataset)}"
        )

        return result

    def prepare_dataset(
        self,
        dataset: Dataset,
        format_for_lm: bool = True,
        tokenize: bool = True,
        split: bool = True,
        **kwargs
    ) -> Union[Dataset, DatasetDict]:
        """
        Complete dataset preparation pipeline.

        Args:
            dataset: Input dataset
            format_for_lm: Whether to format for language modeling
            tokenize: Whether to tokenize
            split: Whether to split into train/val/test
            **kwargs: Additional arguments for formatting/tokenization

        Returns:
            Prepared dataset or DatasetDict
        """
        logger.info("Starting dataset preparation pipeline...")

        if format_for_lm:
            dataset = self.format_for_causal_lm(dataset, **kwargs)

        if tokenize:
            dataset = self.tokenize_dataset(dataset, **kwargs)

        if split:
            dataset = self.split_dataset(dataset, **kwargs)

        logger.info("Dataset preparation complete")
        return dataset

    def save_dataset(
        self,
        dataset: Union[Dataset, DatasetDict],
        output_dir: str,
    ) -> None:
        """
        Save prepared dataset to disk.

        Args:
            dataset: Dataset or DatasetDict to save
            output_dir: Output directory
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Saving dataset to {output_dir}...")

        if isinstance(dataset, DatasetDict):
            dataset.save_to_disk(str(output_dir))
        else:
            dataset.save_to_disk(str(output_dir))

        logger.info("Dataset saved successfully")


# Example usage
if __name__ == "__main__":
    preparer = {{PREPARATION_CLASS_NAME}}(
        tokenizer_name="{{TOKENIZER_NAME}}",
        max_length={{MAX_LENGTH}},
    )

    dataset = preparer.load_from_json("{{INPUT_DATA_PATH}}")

    prepared = preparer.prepare_dataset(
        dataset,
        format_for_lm=True,
        tokenize=True,
        split=True,
        train_split={{TRAIN_SPLIT}},
        val_split={{VAL_SPLIT}},
    )

    preparer.save_dataset(prepared, "{{OUTPUT_DATA_PATH}}")
