"""
vLLM Inference Server - High-performance LLM serving with vLLM

Purpose: Serve large language models using vLLM for optimized inference
Author: {{AUTHOR}}
Created: {{DATE}}

Axiom Alignment:
- A1 (Verifiability): Token-level logging and generation metadata
- A3 (Transparency): Clear visibility into generation parameters
- A4 (Non-Harm): Content filtering and safety checks

This server provides:
- vLLM-based LLM serving with PagedAttention
- Continuous batching for high throughput
- Token streaming support
- Multi-GPU support
- Quantization support (AWQ, GPTQ, etc.)
"""

from typing import List, Optional, Dict, Any, AsyncIterator, Union
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field, validator
from vllm import LLM, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
import logging
import json
import asyncio
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Global engine instance
engine: Optional[AsyncLLMEngine] = None


class GenerationRequest(BaseModel):
    """Request schema for text generation."""
    prompt: Union[str, List[str]] = Field(
        ...,
        description="Input prompt(s) for generation"
    )
    max_tokens: int = Field(
        default={{MAX_TOKENS}},
        ge=1,
        le={{MAX_MAX_TOKENS}},
        description="Maximum tokens to generate"
    )
    temperature: float = Field(
        default={{TEMPERATURE}},
        ge=0.0,
        le=2.0,
        description="Sampling temperature"
    )
    top_p: float = Field(
        default={{TOP_P}},
        ge=0.0,
        le=1.0,
        description="Nucleus sampling parameter"
    )
    top_k: int = Field(
        default={{TOP_K}},
        ge=-1,
        description="Top-k sampling (-1 for disabled)"
    )
    frequency_penalty: float = Field(
        default={{FREQUENCY_PENALTY}},
        ge=-2.0,
        le=2.0,
        description="Frequency penalty"
    )
    presence_penalty: float = Field(
        default={{PRESENCE_PENALTY}},
        ge=-2.0,
        le=2.0,
        description="Presence penalty"
    )
    stop: Optional[List[str]] = Field(
        default=None,
        description="Stop sequences"
    )
    stop_token_ids: Optional[List[int]] = Field(
        default=None,
        description="Stop token IDs"
    )
    stream: bool = Field(
        default=False,
        description="Whether to stream tokens"
    )
    logprobs: Optional[int] = Field(
        default=None,
        ge=0,
        description="Number of logprobs to return"
    )

    @validator('prompt')
    def validate_prompt(cls, v):
        """Validate prompt is not empty."""
        if isinstance(v, str) and not v.strip():
            raise ValueError("Prompt cannot be empty")
        if isinstance(v, list) and not v:
            raise ValueError("Prompt list cannot be empty")
        return v


class GenerationResponse(BaseModel):
    """Response schema for text generation."""
    text: Union[str, List[str]] = Field(
        ...,
        description="Generated text(s)"
    )
    prompt: Union[str, List[str]] = Field(
        ...,
        description="Input prompt(s)"
    )
    finish_reason: Union[str, List[str]] = Field(
        ...,
        description="Finish reason(s) (stop, length, etc.)"
    )
    usage: Dict[str, int] = Field(
        ...,
        description="Token usage statistics"
    )
    model: str = Field(..., description="Model name")
    timestamp: str = Field(..., description="Generation timestamp")
    logprobs: Optional[List[Dict[str, Any]]] = Field(
        default=None,
        description="Log probabilities if requested"
    )


class HealthResponse(BaseModel):
    """Health check response."""
    status: str = Field(..., description="Service status")
    model_loaded: bool = Field(..., description="Whether model is loaded")
    model_name: Optional[str] = Field(None, description="Loaded model name")
    gpu_count: int = Field(..., description="Number of GPUs")
    tensor_parallel_size: int = Field(..., description="Tensor parallelism size")


async def initialize_engine():
    """
    Initialize vLLM async engine.

    This function sets up the vLLM engine with the specified configuration.
    """
    global engine

    logger.info("Initializing vLLM engine")
    logger.info(f"Model: {{MODEL_NAME}}")
    logger.info(f"Tensor parallel size: {{TENSOR_PARALLEL_SIZE}}")
    logger.info(f"Pipeline parallel size: {{PIPELINE_PARALLEL_SIZE}}")

    try:
        engine_args = AsyncEngineArgs(
            model="{{MODEL_NAME}}",
            tensor_parallel_size={{TENSOR_PARALLEL_SIZE}},
            pipeline_parallel_size={{PIPELINE_PARALLEL_SIZE}},
            trust_remote_code={{TRUST_REMOTE_CODE}},
            dtype="{{DTYPE}}",
            max_model_len={{MAX_MODEL_LEN}},
            gpu_memory_utilization={{GPU_MEMORY_UTILIZATION}},
            max_num_seqs={{MAX_NUM_SEQS}},
            max_num_batched_tokens={{MAX_NUM_BATCHED_TOKENS}},
            enable_prefix_caching={{ENABLE_PREFIX_CACHING}},
            quantization="{{QUANTIZATION}}" if "{{QUANTIZATION}}" else None,
            load_format="{{LOAD_FORMAT}}",
            disable_log_stats={{DISABLE_LOG_STATS}},
            engine_use_ray={{ENGINE_USE_RAY}},
            disable_custom_all_reduce={{DISABLE_CUSTOM_ALL_REDUCE}},
        )

        engine = AsyncLLMEngine.from_engine_args(engine_args)

        logger.info("vLLM engine initialized successfully")

    except Exception as e:
        logger.error(f"Failed to initialize vLLM engine: {e}", exc_info=True)
        raise


async def generate_stream(
    request: GenerationRequest,
    request_id: str
) -> AsyncIterator[str]:
    """
    Stream generation results.

    Args:
        request: Generation request
        request_id: Unique request ID

    Yields:
        JSON strings with token updates
    """
    global engine

    if engine is None:
        yield json.dumps({"error": "Engine not initialized"})
        return

    # Create sampling params
    sampling_params = SamplingParams(
        temperature=request.temperature,
        top_p=request.top_p,
        top_k=request.top_k if request.top_k > 0 else -1,
        max_tokens=request.max_tokens,
        frequency_penalty=request.frequency_penalty,
        presence_penalty=request.presence_penalty,
        stop=request.stop,
        stop_token_ids=request.stop_token_ids,
        logprobs=request.logprobs,
    )

    # Prepare prompts
    prompts = [request.prompt] if isinstance(request.prompt, str) else request.prompt

    try:
        # Stream results
        async for request_output in engine.generate(
            prompt=prompts[0] if len(prompts) == 1 else prompts,
            sampling_params=sampling_params,
            request_id=request_id
        ):
            if len(prompts) == 1:
                # Single prompt
                output = request_output.outputs[0]
                yield json.dumps({
                    "text": output.text,
                    "finish_reason": output.finish_reason,
                    "logprobs": output.logprobs if request.logprobs else None
                }) + "\n"
            else:
                # Batch
                yield json.dumps({
                    "texts": [out.text for out in request_output.outputs],
                    "finish_reasons": [out.finish_reason for out in request_output.outputs]
                }) + "\n"

    except Exception as e:
        logger.error(f"Generation error: {e}", exc_info=True)
        yield json.dumps({"error": str(e)}) + "\n"


def create_app() -> FastAPI:
    """
    Create and configure FastAPI application.

    Returns:
        Configured FastAPI application instance
    """
    app = FastAPI(
        title="{{API_TITLE}}",
        version="{{API_VERSION}}",
        description="{{API_DESCRIPTION}}",
        docs_url="/docs" if {{ENABLE_DOCS}} else None,
    )

    # Startup event
    @app.on_event("startup")
    async def startup_event():
        """Initialize engine on startup."""
        await initialize_engine()

    # Shutdown event
    @app.on_event("shutdown")
    async def shutdown_event():
        """Cleanup on shutdown."""
        global engine
        if engine:
            # vLLM engine cleanup is handled automatically
            logger.info("Shutting down vLLM engine")

    # Register routes
    setup_routes(app)

    return app


def setup_routes(app: FastAPI) -> None:
    """
    Register API routes.

    Args:
        app: FastAPI application instance
    """
    import uuid

    @app.post("/generate", response_model=GenerationResponse, tags=["Generation"])
    async def generate(request: GenerationRequest):
        """
        Generate text using the loaded model.

        Args:
            request: Generation request with prompt and parameters

        Returns:
            GenerationResponse with generated text and metadata
        """
        global engine

        if engine is None:
            raise HTTPException(
                status_code=503,
                detail="Engine not initialized"
            )

        if request.stream:
            # Return streaming response
            request_id = str(uuid.uuid4())
            return StreamingResponse(
                generate_stream(request, request_id),
                media_type="application/x-ndjson"
            )

        # Non-streaming generation
        try:
            sampling_params = SamplingParams(
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k if request.top_k > 0 else -1,
                max_tokens=request.max_tokens,
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
                stop=request.stop,
                stop_token_ids=request.stop_token_ids,
                logprobs=request.logprobs,
            )

            prompts = [request.prompt] if isinstance(request.prompt, str) else request.prompt

            # Generate
            request_id = str(uuid.uuid4())
            results = []

            async for request_output in engine.generate(
                prompt=prompts[0] if len(prompts) == 1 else prompts,
                sampling_params=sampling_params,
                request_id=request_id
            ):
                if len(prompts) == 1:
                    output = request_output.outputs[0]
                    results.append({
                        "text": output.text,
                        "finish_reason": output.finish_reason,
                        "logprobs": output.logprobs if request.logprobs else None
                    })
                else:
                    results.append({
                        "texts": [out.text for out in request_output.outputs],
                        "finish_reasons": [out.finish_reason for out in request_output.outputs]
                    })

            # Get final result
            final_result = results[-1] if results else {}

            # Calculate usage (simplified - vLLM provides this in request_output)
            usage = {
                "prompt_tokens": 0,  # Would need to tokenize to get exact count
                "completion_tokens": 0,
                "total_tokens": 0
            }

            if len(prompts) == 1:
                return GenerationResponse(
                    text=final_result.get("text", ""),
                    prompt=request.prompt,
                    finish_reason=final_result.get("finish_reason", "unknown"),
                    usage=usage,
                    model="{{MODEL_NAME}}",
                    timestamp=datetime.utcnow().isoformat(),
                    logprobs=final_result.get("logprobs")
                )
            else:
                return GenerationResponse(
                    text=final_result.get("texts", []),
                    prompt=request.prompt,
                    finish_reason=final_result.get("finish_reasons", []),
                    usage=usage,
                    model="{{MODEL_NAME}}",
                    timestamp=datetime.utcnow().isoformat()
                )

        except Exception as e:
            logger.error(f"Generation failed: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"Generation failed: {str(e)}"
            )

    @app.get("/health", response_model=HealthResponse, tags=["Health"])
    async def health_check():
        """
        Health check endpoint.

        Returns:
            Health status and model information
        """
        global engine

        import torch

        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0

        return HealthResponse(
            status="healthy" if engine is not None else "unhealthy",
            model_loaded=engine is not None,
            model_name="{{MODEL_NAME}}" if engine else None,
            gpu_count=gpu_count,
            tensor_parallel_size={{TENSOR_PARALLEL_SIZE}}
        )

    @app.get("/model/info", tags=["Model"])
    async def get_model_info():
        """
        Get model information.

        Returns:
            Model metadata and configuration
        """
        global engine

        if engine is None:
            raise HTTPException(
                status_code=503,
                detail="Engine not initialized"
            )

        return {
            "model": "{{MODEL_NAME}}",
            "tensor_parallel_size": {{TENSOR_PARALLEL_SIZE}},
            "pipeline_parallel_size": {{PIPELINE_PARALLEL_SIZE}},
            "max_model_len": {{MAX_MODEL_LEN}},
            "dtype": "{{DTYPE}}",
            "quantization": "{{QUANTIZATION}}" if "{{QUANTIZATION}}" else None,
            "gpu_memory_utilization": {{GPU_MEMORY_UTILIZATION}},
            "max_num_seqs": {{MAX_NUM_SEQS}}
        }

    logger.info("Routes registered")


# Create application instance
app = create_app()


def main():
    """
    Main entry point for running the server.
    """
    import uvicorn

    uvicorn.run(
        "{{MODULE_NAME}}.serving.vllm_server:app",
        host="{{HOST}}",
        port={{PORT}},
        log_level="{{LOG_LEVEL}}",
        workers=1  # vLLM handles parallelism internally
    )


if __name__ == "__main__":
    main()
