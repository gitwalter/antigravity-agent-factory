"""
Data Loading and Preprocessing Template

Purpose: Data loading utilities with preprocessing, augmentation, and batching
Author: {{AUTHOR}}
Created: {{DATE}}

This template provides:
- Dataset classes for various data types
- Data preprocessing pipelines
- Data augmentation strategies
- Efficient data loading with caching
- Data validation and quality checks
"""

import os
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Callable, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod

import numpy as np
import pandas as pd
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torchvision import transforms
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split

# Configure logging
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class DataConfig:
    """Configuration for data loading and preprocessing."""
    # Data paths
    data_dir: str = "{{DATA_DIR}}"
    train_file: Optional[str] = "{{TRAIN_FILE}}"
    val_file: Optional[str] = "{{VAL_FILE}}"
    test_file: Optional[str] = "{{TEST_FILE}}"
    
    # Data loading
    batch_size: int = {{BATCH_SIZE}}
    num_workers: int = {{NUM_WORKERS}}
    shuffle: bool = True
    pin_memory: bool = True
    
    # Preprocessing
    normalize: bool = True
    normalize_method: str = "standard"  # Options: "standard", "minmax", "none"
    augment: bool = True
    
    # Data validation
    validate_data: bool = True
    cache_data: bool = True
    cache_dir: str = "{{CACHE_DIR}}"
    
    # Image-specific
    image_size: Tuple[int, int] = ({{IMAGE_HEIGHT}}, {{IMAGE_WIDTH}})
    image_channels: int = {{IMAGE_CHANNELS}}
    
    # Text-specific
    max_sequence_length: int = {{MAX_SEQUENCE_LENGTH}}
    tokenizer_name: Optional[str] = "{{TOKENIZER_NAME}}"
    
    # Split configuration
    val_split: float = {{VAL_SPLIT}}
    test_split: float = {{TEST_SPLIT}}
    random_seed: int = {{RANDOM_SEED}}


class BaseDataset(Dataset, ABC):
    """
    Abstract base class for datasets.
    
    This class provides a common interface for all dataset types
    and includes caching functionality.
    """
    
    def __init__(
        self,
        data_path: str,
        transform: Optional[Callable] = None,
        cache: bool = True,
        cache_dir: Optional[str] = None
    ):
        """
        Initialize dataset.
        
        Args:
            data_path: Path to data file or directory
            transform: Optional transform to apply to samples
            cache: Whether to cache processed data
            cache_dir: Directory for caching
        """
        self.data_path = Path(data_path)
        self.transform = transform
        self.cache = cache
        self.cache_dir = Path(cache_dir) if cache_dir else None
        
        if self.cache and self.cache_dir:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self._load_data()
    
    @abstractmethod
    def _load_data(self) -> None:
        """Load and preprocess data."""
        pass
    
    @abstractmethod
    def __len__(self) -> int:
        """Return dataset size."""
        pass
    
    @abstractmethod
    def __getitem__(self, idx: int) -> Tuple[Any, Any]:
        """Get a sample from the dataset."""
        pass
    
    def _get_cache_path(self) -> Optional[Path]:
        """Get cache file path."""
        if not self.cache or not self.cache_dir:
            return None
        cache_name = f"{self.data_path.stem}_{hash(str(self.data_path))}.pkl"
        return self.cache_dir / cache_name
    
    def _load_from_cache(self) -> Optional[Any]:
        """Load data from cache if available."""
        cache_path = self._get_cache_path()
        if cache_path and cache_path.exists():
            logger.info(f"Loading data from cache: {cache_path}")
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        return None
    
    def _save_to_cache(self, data: Any) -> None:
        """Save data to cache."""
        cache_path = self._get_cache_path()
        if cache_path:
            logger.info(f"Saving data to cache: {cache_path}")
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)


class ImageDataset(BaseDataset):
    """
    Dataset for image data with augmentation support.
    
    Supports common image formats (PNG, JPG, JPEG) and provides
    data augmentation capabilities.
    """
    
    def __init__(
        self,
        data_path: str,
        image_size: Tuple[int, int] = (224, 224),
        augment: bool = True,
        normalize: bool = True,
        **kwargs
    ):
        """
        Initialize image dataset.
        
        Args:
            data_path: Path to image directory or CSV file with image paths
            image_size: Target image size (height, width)
            augment: Whether to apply data augmentation
            normalize: Whether to normalize images
        """
        self.image_size = image_size
        self.augment = augment
        self.normalize = normalize
        
        # Setup transforms
        self.transform = self._create_transforms()
        
        super().__init__(data_path, transform=self.transform, **kwargs)
    
    def _create_transforms(self) -> transforms.Compose:
        """
        Create image transforms pipeline.
        
        Returns:
            Composed transforms
        """
        transform_list = []
        
        # Resize
        transform_list.append(transforms.Resize(self.image_size))
        
        # Augmentation (only for training)
        if self.augment:
            transform_list.extend([
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomRotation(degrees=15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2),
                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
            ])
        
        # Convert to tensor
        transform_list.append(transforms.ToTensor())
        
        # Normalize
        if self.normalize:
            transform_list.append(
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],  # ImageNet stats
                    std=[0.229, 0.224, 0.225]
                )
            )
        
        return transforms.Compose(transform_list)
    
    def _load_data(self) -> None:
        """Load image paths and labels."""
        cached_data = self._load_from_cache()
        if cached_data:
            self.samples = cached_data
            return
        
        self.samples = []
        
        if self.data_path.is_dir():
            # Load from directory structure: data_dir/class_name/image.jpg
            for class_dir in self.data_path.iterdir():
                if class_dir.is_dir():
                    class_name = class_dir.name
                    for img_file in class_dir.glob("*.jpg") + class_dir.glob("*.png"):
                        self.samples.append((str(img_file), class_name))
        elif self.data_path.suffix == '.csv':
            # Load from CSV: path, label columns
            df = pd.read_csv(self.data_path)
            for _, row in df.iterrows():
                self.samples.append((row['{{IMAGE_PATH_COL}}'], row['{{LABEL_COL}}']))
        else:
            raise ValueError(f"Unsupported data path format: {self.data_path}")
        
        if self.cache:
            self._save_to_cache(self.samples)
        
        logger.info(f"Loaded {len(self.samples)} image samples")
    
    def __len__(self) -> int:
        """Return dataset size."""
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Any]:
        """
        Get an image sample.
        
        Args:
            idx: Sample index
            
        Returns:
            Tuple of (image tensor, label)
        """
        img_path, label = self.samples[idx]
        
        # Load image
        image = Image.open(img_path).convert('RGB')
        
        # Apply transforms
        if self.transform:
            image = self.transform(image)
        
        return image, label


class TabularDataset(BaseDataset):
    """
    Dataset for tabular data with preprocessing.
    
    Supports CSV files and provides normalization and encoding capabilities.
    """
    
    def __init__(
        self,
        data_path: str,
        target_column: str,
        feature_columns: Optional[List[str]] = None,
        normalize: bool = True,
        normalize_method: str = "standard",
        **kwargs
    ):
        """
        Initialize tabular dataset.
        
        Args:
            data_path: Path to CSV file
            target_column: Name of target column
            feature_columns: List of feature column names (None = all except target)
            normalize: Whether to normalize features
            normalize_method: Normalization method ("standard" or "minmax")
        """
        self.target_column = target_column
        self.feature_columns = feature_columns
        self.normalize = normalize
        self.normalize_method = normalize_method
        self.scaler = None
        self.label_encoder = LabelEncoder()
        
        super().__init__(data_path, **kwargs)
    
    def _load_data(self) -> None:
        """Load and preprocess tabular data."""
        cached_data = self._load_from_cache()
        if cached_data:
            self.data = cached_data['data']
            self.labels = cached_data['labels']
            self.scaler = cached_data.get('scaler')
            return
        
        # Load CSV
        df = pd.read_csv(self.data_path)
        
        # Extract features and target
        if self.feature_columns is None:
            self.feature_columns = [col for col in df.columns if col != self.target_column]
        
        X = df[self.feature_columns].values
        y = df[self.target_column].values
        
        # Encode labels if needed
        if y.dtype == object:
            y = self.label_encoder.fit_transform(y)
        
        # Normalize features
        if self.normalize:
            if self.normalize_method == "standard":
                self.scaler = StandardScaler()
            elif self.normalize_method == "minmax":
                self.scaler = MinMaxScaler()
            else:
                raise ValueError(f"Unknown normalization method: {self.normalize_method}")
            
            X = self.scaler.fit_transform(X)
        
        self.data = torch.FloatTensor(X)
        self.labels = torch.LongTensor(y)
        
        if self.cache:
            self._save_to_cache({
                'data': self.data,
                'labels': self.labels,
                'scaler': self.scaler
            })
        
        logger.info(f"Loaded {len(self.data)} samples with {len(self.feature_columns)} features")
    
    def __len__(self) -> int:
        """Return dataset size."""
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get a tabular sample.
        
        Args:
            idx: Sample index
            
        Returns:
            Tuple of (features tensor, label tensor)
        """
        return self.data[idx], self.labels[idx]


class DataLoaderFactory:
    """
    Factory class for creating data loaders with proper configuration.
    
    Provides convenient methods for creating train/val/test splits
    and data loaders with appropriate settings.
    """
    
    @staticmethod
    def create_image_loaders(
        data_dir: str,
        batch_size: int = 32,
        image_size: Tuple[int, int] = (224, 224),
        val_split: float = 0.2,
        test_split: float = 0.1,
        augment_train: bool = True,
        num_workers: int = 4,
        **kwargs
    ) -> Tuple[DataLoader, Optional[DataLoader], Optional[DataLoader]]:
        """
        Create image data loaders with train/val/test splits.
        
        Args:
            data_dir: Path to image data directory
            batch_size: Batch size
            image_size: Target image size
            val_split: Validation split ratio
            test_split: Test split ratio
            augment_train: Whether to augment training data
            num_workers: Number of data loading workers
            
        Returns:
            Tuple of (train_loader, val_loader, test_loader)
        """
        # Create full dataset
        full_dataset = ImageDataset(
            data_path=data_dir,
            image_size=image_size,
            augment=False,  # Will set per split
            **kwargs
        )
        
        # Split dataset
        train_size = int((1 - val_split - test_split) * len(full_dataset))
        val_size = int(val_split * len(full_dataset))
        test_size = len(full_dataset) - train_size - val_size
        
        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
            full_dataset,
            [train_size, val_size, test_size]
        )
        
        # Enable augmentation for training
        train_dataset.dataset.augment = augment_train
        train_dataset.dataset.transform = train_dataset.dataset._create_transforms()
        
        # Create data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        ) if val_size > 0 else None
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        ) if test_size > 0 else None
        
        return train_loader, val_loader, test_loader
    
    @staticmethod
    def create_tabular_loaders(
        data_path: str,
        target_column: str,
        batch_size: int = 32,
        val_split: float = 0.2,
        test_split: float = 0.1,
        normalize: bool = True,
        num_workers: int = 0,  # Tabular data doesn't benefit from multiple workers
        **kwargs
    ) -> Tuple[DataLoader, Optional[DataLoader], Optional[DataLoader]]:
        """
        Create tabular data loaders with train/val/test splits.
        
        Args:
            data_path: Path to CSV file
            target_column: Name of target column
            batch_size: Batch size
            val_split: Validation split ratio
            test_split: Test split ratio
            normalize: Whether to normalize features
            num_workers: Number of data loading workers
            
        Returns:
            Tuple of (train_loader, val_loader, test_loader)
        """
        # Load and split data
        df = pd.read_csv(data_path)
        
        # Split indices
        indices = np.arange(len(df))
        train_idx, temp_idx = train_test_split(
            indices,
            test_size=val_split + test_split,
            random_state=42
        )
        val_idx, test_idx = train_test_split(
            temp_idx,
            test_size=test_split / (val_split + test_split),
            random_state=42
        )
        
        # Create datasets
        train_dataset = TabularDataset(
            data_path=data_path,
            target_column=target_column,
            normalize=normalize,
            **kwargs
        )
        # Filter to train indices
        train_dataset.data = train_dataset.data[train_idx]
        train_dataset.labels = train_dataset.labels[train_idx]
        
        val_dataset = TabularDataset(
            data_path=data_path,
            target_column=target_column,
            normalize=normalize,
            scaler=train_dataset.scaler,  # Use same scaler
            **kwargs
        )
        val_dataset.data = val_dataset.data[val_idx]
        val_dataset.labels = val_dataset.labels[val_idx]
        
        test_dataset = TabularDataset(
            data_path=data_path,
            target_column=target_column,
            normalize=normalize,
            scaler=train_dataset.scaler,  # Use same scaler
            **kwargs
        )
        test_dataset.data = test_dataset.data[test_idx]
        test_dataset.labels = test_dataset.labels[test_idx]
        
        # Create data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers
        ) if len(val_idx) > 0 else None
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers
        ) if len(test_idx) > 0 else None
        
        return train_loader, val_loader, test_loader


# Example usage
if __name__ == "__main__":
    # Example: Create image data loaders
    # train_loader, val_loader, test_loader = DataLoaderFactory.create_image_loaders(
    #     data_dir="./data/images",
    #     batch_size=32,
    #     image_size=(224, 224),
    #     val_split=0.2,
    #     test_split=0.1
    # )
    # 
    # # Example: Create tabular data loaders
    # train_loader, val_loader, test_loader = DataLoaderFactory.create_tabular_loaders(
    #     data_path="./data/tabular_data.csv",
    #     target_column="target",
    #     batch_size=32,
    #     val_split=0.2,
    #     test_split=0.1
    # )
    pass
