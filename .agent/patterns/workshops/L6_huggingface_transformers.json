{
  "$schema": "../learning-workshop-pattern.json",
  "workshopId": "L6_huggingface_transformers",
  "name": "Hugging Face Transformers Mastery",
  "technology": {
    "category": "ai_framework",
    "stack": "Hugging Face",
    "language": "Python",
    "version": "Transformers 4.40+"
  },
  "level": "fundamentals",
  "prerequisites": {
    "workshops": [],
    "knowledge": [
      "Python programming (functions, classes)",
      "Basic understanding of neural networks",
      "PyTorch or TensorFlow basics"
    ],
    "tools": [
      "Python 3.10+",
      "PyTorch or TensorFlow installed",
      "Hugging Face account (for Hub access)",
      "VS Code or similar IDE"
    ]
  },
  "duration": {
    "total_hours": 2.5,
    "concept_minutes": 30,
    "demo_minutes": 30,
    "exercise_minutes": 45,
    "challenge_minutes": 30,
    "reflection_minutes": 15
  },
  "learning_objectives": [
    {
      "objective": "Load pre-trained models using Auto classes (AutoModel, AutoTokenizer, AutoModelForCausalLM)",
      "bloom_level": "apply",
      "verification": "Successfully loads model and tokenizer from Hugging Face Hub"
    },
    {
      "objective": "Implement inference patterns for text generation and embeddings",
      "bloom_level": "apply",
      "verification": "Generates text and extracts embeddings from models"
    },
    {
      "objective": "Apply quantization techniques (4-bit, 8-bit) for efficient inference",
      "bloom_level": "apply",
      "verification": "Loads and runs quantized model successfully"
    },
    {
      "objective": "Fine-tune models using PEFT/LoRA techniques",
      "bloom_level": "apply",
      "verification": "Fine-tunes a model with LoRA and achieves improved performance"
    },
    {
      "objective": "Push models and datasets to Hugging Face Hub",
      "bloom_level": "apply",
      "verification": "Successfully uploads model to Hub and can load it back"
    }
  ],
  "knowledge_files": [
    "huggingface-patterns.json",
    "deep-learning-patterns.json"
  ],
  "phases": [
    {
      "phaseId": "concept",
      "name": "Transformers Architecture Overview",
      "type": "concept",
      "duration_minutes": 30,
      "description": "Understand the Hugging Face Transformers ecosystem",
      "content": {
        "topics": [
          "Hugging Face Hub: Model and dataset repository",
          "Auto Classes: Dynamic model loading",
          "Tokenizers: Text preprocessing and encoding",
          "Model Architectures: Encoder, Decoder, Encoder-Decoder",
          "Pipelines: High-level inference API",
          "Quantization: 4-bit and 8-bit model compression",
          "PEFT: Parameter-Efficient Fine-Tuning (LoRA, QLoRA)"
        ],
        "diagrams": [
          "Transformers library architecture",
          "Model loading flow",
          "Text generation pipeline",
          "LoRA fine-tuning diagram"
        ],
        "key_points": [
          "Auto classes automatically select the right model architecture",
          "Tokenizers handle text preprocessing consistently",
          "Pipelines provide simple high-level APIs",
          "Quantization reduces memory without significant accuracy loss",
          "LoRA enables efficient fine-tuning with minimal parameters",
          "Hub provides centralized model and dataset storage"
        ]
      },
      "facilitator_notes": "Start with simple examples before showing advanced techniques. Emphasize the ecosystem approach.",
      "common_questions": [
        "What's the difference between AutoModel and AutoModelForCausalLM?",
        "When should I use quantization?",
        "How does LoRA work compared to full fine-tuning?"
      ]
    },
    {
      "phaseId": "demo",
      "name": "Loading and Using Models",
      "type": "demo",
      "duration_minutes": 30,
      "description": "Live coding model loading and inference",
      "content": {
        "topics": [
          "Loading models from Hub",
          "Using tokenizers for preprocessing",
          "Text generation with generate()",
          "Extracting embeddings",
          "Using pipelines for quick inference",
          "Loading quantized models"
        ],
        "code_examples": [
          "Basic model and tokenizer loading",
          "Text generation example",
          "Embedding extraction",
          "Pipeline usage",
          "Quantized model loading"
        ],
        "key_points": [
          "Always use appropriate Auto class for your task",
          "Tokenizers must match the model",
          "Generation parameters affect output quality",
          "Pipelines simplify common tasks",
          "Quantization requires compatible libraries"
        ]
      },
      "facilitator_notes": "Show real model loading times and memory usage. Discuss GPU requirements."
    },
    {
      "phaseId": "exercise_1",
      "name": "Text Generation Pipeline",
      "type": "exercise",
      "duration_minutes": 45,
      "description": "Build a text generation system",
      "content": {
        "topics": [
          "Load a causal language model",
          "Implement text generation",
          "Add sampling strategies",
          "Handle long sequences",
          "Optimize generation parameters"
        ]
      }
    },
    {
      "phaseId": "challenge",
      "name": "LoRA Fine-Tuning",
      "type": "challenge",
      "duration_minutes": 30,
      "description": "Fine-tune a model using LoRA",
      "content": {
        "topics": [
          "Prepare training dataset",
          "Configure LoRA parameters",
          "Set up training loop",
          "Monitor training metrics",
          "Save and load fine-tuned model"
        ]
      }
    },
    {
      "phaseId": "reflection",
      "name": "Key Takeaways and Production Considerations",
      "type": "reflection",
      "duration_minutes": 15,
      "description": "Consolidate learning and discuss production deployment",
      "content": {
        "topics": [
          "Summary of Transformers patterns",
          "Model selection best practices",
          "Quantization trade-offs",
          "Fine-tuning strategies",
          "Production considerations (deployment, optimization)",
          "Resources for continued learning"
        ],
        "key_points": [
          "Choose models based on task and resource constraints",
          "Quantization enables efficient deployment",
          "LoRA is efficient for domain adaptation",
          "Monitor memory usage and inference speed",
          "Use pipelines for rapid prototyping"
        ]
      }
    }
  ],
  "exercises": [
    {
      "exerciseId": "ex1_text_generation",
      "name": "Text Generation Pipeline",
      "type": "guided",
      "difficulty": "medium",
      "duration_minutes": 45,
      "description": "Build a text generation system with configurable parameters",
      "starter_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# TODO: Load model and tokenizer\n# Use a small model like \"gpt2\" or \"distilgpt2\"\nmodel_name = \"gpt2\"\n\n# TODO: Load tokenizer\ntokenizer = # AutoTokenizer.from_pretrained(...)\n\n# TODO: Load model\ntokenizer = # AutoModelForCausalLM.from_pretrained(...)\n\n# TODO: Set model to evaluation mode\n# model.eval()\n\n# TODO: Define generation function\ndef generate_text(prompt, max_length=50, temperature=1.0, top_p=0.9):\n    \"\"\"Generate text from a prompt.\"\"\"\n    # TODO: Tokenize input\n    # inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n    \n    # TODO: Generate with parameters\n    # outputs = model.generate(...)\n    \n    # TODO: Decode output\n    # generated_text = tokenizer.decode(...)\n    \n    return \"\"\n\n# Test\nprompt = \"The future of AI is\"\nresult = generate_text(prompt, max_length=100, temperature=0.7)\nprint(result)",
      "solution_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Load model and tokenizer\nmodel_name = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Set pad token if not present\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Move to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\ndef generate_text(prompt, max_length=50, temperature=1.0, top_p=0.9, top_k=50):\n    \"\"\"Generate text from a prompt with configurable parameters.\"\"\"\n    # Tokenize input\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    \n    # Generate with parameters\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_length=max_length,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n            num_return_sequences=1\n        )\n    \n    # Decode output\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return generated_text\n\n# Test\nprompt = \"The future of AI is\"\nresult = generate_text(prompt, max_length=100, temperature=0.7)\nprint(result)\n\n# Test with different parameters\nprint(\"\\n--- Creative generation ---\")\ncreative = generate_text(prompt, max_length=100, temperature=1.2, top_p=0.95)\nprint(creative)\n\nprint(\"\\n--- Conservative generation ---\")\nconservative = generate_text(prompt, max_length=100, temperature=0.3, top_p=0.5)\nprint(conservative)",
      "hints": [
        "Use AutoTokenizer.from_pretrained() for tokenizer",
        "Use AutoModelForCausalLM.from_pretrained() for generation models",
        "Set pad_token if needed",
        "Use model.generate() with appropriate parameters",
        "Temperature controls randomness (lower = more deterministic)",
        "top_p and top_k control sampling diversity"
      ],
      "verification": "Function generates coherent text with different parameter settings",
      "common_mistakes": [
        "Not setting pad_token",
        "Forgetting to set model.eval()",
        "Not handling device placement (CPU vs GPU)",
        "Wrong model class for generation task",
        "Not using do_sample=True with temperature"
      ]
    }
  ],
  "challenges": [
    {
      "challengeId": "lora_finetuning",
      "name": "LoRA Fine-Tuning",
      "description": "Fine-tune a language model using LoRA for a specific task",
      "requirements": [
        "Prepare a training dataset (at least 100 examples)",
        "Load base model and configure LoRA",
        "Set up training with Trainer API",
        "Train for at least 3 epochs",
        "Evaluate on validation set",
        "Save fine-tuned model",
        "Compare performance before and after fine-tuning"
      ],
      "evaluation_criteria": [
        "Dataset is properly formatted",
        "LoRA configuration is correct",
        "Training completes without errors",
        "Model performance improves",
        "Fine-tuned model can be loaded and used",
        "Memory usage is significantly lower than full fine-tuning"
      ],
      "stretch_goals": [
        "Use QLoRA (quantized LoRA) for even lower memory",
        "Push fine-tuned model to Hub",
        "Create inference script using fine-tuned model",
        "Compare LoRA vs full fine-tuning performance"
      ]
    }
  ],
  "resources": {
    "official_docs": [
      "https://huggingface.co/docs/transformers/",
      "https://huggingface.co/docs/peft/"
    ],
    "tutorials": [
      "https://huggingface.co/learn/nlp-course/",
      "https://huggingface.co/docs/transformers/training"
    ],
    "videos": [
      "Hugging Face Transformers Course",
      "Fine-tuning Tutorials"
    ],
    "community": [
      "Hugging Face Forums",
      "Discord Community"
    ]
  },
  "assessment": {
    "knowledge_check": [
      {
        "question": "What are Auto classes and why are they useful?",
        "type": "short_answer",
        "answer": "Auto classes (AutoModel, AutoTokenizer, etc.) automatically select the appropriate model architecture or tokenizer class based on the model name. They simplify model loading and make code more flexible.",
        "explanation": "Auto classes reduce boilerplate and make switching models easier"
      },
      {
        "question": "What is quantization and what are its benefits?",
        "type": "short_answer",
        "answer": "Quantization reduces model precision (e.g., from 32-bit to 8-bit or 4-bit) to decrease memory usage and speed up inference. Benefits include lower memory requirements, faster inference, and ability to run larger models on limited hardware.",
        "explanation": "Trade-off is slight accuracy reduction for significant resource savings"
      },
      {
        "question": "How does LoRA differ from full fine-tuning?",
        "type": "short_answer",
        "answer": "LoRA (Low-Rank Adaptation) adds trainable low-rank matrices to the model instead of training all parameters. It uses much less memory and trains faster while achieving similar performance for domain adaptation tasks.",
        "explanation": "LoRA is parameter-efficient, making fine-tuning accessible with limited resources"
      }
    ],
    "practical_assessment": "Load a model, generate text with different parameters, and fine-tune it using LoRA",
    "self_assessment": [
      "Can I load models using Auto classes?",
      "Do I understand how to configure generation parameters?",
      "Can I implement quantization for efficient inference?",
      "Do I know how to fine-tune models with LoRA?",
      "Can I push models to Hugging Face Hub?"
    ]
  },
  "next_steps": {
    "next_workshop": "L7_langchain_fundamentals",
    "practice_projects": [
      "Domain-specific chatbot fine-tuning",
      "Text classification with transformers",
      "Multi-modal model exploration"
    ],
    "deeper_learning": [
      "Advanced fine-tuning techniques",
      "Model optimization and deployment",
      "Multi-modal transformers"
    ]
  },
  "axiom_zero_integration": {
    "love_moments": [
      "Celebrating successful model loading and generation",
      "Encouraging experimentation with different models",
      "Patient debugging of quantization and fine-tuning issues"
    ],
    "truth_moments": [
      "Honest discussion of model limitations and biases",
      "Clear explanation of computational costs",
      "Acknowledging when simpler models work better"
    ],
    "beauty_moments": [
      "Elegant model loading patterns",
      "The satisfaction of seeing fine-tuned models perform well",
      "Clean, well-structured inference pipelines"
    ]
  }
}
