{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "LangChain Patterns",
  "description": "Best practices and patterns for LangChain 1.x agent development",
  "version": "2.1.0",  "axiomAlignment": {
    "A1_verifiability": "Patterns include testing strategies for verification",
    "A3_transparency": "All patterns emphasize explainable agent behavior"
  },
      "code_example": "from langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.chat_messages import HumanMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import RunnablePassthrough\n\nhistory = InMemoryChatMessageHistory()\nllm = ChatOpenAI(model='gpt-4')\n\nprompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a helpful assistant.'),\n    MessagesPlaceholder(variable_name='history'),\n    ('human', '{input}')\n])\n\nchain = (\n    RunnablePassthrough.assign(\n        history=lambda x: history.messages\n    )\n    | prompt\n    | llm\n)\n\nresponse = chain.invoke({'input': 'Hello, how are you?'})\nhistory.add_message(HumanMessage(content='Hello, how are you?'))\nhistory.add_message(AIMessage(content=response.content))",      "limitations": "Token limit for long conversations"
    },
    "conversation_summary": {
      "description": "Summarize conversation to save tokens",
      "use_when": "Long conversations, need to stay within token limits",
      "implementation": "ConversationSummaryBufferMemory with LCEL",
      "code_example": "from langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\nhistory = InMemoryChatMessageHistory()\nllm = ChatOpenAI(model='gpt-4')\nsummary_llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n\n# Summarize when history exceeds token limit\nsummary_prompt = ChatPromptTemplate.from_template(\n    'Summarize this conversation: {messages}'\n)\n\n# Use RunnableLambda to check and summarize when needed\n# Implementation would check token count and summarize if needed",
      "best_practices": [
        "Use smaller model for summarization if cost-sensitive",
        "Periodically verify summary quality",
        "Set max_token_limit appropriately",
        "Use LCEL for memory management in LangChain 1.x"      ]
    },
    "conversation_window": {
      "description": "Keep only last N messages",
      "use_when": "Recent context is most important",
      "implementation": "InMemoryChatMessageHistory with windowing",
      "code_example": "from langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.chat_messages import HumanMessage, AIMessage\n\nhistory = InMemoryChatMessageHistory()\n\n# Add messages\nhistory.add_message(HumanMessage(content='Message 1'))\nhistory.add_message(AIMessage(content='Response 1'))\n\n# Get last N messages\nwindow_size = 5\nrecent_messages = history.messages[-window_size:]",
      "best_practices": [
        "Choose k based on context window size",
        "Consider message length when setting k",
        "Use InMemoryChatMessageHistory for LangChain 1.x"      ]
    },
    "conversation_summary_buffer": {
      "description": "Combine summary with recent messages",
      "use_when": "Need both long-term context and recent details",
      "code_example": "from langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.runnables import RunnableLambda\n\nhistory = InMemoryChatMessageHistory()\nllm = ChatOpenAI(model='gpt-4')\nsummary_llm = ChatOpenAI(model='gpt-3.5-turbo')\n\n# Implement custom logic to maintain summary + recent messages\n# Check token count, summarize older messages, keep recent ones",
      "best_practices": [
        "Set max_token_limit based on model context window",
        "Monitor token usage",
        "Use LCEL for custom memory management in LangChain 1.x"      ]
    },
    "vector_store_memory": {
      "description": "Store and retrieve relevant past interactions",
      "use_when": "Long-term memory with semantic retrieval",
      "implementation": "VectorStore with custom memory retrieval",
      "code_example": "from langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.documents import Document\nfrom langchain_core.runnables import RunnableLambda\n\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma(embedding_function=embeddings)\nretriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n\n# Store conversation as documents\nconversation_doc = Document(\n    page_content='User: Hello\\nAssistant: Hi there!',\n    metadata={'timestamp': '2024-01-01', 'session_id': 'abc123'}\n)\nvectorstore.add_documents([conversation_doc])\n\n# Retrieve relevant past conversations\nrelevant_memories = retriever.invoke('user asked about weather')",      "best_practices": [
        "Choose appropriate embedding model",
        "Set retrieval k based on context window",
        "Consider memory decay strategies",
        "Use metadata filters for better retrieval",
        "Use langchain_community for vector stores in LangChain 1.x"      ]
    },
    "entity_memory": {
      "description": "Track entities and their attributes across conversation",
      "use_when": "Need to remember facts about entities",
      "code_example": "from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom pydantic import BaseModel\nfrom typing import Dict\n\nllm = ChatOpenAI(model='gpt-4')\n\nclass EntityMemory(BaseModel):\n    entities: Dict[str, Dict[str, str]] = {}\n\n# Use structured output to extract entities\nentity_extraction_prompt = ChatPromptTemplate.from_template(\n    'Extract entities from: {message}'\n)\n\n# Implement custom entity tracking using structured outputs",
      "best_practices": [
        "Useful for maintaining entity context",
        "Works well with structured information",
        "Use structured outputs for entity extraction in LangChain 1.x"      ]
    }
  },
  "rag_patterns": {
    "basic_rag": {
      "description": "Retrieve relevant documents, augment prompt",
      "use_when": "Need to ground responses in source documents",
      "components": ["Document loader", "Text splitter", "Embeddings", "Vector store", "Retriever"],
      "code_example": "from langchain_community.document_loaders import TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\nloader = TextLoader('documents.txt')\ndocuments = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\nretriever = vectorstore.as_retriever()\n\nllm = ChatOpenAI(model='gpt-4')\nprompt = ChatPromptTemplate.from_template(\n    'Answer the question based on context:\\n\\n{context}\\n\\nQuestion: {question}'\n)\n\nqa_chain = (\n    {'context': retriever, 'question': RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\nresult = qa_chain.invoke('What is the main topic?')",      "best_practices": [
        "Chunk documents appropriately (500-1000 tokens)",
        "Use overlap for context preservation",
        "Experiment with different retrieval strategies",
        "Include source citations in responses"
      ]
    },
    "self_query_rag": {
      "description": "LLM generates query filters automatically",
      "use_when": "Documents have structured metadata",
      "implementation": "SelfQueryRetriever",
      "code_example": "from langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model='gpt-4')\n\nmetadata_field_info = [\n    AttributeInfo(\n        name='source',\n        description='The source document',\n        type='string'\n    ),\n    AttributeInfo(\n        name='date',\n        description='Publication date',\n        type='date'\n    )\n]\n\nretriever = SelfQueryRetriever.from_llm(\n    llm=llm,\n    vectorstore=vectorstore,\n    document_contents='Document content',\n    metadata_field_info=metadata_field_info\n)",      "best_practices": [
        "Define clear metadata schema",
        "Provide examples in the prompt",
        "Test query parsing accuracy"
      ]
    },
    "multi_query_rag": {
      "description": "Generate multiple queries for better retrieval",
      "use_when": "User queries are ambiguous or complex",
      "implementation": "MultiQueryRetriever",
      "code_example": "from langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model='gpt-4')\nretriever = MultiQueryRetriever.from_llm(\n    retriever=vectorstore.as_retriever(),\n    llm=llm\n)\n\n# Automatically generates multiple queries and deduplicates results",      "best_practices": [
        "Limit to 3-5 generated queries",
        "Deduplicate retrieved documents",
        "Consider query quality over quantity"
      ]
    },
    "parent_document_rag": {
      "description": "Retrieve small chunks, return parent documents",
      "use_when": "Need context around matched content",
      "implementation": "ParentDocumentRetriever",
      "code_example": "from langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter\n)\nretriever.add_documents(documents)",      "best_practices": [
        "Balance chunk size for retrieval vs context",
        "Use smaller chunks for retrieval, larger for context"
      ]
    },
    "compression_rag": {
      "description": "Compress retrieved documents to reduce token usage",
      "use_when": "Retrieving many documents but need to save tokens",
      "code_example": "from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\ncompressor = LLMChainExtractor.from_llm(llm)\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=retriever\n)",
      "best_practices": [
        "Use when retrieving many documents",
        "Consider cost of compression vs token savings"
      ]
    },
    "ensemble_rag": {
      "description": "Combine multiple retrieval strategies",
      "use_when": "Want to leverage different retrieval approaches",
      "code_example": "from langchain.retrievers import EnsembleRetriever\nfrom langchain.retrievers import BM25Retriever\n\nbm25_retriever = BM25Retriever.from_documents(documents)\nbm25_retriever.k = 2\n\nensemble_retriever = EnsembleRetriever(\n    retrievers=[vector_retriever, bm25_retriever],\n    weights=[0.5, 0.5]\n)",
      "best_practices": [
        "Tune weights based on performance",
        "Use complementary retrieval methods"
      ]
    }
  },
  "tool_patterns": {
    "structured_tool": {
      "description": "Tool with Pydantic input schema",
      "use_when": "Tool needs validated, structured inputs",
      "implementation": "@tool decorator with Pydantic model",
      "code_example": "from langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass SearchInput(BaseModel):\n    query: str = Field(description='Search query')\n    max_results: int = Field(default=5, ge=1, le=20)\n    filters: List[str] = Field(default=[], description='Search filters')\n\n@tool(args_schema=SearchInput)\ndef search_documents(query: str, max_results: int, filters: List[str]) -> str:\n    '''Search for documents matching the query.'''\n    results = perform_search(query, max_results, filters)\n    return json.dumps(results, indent=2)",
      "best_practices": [
        "Use Field descriptions for better tool understanding",
        "Add validation constraints (ge, le, etc.)",
        "Return structured outputs when possible"
      ]
    },
    "tool_with_error_handling": {
      "description": "Tool that handles errors gracefully",
      "use_when": "Tool can fail and agent should recover",
      "code_example": "from langchain_core.tools import tool\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@tool\ndef safe_api_call(url: str) -> str:\n    '''Make a safe API call with error handling.'''\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        return response.text\n    except requests.exceptions.Timeout:\n        error_msg = f'Request to {url} timed out'\n        logger.error(error_msg)\n        return f'Error: {error_msg}'\n    except requests.exceptions.RequestException as e:\n        error_msg = f'Request failed: {str(e)}'\n        logger.error(error_msg)\n        return f'Error: {error_msg}'",
      "best_practices": [
        "Return error messages, don't raise exceptions",
        "Provide actionable error information",
        "Consider retry logic for transient failures",
        "Log errors for debugging"
      ]
    },
    "async_tool": {
      "description": "Asynchronous tool for I/O operations",
      "use_when": "Tool performs network/disk I/O",
      "implementation": "async def implementation with @tool decorator",
      "code_example": "from langchain_core.tools import tool\nimport aiohttp\n\n@tool\nasync def async_fetch(url: str) -> str:\n    '''Fetch content from URL asynchronously.'''\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:\n            return await response.text()",
      "best_practices": [
        "Use async for I/O-bound operations",
        "Implement proper timeout handling",
        "Consider rate limiting",
        "Use connection pooling for multiple requests"
      ]
    },
    "dynamic_tool_loading": {
      "description": "Load tools dynamically based on context",
      "use_when": "Different tools needed for different scenarios",
      "code_example": "from langchain_core.tools import Tool\nfrom typing import List\n\ndef get_tools_for_context(context: str) -> List[Tool]:\n    base_tools = [search_tool, calculator_tool]\n    if context == 'code':\n        base_tools.extend([code_search_tool, syntax_check_tool])\n    elif context == 'data':\n        base_tools.extend([data_analysis_tool, visualization_tool])\n    return base_tools",      "best_practices": [
        "Cache tool definitions when possible",
        "Document tool selection logic",
        "Test with different contexts"
      ]
      "code_example": "from pydantic import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\nfrom typing import List, Literal\n\nclass AnalysisResult(BaseModel):\n    summary: str = Field(description='Brief summary')\n    key_points: List[str] = Field(description='Main points')\n    sentiment: Literal['positive', 'negative', 'neutral'] = Field(description='Overall sentiment')\n    confidence: float = Field(description='Confidence score', ge=0.0, le=1.0)\n\nllm = ChatOpenAI(model='gpt-4')\nstructured_llm = llm.with_structured_output(AnalysisResult)\n\nresult = structured_llm.invoke('Analyze this text: ...')\n# Returns AnalysisResult instance with validated fields\nprint(result.summary)\nprint(result.sentiment)",
      "best_practices": [
        "Use Field descriptions for better model understanding",
        "Add validation constraints",
        "Use Literal types for enums (preferred over pattern in LangChain 1.x)",
        "Handle parsing errors gracefully",
        "with_structured_output() is the recommended method in LangChain 1.x"
      ]
    },
    "structured_output_with_method": {
      "description": "Structured output with method specification",
      "use_when": "Need to specify JSON Schema or Pydantic mode explicitly",
      "code_example": "from pydantic import BaseModel\nfrom langchain_openai import ChatOpenAI\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nllm = ChatOpenAI(model='gpt-4')\n\n# Use method='pydantic' for Pydantic models (default)\nstructured_llm = llm.with_structured_output(Person, method='pydantic')\n\n# Use method='json_schema' for JSON Schema mode\nstructured_llm_json = llm.with_structured_output(Person, method='json_schema')\n\nresult = structured_llm.invoke('Extract person info: John, 30')",
      "best_practices": [
        "method='pydantic' is default and recommended",
        "method='json_schema' for models without Pydantic support",
        "Use include_raw for debugging"      ]
    },
    "json_output": {
      "description": "Output JSON with schema validation",
      "use_when": "Need JSON output but want schema validation",
      "code_example": "from langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\n\njson_parser = JsonOutputParser(pydantic_object=AnalysisResult)\n\nprompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a helpful assistant. Output valid JSON.'),\n    ('user', '{input}')\n])\n\nchain = prompt | llm | json_parser",
      "best_practices": [
        "Use JsonOutputParser for JSON responses",
        "Provide schema in prompt for better results",
        "Handle JSON parsing errors"
      ]
    }
  },
  "prompt_patterns": {
    "chat_prompt_template": {
      "description": "Structured prompt templates for chat models",
      "use_when": "Need consistent prompt structure",
      "code_example": "from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a helpful assistant specialized in {domain}.'),\n    ('human', '{user_input}')\n])\n\nformatted = prompt.format_messages(domain='software engineering', user_input='Explain OOP')\n\n# Use with LLM\nllm = ChatOpenAI(model='gpt-4')\nchain = prompt | llm\nresult = chain.invoke({'domain': 'software engineering', 'user_input': 'Explain OOP'})",      "best_practices": [
        "Separate system and user messages",
        "Use template variables for dynamic content",
        "Keep system prompts focused and clear"
      ]
    },
    "few_shot_prompting": {
      "description": "Include examples in prompts",
      "use_when": "Need to guide model behavior with examples",
      "code_example": "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nexamples = [\n    {'input': 'happy', 'output': 'positive'},\n    {'input': 'sad', 'output': 'negative'}\n]\n\nexample_prompt = ChatPromptTemplate.from_messages([\n    ('human', '{input}'),\n    ('ai', '{output}')\n])\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    example_prompt=example_prompt,\n    examples=examples\n)\n\nfinal_prompt = ChatPromptTemplate.from_messages([\n    ('system', 'Classify sentiment.'),\n    few_shot_prompt,\n    ('human', '{input}')\n])\n\n# Use with LLM\nllm = ChatOpenAI(model='gpt-4')\nchain = final_prompt | llm\nresult = chain.invoke({'input': 'excited'})",      "best_practices": [
        "Use diverse, representative examples",
        "Keep examples concise",
        "Order examples logically"
      ]
    },
    "prompt_partials": {
      "description": "Partially format prompts with some variables",
      "use_when": "Some variables known at construction time",
      "code_example": "from langchain_core.prompts import ChatPromptTemplate\n\nbase_prompt = ChatPromptTemplate.from_template(\n    'You are a {role}. Answer questions about {domain}.'\n)\n\n# Partial with role filled\nspecialized_prompt = base_prompt.partial(role='software engineer')\n\n# Later fill domain\nfinal = specialized_prompt.format(domain='Python')",
      "best_practices": [
        "Use partials for reusable prompt components",
        "Document which variables are partial vs runtime"
      ]
    }
  },
  "document_processing": {
    "document_loaders": {
      "description": "Load documents from various sources",
      "types": {
        "text": "TextLoader for .txt files",
        "pdf": "PyPDFLoader for PDF files",
        "web": "WebBaseLoader for web pages",
        "csv": "CSVLoader for CSV files",
        "json": "JSONLoader for JSON files"
      },
      "code_example": "from langchain_community.document_loaders import (\n    TextLoader,\n    PyPDFLoader,\n    WebBaseLoader,\n    CSVLoader\n)\n\n# Text file\nloader = TextLoader('document.txt', encoding='utf-8')\ndocs = loader.load()\n\n# PDF\nloader = PyPDFLoader('document.pdf')\ndocs = loader.load()\n\n# Web page\nloader = WebBaseLoader(['https://example.com'])\ndocs = loader.load()\n\n# CSV\nloader = CSVLoader('data.csv')\ndocs = loader.load()\n\n# Note: All loaders are in langchain_community in LangChain 1.x",      "best_practices": [
        "Handle encoding issues for text files",
        "Use appropriate loader for file type",
        "Handle loading errors gracefully"
      ]
    },
    "text_splitters": {
      "description": "Split documents into chunks",
      "types": {
        "recursive_character": "RecursiveCharacterTextSplitter - general purpose",
        "token": "TokenTextSplitter - split by tokens",
        "markdown": "MarkdownTextSplitter - preserve markdown structure",
        "python": "PythonCodeTextSplitter - preserve code structure"
      },
      "code_example": "from langchain_text_splitters import (\n    RecursiveCharacterTextSplitter,\n    TokenTextSplitter,\n    MarkdownHeaderTextSplitter\n)\n\n# Recursive character splitter (most common)\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    separators=['\\n\\n', '\\n', ' ', '']\n)\nsplits = splitter.split_documents(documents)\n\n# Token-based splitter\ntoken_splitter = TokenTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    encoding_name='cl100k_base'\n)\n\n# Markdown splitter\nmarkdown_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on=[\n        ('#', 'Header 1'),\n        ('##', 'Header 2'),\n        ('###', 'Header 3')\n    ]\n)",      "best_practices": [
        "Use chunk_size 500-1000 tokens typically",
        "Set chunk_overlap 10-20% of chunk_size",
        "Choose splitter based on document type",
        "Test chunk quality for your use case"
      ]
    }
  },
  "vector_store_patterns": {
    "chroma": {
      "description": "Chroma vector store for embeddings",
      "code_example": "from langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma(\n    persist_directory='./chroma_db',\n    embedding_function=embeddings\n)\n\n# Add documents\nvectorstore.add_documents(documents)\n\n# Similarity search\nresults = vectorstore.similarity_search('query', k=5)\n\n# Similarity search with score\nresults = vectorstore.similarity_search_with_score('query', k=5)",
      "best_practices": [
        "Use persist_directory for production",
        "Choose appropriate embedding model",
        "Set k based on context window"
      ]
    },
    "faiss": {
      "description": "FAISS vector store (in-memory or file-based)",
      "code_example": "from langchain_community.vectorstores import FAISS\n\nvectorstore = FAISS.from_documents(documents, embeddings)\n\n# Save to disk\nvectorstore.save_local('./faiss_index')\n\n# Load from disk\nvectorstore = FAISS.load_local('./faiss_index', embeddings)",
      "best_practices": [
        "Use for in-memory or file-based storage",
        "Good for development and testing",
        "Consider Chroma or Pinecone for production"
      ]
    },
    "pinecone": {
      "description": "Pinecone managed vector database",
      "code_example": "from langchain_community.vectorstores import Pinecone\nimport pinecone\n\npinecone.init(api_key='your-key', environment='us-east-1')\n\nindex = pinecone.Index('my-index')\nvectorstore = Pinecone(index, embeddings.embed_query, 'text')\n\nvectorstore.add_documents(documents)",
      "best_practices": [
        "Use for production scale",
        "Handle API rate limits",
        "Monitor costs"
      ]
    },
    "metadata_filtering": {
      "description": "Filter retrieval by metadata",
      "code_example": "from langchain_community.vectorstores import Chroma\n\n# Add documents with metadata\nvectorstore.add_documents(\n    documents,\n    metadatas=[{'source': 'doc1', 'date': '2024-01-01'} for _ in documents]\n)\n\n# Filter by metadata\nresults = vectorstore.similarity_search(\n    'query',\n    k=5,\n    filter={'source': 'doc1'}\n)",
      "best_practices": [
        "Include relevant metadata when indexing",
        "Use filters to narrow search space",
        "Index metadata fields for efficient filtering"
      ]
    }
  },
  "callbacks_and_tracing": {
    "langsmith_tracing": {
      "description": "Trace agent execution with LangSmith",
      "code_example": "import os\n\n# In LangChain 1.x, tracing is automatic when env vars are set\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_API_KEY'] = 'your-key'\nos.environ['LANGCHAIN_PROJECT'] = 'my-project'\n\n# All chains automatically traced\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\nllm = ChatOpenAI(model='gpt-4')\nprompt = ChatPromptTemplate.from_template('Say hello to {name}')\nchain = prompt | llm\n\n# This invocation will be automatically traced\nchain.invoke({'name': 'Alice'})",
      "best_practices": [
        "Set LANGCHAIN_PROJECT for organization",
        "Add metadata to traces using config",
        "Use for debugging and optimization",
        "Tracing is automatic in LangChain 1.x - no need to import LangChainTracer"      ]
    },
    "custom_callbacks": {
      "description": "Create custom callbacks for monitoring",
      "code_example": "from langchain_core.callbacks import BaseCallbackHandler\nfrom langchain_core.outputs import LLMResult\n\nclass CustomCallbackHandler(BaseCallbackHandler):\n    def on_llm_start(self, serialized, prompts, **kwargs):\n        print(f'LLM started with prompts: {prompts}')\n    \n    def on_llm_end(self, response: LLMResult, **kwargs):\n        print(f'LLM ended with response: {response}')\n    \n    def on_llm_error(self, error, **kwargs):\n        print(f'LLM error: {error}')\n\nhandler = CustomCallbackHandler()\nchain.invoke({'input': 'test'}, config={'callbacks': [handler]})",
      "best_practices": [
        "Implement only needed callback methods",
        "Keep callbacks lightweight",
        "Use for logging and monitoring"
      ]
    },
    "streaming": {
      "description": "Stream LLM responses token by token",
      "code_example": "from langchain_core.callbacks import StreamingStdOutCallbackHandler\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model='gpt-4',\n    streaming=True,\n    callbacks=[StreamingStdOutCallbackHandler()]\n)\n\nfor chunk in llm.stream('Tell me a story'):\n    print(chunk.content, end='', flush=True)",      "best_practices": [
        "Use streaming for better UX",
        "Handle streaming errors gracefully",
        "Consider rate limiting for streaming"
      ]
      "code_example": "from langchain.agents import create_react_agent, AgentExecutor\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef get_weather(location: str) -> str:\n    '''Get the current weather for a location.'''\n    return f'Weather in {location}: Sunny, 72Â°F'\n\ntools = [get_weather]\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\nprompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a helpful assistant with access to tools.'),\n    ('placeholder', '{chat_history}'),\n    ('human', '{input}'),\n    ('placeholder', '{agent_scratchpad}')\n])\n\nagent = create_react_agent(llm, tools, prompt)\nexecutor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    max_iterations=15,\n    handle_parsing_errors=True\n)\nresult = executor.invoke({'input': 'What is the weather in San Francisco?'})",      "best_practices": [
        "Keep tool descriptions clear and specific",
        "Use structured outputs for consistent parsing",
        "Implement proper error handling for tool failures",
        "Add observability with LangSmith tracing",
        "Set max_iterations to prevent infinite loops"
      ]
    },
    "tool_calling_agent": {
      "description": "Modern tool-calling agent using native LLM tool use",
      "use_when": "Using models with native tool calling (GPT-4, Claude)",
      "implementation": {
        "framework": "langchain.agents.create_tool_calling_agent",
        "components": ["ChatModel with tool support", "Tools", "Prompt"]
      },
      "code_example": "from langchain.agents import create_tool_calling_agent, AgentExecutor\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef calculator(expression: str) -> str:\n    '''Evaluate a mathematical expression.'''\n    return str(eval(expression))\n\ntools = [calculator]\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)\nprompt = ChatPromptTemplate.from_messages([\n    ('system', 'You are a helpful assistant.'),\n    ('human', '{input}')\n])\n\nagent = create_tool_calling_agent(llm, tools, prompt)\nexecutor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    max_iterations=10\n)\nresult = executor.invoke({'input': 'What is 15 * 23?'})",      "best_practices": [
        "Prefer this over ReAct for models with native tool calling",
        "Use Pydantic models for tool arguments",
        "Implement retry logic for transient failures"
      ]
    },
    "structured_output_agent": {
      "description": "Agent that always produces structured output",
      "use_when": "Need guaranteed output format for downstream processing",
      "implementation": {
        "framework": "llm.with_structured_output(OutputModel)",
        "components": ["ChatModel", "Pydantic output model"]
      },
      "code_example": "from pydantic import BaseModel\n\nclass AgentResponse(BaseModel):\n    reasoning: str\n    action: str\n    confidence: float\n\nstructured_llm = llm.with_structured_output(AgentResponse)\nresult = structured_llm.invoke('Analyze this situation...')",
      "best_practices": [
        "Define clear Pydantic models with field descriptions",
        "Use Literal types for enum-like fields",
        "Add validation in the Pydantic model"
      ]
    }
  },
  "anti_patterns": {
    "god_agent": {
      "description": "Single agent trying to do everything",
      "problem": "Hard to debug, maintain, and improve",
      "solution": "Decompose into specialized agents with clear responsibilities"
    },
    "prompt_injection_vulnerability": {
      "description": "Allowing untrusted input directly in prompts",
      "problem": "Security vulnerability, unexpected behavior",
      "solution": "Sanitize inputs, use separate user/system message sections"
    },
    "unbounded_loops": {
      "description": "Agent without iteration limits",
      "problem": "Infinite loops, runaway costs",
      "solution": "Set max_iterations, implement timeout, add recursion limits"
    },
    "silent_failures": {
      "description": "Swallowing errors without logging",
      "problem": "Violates A3 (Transparency), hard to debug",
      "solution": "Log all errors, return informative error messages"
    },
    "ignoring_token_limits": {
      "description": "Not managing context window size",
      "problem": "Token limit errors, high costs",
      "solution": "Use summarization, chunking, and windowed memory"
    }
  },
  "testing_strategies": {
    "unit_testing": {
      "description": "Test individual components in isolation",
      "tools": ["pytest", "unittest.mock"],
      "code_example": "from unittest.mock import Mock, patch\nimport pytest\nfrom langchain_core.runnables import RunnableLambda\n\ndef test_chain_step():\n    mock_llm = Mock()\n    mock_llm.invoke.return_value.content = 'test response'\n    \n    chain = RunnableLambda(lambda x: x) | mock_llm\n    result = chain.invoke({'input': 'test'})\n    \n    assert result.content == 'test response'",
      "best_practices": [
        "Mock LLM calls for deterministic tests",
        "Test tool implementations independently",
        "Verify prompt templates render correctly"
      ]
    },
    "integration_testing": {
      "description": "Test agent end-to-end with real LLM",
      "tools": ["pytest", "LangSmith"],
      "code_example": "def test_agent_integration():\n    llm = ChatOpenAI(model='gpt-4', temperature=0)\n    agent = create_react_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools)\n    \n    result = executor.invoke({'input': 'What is 2+2?'})\n    assert '4' in result['output'].lower()",
      "best_practices": [
        "Use low-temperature for reproducibility",
        "Test with representative input scenarios",
        "Verify tool calling works correctly"
      ]
    },
    "evaluation": {
      "description": "Measure agent quality systematically",
      "tools": ["LangSmith Evaluators", "Custom metrics"],
      "code_example": "from langchain.evaluation import EvaluatorType\nfrom langchain.evaluation import load_evaluator\n\nevaluator = load_evaluator(EvaluatorType.QA)\n\nresult = evaluator.evaluate(\n    examples=[{'question': 'What is 2+2?', 'answer': '4'}],\n    prediction='The answer is 4',\n    reference='4'\n)",
      "metrics": [
        "Task completion rate",
        "Factual accuracy",
        "Tool usage efficiency",
        "Response quality"
      ]
    }
  },
  "observability": {
    "langsmith": {
      "description": "LangChain's observability platform",
      "features": ["Tracing", "Debugging", "Evaluation", "Monitoring"],
      "setup": "export LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_API_KEY=your_key\nexport LANGCHAIN_PROJECT=my-project",
      "code_example": "import os\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_API_KEY'] = 'your-key'\n\n# All chains automatically traced\nchain.invoke({'input': 'test'})",
      "best_practices": [
        "Enable tracing in all environments",
        "Add metadata to traces for filtering",
        "Set up alerts for anomalies",
        "Use LANGCHAIN_PROJECT for organization"
      ]
    },
    "logging": {
      "description": "Structured logging for agents",
      "code_example": "import logging\nfrom langchain_core.callbacks import BaseCallbackHandler\n\nlogger = logging.getLogger(__name__)\n\nclass LoggingCallbackHandler(BaseCallbackHandler):\n    def on_chain_start(self, serialized, inputs, **kwargs):\n        logger.info(f'Chain started: {serialized.get(\"name\")}')\n    \n    def on_chain_end(self, outputs, **kwargs):\n        logger.info(f'Chain ended with outputs: {outputs}')\n    \n    def on_chain_error(self, error, **kwargs):\n        logger.error(f'Chain error: {error}')",
      "best_practices": [
        "Log at appropriate levels (DEBUG for traces, INFO for actions)",
        "Include correlation IDs for request tracking",
        "Avoid logging sensitive information",
        "Use structured logging format"
      ]
    }
  }
}
