{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Data Pipeline Patterns",
  "description": "Patterns for ETL, data transformation, and automated reporting",
  "version": "1.0.0",
  "patterns": {
    "dataIngestion": {
      "description": "Data acquisition and loading patterns",
      "multiSourceFetcher": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\nimport asyncio\n\nclass DataSource(ABC):\n    @abstractmethod\n    async def fetch(self, **kwargs) -> List[Dict[str, Any]]:\n        pass\n    \n    @abstractmethod\n    def validate(self, data: List[Dict]) -> bool:\n        pass\n\nclass APIDataSource(DataSource):\n    def __init__(self, base_url: str, api_key: str):\n        self.base_url = base_url\n        self.api_key = api_key\n    \n    async def fetch(self, endpoint: str, params: dict = None) -> List[Dict]:\n        import httpx\n        async with httpx.AsyncClient() as client:\n            response = await client.get(\n                f'{self.base_url}/{endpoint}',\n                headers={'Authorization': f'Bearer {self.api_key}'},\n                params=params\n            )\n            response.raise_for_status()\n            return response.json()\n    \n    def validate(self, data: List[Dict]) -> bool:\n        return len(data) > 0\n\nclass DatabaseDataSource(DataSource):\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n    \n    async def fetch(self, query: str, params: dict = None) -> List[Dict]:\n        import asyncpg\n        conn = await asyncpg.connect(self.connection_string)\n        rows = await conn.fetch(query, *params.values() if params else [])\n        await conn.close()\n        return [dict(row) for row in rows]\n    \n    def validate(self, data: List[Dict]) -> bool:\n        return True\n\nclass FileDataSource(DataSource):\n    async def fetch(self, file_path: str) -> List[Dict]:\n        import pandas as pd\n        if file_path.endswith('.csv'):\n            df = pd.read_csv(file_path)\n        elif file_path.endswith('.parquet'):\n            df = pd.read_parquet(file_path)\n        elif file_path.endswith('.json'):\n            df = pd.read_json(file_path)\n        else:\n            raise ValueError(f'Unsupported file format: {file_path}')\n        return df.to_dict('records')\n    \n    def validate(self, data: List[Dict]) -> bool:\n        return len(data) > 0",
      "incrementalLoad": "from datetime import datetime, timedelta\nfrom typing import Optional\n\nclass IncrementalLoader:\n    def __init__(self, source: DataSource, state_store: 'StateStore'):\n        self.source = source\n        self.state_store = state_store\n    \n    async def load(self, source_id: str, timestamp_column: str = 'updated_at') -> List[Dict]:\n        last_sync = await self.state_store.get_last_sync(source_id)\n        \n        if last_sync:\n            data = await self.source.fetch(\n                filters={timestamp_column: {'$gt': last_sync}}\n            )\n        else:\n            data = await self.source.fetch()\n        \n        if data:\n            max_timestamp = max(row[timestamp_column] for row in data)\n            await self.state_store.set_last_sync(source_id, max_timestamp)\n        \n        return data\n\nclass StateStore:\n    def __init__(self, redis_url: str):\n        import redis.asyncio as redis\n        self.redis = redis.from_url(redis_url)\n    \n    async def get_last_sync(self, source_id: str) -> Optional[datetime]:\n        value = await self.redis.get(f'sync:{source_id}')\n        return datetime.fromisoformat(value.decode()) if value else None\n    \n    async def set_last_sync(self, source_id: str, timestamp: datetime):\n        await self.redis.set(f'sync:{source_id}', timestamp.isoformat())"
    },
    "transformation": {
      "description": "Data transformation patterns",
      "pandasTransformer": "import pandas as pd\nfrom typing import Callable, List\n\nclass DataTransformer:\n    def __init__(self):\n        self.steps: List[Callable[[pd.DataFrame], pd.DataFrame]] = []\n    \n    def add_step(self, func: Callable[[pd.DataFrame], pd.DataFrame]):\n        self.steps.append(func)\n        return self\n    \n    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n        for step in self.steps:\n            df = step(df)\n        return df\n\n# Common transformation functions\ndef clean_nulls(df: pd.DataFrame) -> pd.DataFrame:\n    return df.dropna()\n\ndef normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n    return df\n\ndef add_timestamp(df: pd.DataFrame) -> pd.DataFrame:\n    df['processed_at'] = pd.Timestamp.now()\n    return df\n\ndef deduplicate(df: pd.DataFrame, subset: List[str] = None) -> pd.DataFrame:\n    return df.drop_duplicates(subset=subset)\n\n# Usage\ntransformer = DataTransformer()\ntransformer.add_step(normalize_columns)\ntransformer.add_step(clean_nulls)\ntransformer.add_step(deduplicate)\ntransformer.add_step(add_timestamp)\n\nresult = transformer.transform(raw_df)",
      "polarsTransformer": "import polars as pl\nfrom typing import List\n\ndef transform_with_polars(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"High-performance transformation with Polars.\"\"\"\n    return (\n        df\n        .with_columns([\n            pl.col('*').name.map(lambda x: x.lower().replace(' ', '_'))\n        ])\n        .filter(pl.all_horizontal(pl.all().is_not_null()))\n        .unique()\n        .with_columns([\n            pl.lit(pl.datetime('now')).alias('processed_at')\n        ])\n    )\n\n# Lazy evaluation for large datasets\ndef transform_lazy(file_path: str) -> pl.DataFrame:\n    return (\n        pl.scan_parquet(file_path)\n        .filter(pl.col('amount') > 0)\n        .group_by('category')\n        .agg([\n            pl.col('amount').sum().alias('total'),\n            pl.col('amount').mean().alias('average'),\n            pl.count().alias('count')\n        ])\n        .collect()\n    )",
      "validationSchema": "from pydantic import BaseModel, Field, validator\nfrom typing import List, Optional\nfrom datetime import datetime\n\nclass DataRecord(BaseModel):\n    id: str\n    name: str = Field(min_length=1, max_length=100)\n    amount: float = Field(ge=0)\n    category: str\n    created_at: datetime\n    email: Optional[str] = None\n    \n    @validator('email')\n    def validate_email(cls, v):\n        if v and '@' not in v:\n            raise ValueError('Invalid email format')\n        return v\n    \n    @validator('category')\n    def validate_category(cls, v):\n        allowed = ['A', 'B', 'C', 'D']\n        if v not in allowed:\n            raise ValueError(f'Category must be one of {allowed}')\n        return v\n\ndef validate_batch(records: List[dict]) -> tuple[List[DataRecord], List[dict]]:\n    \"\"\"Validate batch of records, returning valid and invalid.\"\"\"\n    valid = []\n    invalid = []\n    \n    for record in records:\n        try:\n            valid.append(DataRecord(**record))\n        except Exception as e:\n            invalid.append({'record': record, 'error': str(e)})\n    \n    return valid, invalid"
    },
    "etlPipeline": {
      "description": "Complete ETL pipeline patterns",
      "pipelineOrchestrator": "from dataclasses import dataclass\nfrom typing import List, Callable, Any\nfrom enum import Enum\nimport asyncio\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass PipelineStatus(str, Enum):\n    PENDING = 'pending'\n    RUNNING = 'running'\n    COMPLETED = 'completed'\n    FAILED = 'failed'\n\n@dataclass\nclass PipelineStep:\n    name: str\n    func: Callable\n    retry_count: int = 3\n    timeout: int = 300\n\n@dataclass\nclass PipelineResult:\n    step: str\n    status: PipelineStatus\n    data: Any = None\n    error: str = None\n    duration_ms: int = 0\n\nclass ETLPipeline:\n    def __init__(self, name: str):\n        self.name = name\n        self.steps: List[PipelineStep] = []\n        self.results: List[PipelineResult] = []\n    \n    def add_step(self, name: str, func: Callable, retry_count: int = 3):\n        self.steps.append(PipelineStep(name=name, func=func, retry_count=retry_count))\n        return self\n    \n    async def run(self, initial_data: Any = None) -> List[PipelineResult]:\n        data = initial_data\n        \n        for step in self.steps:\n            start_time = asyncio.get_event_loop().time()\n            \n            for attempt in range(step.retry_count):\n                try:\n                    logger.info(f'Running step: {step.name} (attempt {attempt + 1})')\n                    \n                    if asyncio.iscoroutinefunction(step.func):\n                        data = await asyncio.wait_for(\n                            step.func(data),\n                            timeout=step.timeout\n                        )\n                    else:\n                        data = step.func(data)\n                    \n                    duration = int((asyncio.get_event_loop().time() - start_time) * 1000)\n                    self.results.append(PipelineResult(\n                        step=step.name,\n                        status=PipelineStatus.COMPLETED,\n                        data=data,\n                        duration_ms=duration\n                    ))\n                    break\n                    \n                except Exception as e:\n                    if attempt == step.retry_count - 1:\n                        duration = int((asyncio.get_event_loop().time() - start_time) * 1000)\n                        self.results.append(PipelineResult(\n                            step=step.name,\n                            status=PipelineStatus.FAILED,\n                            error=str(e),\n                            duration_ms=duration\n                        ))\n                        raise\n                    await asyncio.sleep(2 ** attempt)  # Exponential backoff\n        \n        return self.results\n\n# Usage\npipeline = ETLPipeline('sales_etl')\npipeline.add_step('extract', extract_from_api)\npipeline.add_step('transform', transform_data)\npipeline.add_step('validate', validate_records)\npipeline.add_step('load', load_to_warehouse)\n\nresults = await pipeline.run()",
      "parallelPipeline": "import asyncio\nfrom typing import List, Dict, Callable\n\nclass ParallelETL:\n    def __init__(self):\n        self.sources: Dict[str, Callable] = {}\n        self.transformers: Dict[str, Callable] = {}\n    \n    def add_source(self, name: str, extractor: Callable):\n        self.sources[name] = extractor\n        return self\n    \n    def add_transformer(self, source: str, transformer: Callable):\n        self.transformers[source] = transformer\n        return self\n    \n    async def run(self) -> Dict[str, List]:\n        # Extract in parallel\n        extract_tasks = {\n            name: asyncio.create_task(extractor())\n            for name, extractor in self.sources.items()\n        }\n        \n        extracted = {}\n        for name, task in extract_tasks.items():\n            extracted[name] = await task\n        \n        # Transform in parallel\n        transform_tasks = {\n            name: asyncio.create_task(self.transformers[name](data))\n            for name, data in extracted.items()\n            if name in self.transformers\n        }\n        \n        transformed = {}\n        for name, task in transform_tasks.items():\n            transformed[name] = await task\n        \n        return transformed\n\n# Usage\netl = ParallelETL()\netl.add_source('orders', fetch_orders)\netl.add_source('customers', fetch_customers)\netl.add_source('products', fetch_products)\netl.add_transformer('orders', transform_orders)\netl.add_transformer('customers', transform_customers)\n\nresults = await etl.run()"
    },
    "dataQuality": {
      "description": "Data quality and validation patterns",
      "qualityChecks": "from dataclasses import dataclass\nfrom typing import List, Callable\nimport pandas as pd\n\n@dataclass\nclass QualityRule:\n    name: str\n    check: Callable[[pd.DataFrame], bool]\n    severity: str  # 'error', 'warning'\n    message: str\n\n@dataclass\nclass QualityReport:\n    passed: bool\n    rules_passed: List[str]\n    rules_failed: List[dict]\n    row_count: int\n    column_count: int\n\nclass DataQualityChecker:\n    def __init__(self):\n        self.rules: List[QualityRule] = []\n    \n    def add_rule(self, name: str, check: Callable, severity: str = 'error', message: str = ''):\n        self.rules.append(QualityRule(name, check, severity, message))\n        return self\n    \n    def check(self, df: pd.DataFrame) -> QualityReport:\n        passed = []\n        failed = []\n        \n        for rule in self.rules:\n            try:\n                if rule.check(df):\n                    passed.append(rule.name)\n                else:\n                    failed.append({\n                        'rule': rule.name,\n                        'severity': rule.severity,\n                        'message': rule.message\n                    })\n            except Exception as e:\n                failed.append({\n                    'rule': rule.name,\n                    'severity': 'error',\n                    'message': f'Check failed with error: {str(e)}'\n                })\n        \n        has_errors = any(f['severity'] == 'error' for f in failed)\n        \n        return QualityReport(\n            passed=not has_errors,\n            rules_passed=passed,\n            rules_failed=failed,\n            row_count=len(df),\n            column_count=len(df.columns)\n        )\n\n# Common quality checks\ndef no_nulls(column: str) -> Callable:\n    return lambda df: df[column].notna().all()\n\ndef unique_values(column: str) -> Callable:\n    return lambda df: df[column].is_unique\n\ndef in_range(column: str, min_val: float, max_val: float) -> Callable:\n    return lambda df: df[column].between(min_val, max_val).all()\n\ndef matches_pattern(column: str, pattern: str) -> Callable:\n    return lambda df: df[column].str.match(pattern).all()\n\n# Usage\nchecker = DataQualityChecker()\nchecker.add_rule('id_not_null', no_nulls('id'), 'error', 'ID column has null values')\nchecker.add_rule('id_unique', unique_values('id'), 'error', 'ID column has duplicates')\nchecker.add_rule('amount_positive', in_range('amount', 0, float('inf')), 'warning', 'Amount should be positive')\n\nreport = checker.check(df)"
    },
    "reporting": {
      "description": "Automated reporting patterns",
      "reportGenerator": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any\nimport pandas as pd\nfrom datetime import datetime\n\nclass ReportSection(ABC):\n    @abstractmethod\n    def render(self, data: pd.DataFrame) -> str:\n        pass\n\nclass SummarySection(ReportSection):\n    def __init__(self, title: str, metrics: List[Dict]):\n        self.title = title\n        self.metrics = metrics\n    \n    def render(self, data: pd.DataFrame) -> str:\n        lines = [f'## {self.title}\\n']\n        for metric in self.metrics:\n            column = metric['column']\n            agg = metric['aggregation']\n            value = getattr(data[column], agg)()\n            lines.append(f\"- **{metric['label']}**: {value:,.2f}\")\n        return '\\n'.join(lines)\n\nclass TableSection(ReportSection):\n    def __init__(self, title: str, columns: List[str], limit: int = 10):\n        self.title = title\n        self.columns = columns\n        self.limit = limit\n    \n    def render(self, data: pd.DataFrame) -> str:\n        subset = data[self.columns].head(self.limit)\n        return f'## {self.title}\\n\\n{subset.to_markdown(index=False)}'\n\nclass ReportGenerator:\n    def __init__(self, title: str):\n        self.title = title\n        self.sections: List[ReportSection] = []\n    \n    def add_section(self, section: ReportSection):\n        self.sections.append(section)\n        return self\n    \n    def generate(self, data: pd.DataFrame) -> str:\n        lines = [\n            f'# {self.title}',\n            f'Generated: {datetime.now().isoformat()}\\n'\n        ]\n        \n        for section in self.sections:\n            lines.append(section.render(data))\n            lines.append('')\n        \n        return '\\n'.join(lines)\n\n# Usage\nreport = ReportGenerator('Monthly Sales Report')\nreport.add_section(SummarySection('Summary', [\n    {'column': 'revenue', 'aggregation': 'sum', 'label': 'Total Revenue'},\n    {'column': 'revenue', 'aggregation': 'mean', 'label': 'Average Order'},\n    {'column': 'orders', 'aggregation': 'count', 'label': 'Total Orders'}\n]))\nreport.add_section(TableSection('Top Products', ['product', 'revenue', 'quantity']))\n\nmarkdown = report.generate(sales_df)",
      "dashboardData": "from typing import Dict, List\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nclass DashboardDataBuilder:\n    def __init__(self, df: pd.DataFrame):\n        self.df = df\n    \n    def build_kpis(self, metrics: List[Dict]) -> Dict:\n        kpis = {}\n        for metric in metrics:\n            kpis[metric['name']] = {\n                'value': getattr(self.df[metric['column']], metric['agg'])(),\n                'previous': self._get_previous_period_value(metric)\n            }\n        return kpis\n    \n    def build_time_series(self, date_column: str, value_column: str, freq: str = 'D') -> List[Dict]:\n        grouped = self.df.groupby(pd.Grouper(key=date_column, freq=freq))[value_column].sum()\n        return [{'date': str(k), 'value': v} for k, v in grouped.items()]\n    \n    def build_breakdown(self, category_column: str, value_column: str) -> List[Dict]:\n        grouped = self.df.groupby(category_column)[value_column].sum().sort_values(ascending=False)\n        return [{'category': k, 'value': v} for k, v in grouped.items()]\n    \n    def _get_previous_period_value(self, metric: Dict) -> float:\n        # Implementation for previous period comparison\n        return 0.0\n\n# Usage\nbuilder = DashboardDataBuilder(sales_df)\ndashboard_data = {\n    'kpis': builder.build_kpis([\n        {'name': 'revenue', 'column': 'amount', 'agg': 'sum'},\n        {'name': 'orders', 'column': 'id', 'agg': 'count'}\n    ]),\n    'revenue_trend': builder.build_time_series('date', 'amount'),\n    'category_breakdown': builder.build_breakdown('category', 'amount')\n}"
    },
    "dataWarehouse": {
      "description": "Data warehouse loading patterns",
      "upsert": "import asyncpg\nfrom typing import List, Dict\n\nasync def upsert_batch(\n    conn: asyncpg.Connection,\n    table: str,\n    records: List[Dict],\n    key_columns: List[str],\n    update_columns: List[str]\n):\n    \"\"\"Upsert records using PostgreSQL ON CONFLICT.\"\"\"\n    if not records:\n        return\n    \n    columns = list(records[0].keys())\n    values_placeholder = ', '.join(f'${i+1}' for i in range(len(columns)))\n    \n    conflict_clause = ', '.join(key_columns)\n    update_clause = ', '.join(f'{col} = EXCLUDED.{col}' for col in update_columns)\n    \n    query = f'''\n        INSERT INTO {table} ({', '.join(columns)})\n        VALUES ({values_placeholder})\n        ON CONFLICT ({conflict_clause})\n        DO UPDATE SET {update_clause}\n    '''\n    \n    await conn.executemany(query, [tuple(r.values()) for r in records])",
      "duckdbAnalytics": "import duckdb\n\nclass AnalyticsEngine:\n    def __init__(self, db_path: str = ':memory:'):\n        self.conn = duckdb.connect(db_path)\n    \n    def load_parquet(self, path: str, table_name: str):\n        self.conn.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM '{path}'\")\n    \n    def query(self, sql: str):\n        return self.conn.execute(sql).fetchdf()\n    \n    def aggregate(self, table: str, group_by: List[str], metrics: Dict[str, str]):\n        metric_sql = ', '.join(f'{agg}({col}) as {col}_{agg}' for col, agg in metrics.items())\n        group_sql = ', '.join(group_by)\n        return self.query(f'SELECT {group_sql}, {metric_sql} FROM {table} GROUP BY {group_sql}')\n\n# Usage\nengine = AnalyticsEngine()\nengine.load_parquet('data/sales.parquet', 'sales')\nresult = engine.aggregate('sales', ['category', 'region'], {'amount': 'SUM', 'quantity': 'AVG'})"
    }
  },
  "langchainIntegration": {
    "dataFetcherTool": "from langchain_core.tools import tool\n\n@tool\nasync def fetch_data(source: str, query: str) -> str:\n    \"\"\"Fetch data from a specified source.\n    \n    Args:\n        source: Data source name (e.g., 'database', 'api', 'file')\n        query: Query or path to fetch data\n    \n    Returns:\n        JSON string of fetched data\n    \"\"\"\n    pass\n\n@tool\ndef run_etl_pipeline(pipeline_name: str, params: dict) -> str:\n    \"\"\"Run an ETL pipeline.\n    \n    Args:\n        pipeline_name: Name of the pipeline to run\n        params: Pipeline parameters\n    \n    Returns:\n        Pipeline execution result\n    \"\"\"\n    pass\n\n@tool\ndef generate_report(report_type: str, date_range: str) -> str:\n    \"\"\"Generate a data report.\n    \n    Args:\n        report_type: Type of report (e.g., 'sales', 'inventory', 'performance')\n        date_range: Date range for the report (e.g., 'last_7_days', 'last_month')\n    \n    Returns:\n        Generated report as markdown\n    \"\"\"\n    pass"
  },
  "bestPractices": [
    "Use incremental loading to process only new/changed data",
    "Implement data quality checks at each pipeline stage",
    "Use Polars for large datasets - significantly faster than pandas",
    "Validate data with Pydantic models before loading",
    "Implement idempotent pipelines for safe retries",
    "Use parallel processing for independent data sources",
    "Store pipeline state for recovery from failures",
    "Log all pipeline executions with metrics"
  ],
  "antiPatterns": [
    {"name": "Full reload every time", "problem": "Slow and resource-intensive", "solution": "Use incremental loading with change tracking"},
    {"name": "No validation", "problem": "Bad data propagates to warehouse", "solution": "Validate at extraction and before loading"},
    {"name": "Single-threaded extraction", "problem": "Slow for multiple sources", "solution": "Use asyncio for parallel extraction"},
    {"name": "No error handling", "problem": "Pipeline fails silently", "solution": "Implement proper error handling with alerts"}
  ],
  "documentation": {
    "pandas": "https://pandas.pydata.org/docs/",
    "polars": "https://pola.rs/",
    "duckdb": "https://duckdb.org/docs/",
    "asyncpg": "https://magicstack.github.io/asyncpg/"
  }
}
