{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# RAG Analysis & MCP Server Prototype\n",
    "\n",
    "This notebook serves two purposes:\n",
    "1. Validating the existing RAG pipeline (`scripts/ai/rag`) against the `ebook_library` Qdrant collection.\n",
    "2. Prototyping an MCP Server to expose this RAG functionality to Antigravity agents.\n",
    "3. Testing the running MCP Server (Client Mode) using LangChain's MultiServerMCPClient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install FastMCP and LangChain MCP Adapters\n",
    "# %pip install \"fastmcp>=3.0.0\" langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Ensure we can import from project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from scripts.ai.rag.rag_optimized import get_rag, OptimizedRAG\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9428b",
   "metadata": {},
   "source": [
    "## 1. Direct Qdrant Inspection\n",
    "First, let's verify we can connect to the Docker instance and that the collection exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6647bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/ebook_library/points/count \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Collections:\n",
      "- ebook_library\n",
      "\n",
      "Document Count in 'ebook_library': 15222\n"
     ]
    }
   ],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "collections = client.get_collections()\n",
    "\n",
    "print(\"Available Collections:\")\n",
    "for c in collections.collections:\n",
    "    print(f\"- {c.name}\")\n",
    "\n",
    "count = client.count(collection_name=\"ebook_library\")\n",
    "print(f\"\\nDocument Count in 'ebook_library': {count.count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79c795",
   "metadata": {},
   "source": [
    "## 2. Test Existing RAG Pipeline\n",
    "We will use the `get_rag()` factory from `scripts/ai/rag/rag_optimized.py`. This handles the embedding model, parent-child retrieval, and reranking logic automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3073462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scripts.ai.rag.rag_optimized:Connecting to Qdrant Docker (http://localhost:6333)...\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:6333/collections \"HTTP/1.1 200 OK\"\n",
      "INFO:scripts.ai.rag.rag_optimized:Connected to Qdrant Docker Service.\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:6333/collections \"HTTP/1.1 200 OK\"\n",
      "INFO:scripts.ai.rag.rag_optimized:Loading FastEmbed model: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:6333/collections/ebook_library \"HTTP/1.1 200 OK\"\n",
      "INFO:scripts.ai.rag.rag_optimized:Initializing In-Memory document store...\n",
      "INFO:scripts.ai.rag.rag_optimized:Loading from fast JSON cache: d:\\Users\\wpoga\\Documents\\Python Scripts\\antigravity-agent-factory\\data\\rag\\parent_store_cache.json\n",
      "INFO:scripts.ai.rag.rag_optimized:Hydrated 3180 documents from cache.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:6333/collections/ebook_library/points/count \"HTTP/1.1 200 OK\"\n",
      "INFO:scripts.ai.rag.rag_optimized:Performing RAG warm-up query...\n",
      "INFO:scripts.ai.rag.rag_optimized:RAG warm-up complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying: 'What are the key principles of agentic workflows?'...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:6333/collections/ebook_library/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 [D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\a-practical-guide-to-building-agents.pdf]:\n",
      "Defining tools\n",
      "Tools extend your agent’s capabilities by using APIs from underlying applications or systems. For \n",
      "legacy systems without APIs, agents can rely on computer-use models to interact direct...\n",
      "\n",
      "Result 2 [D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\a-practical-guide-to-building-agents.pdf]:\n",
      "When should you \n",
      "build an agent?\n",
      "Building agents requires rethinking how your systems make decisions and handle complexity. \n",
      "Unlike conventional automation, agents are uniquely suited to workflows whe...\n",
      "\n",
      "Result 3 [D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\a-practical-guide-to-building-agents.pdf]:\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "32\n",
      "32\n",
      "33\n",
      ")\n",
      "\n",
      " \n",
      " main():\n",
      "    msg = input(\n",
      ")\n",
      "\n",
      "    orchestrator_output = await Runner.run(\n",
      "        manager_agent,msg)\n",
      "\n",
      "    \n",
      " message \n",
      " orchestrator_output.new_messages:\n",
      "        \n",
      "(f\"  ...\n",
      "\n",
      "Result 4 [D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\a-practical-guide-to-building-agents.pdf]:\n",
      "As you evaluate where agents can add value, prioritize workflows that have previously resisted \n",
      "automation, especially where traditional methods encounter friction:\n",
      "01\n",
      "Complex  \n",
      "decision-making: \n",
      "Work...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG (warmup=True will load models)\n",
    "rag = get_rag(warmup=True)\n",
    "\n",
    "query = \"What are the key principles of agentic workflows?\"\n",
    "print(f\"\\nQuerying: '{query}'...\\n\")\n",
    "\n",
    "results = rag.query(query)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "    print(f\"Result {i} [{source}]:\\n{doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfed333",
   "metadata": {},
   "source": [
    "## 3. MCP Server Prototype with FastMCP\n",
    "\n",
    "We will use the `fastmcp` library to define the server tools. \n",
    "This allows a very clean, decorator-based definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd805d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:6333/collections/ebook_library/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered tools: <function query_rag at 0x000002A0F529C900> <function ingest_document at 0x000002A0FB16E840>\n",
      "\n",
      "Testing 'query_rag' function directly:\n",
      "Source: D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\AI Fluency_ Key Terminology Cheat Sheet-OCR.pdf\n",
      "Content: A type of error when AI confidently states something that \n",
      "sounds plausible, but is actually incorrect. \n",
      "Knowledge cutoff date \n",
      "The point after which an AI model has no built-in knowledge \n",
      "of the world, based on when it was trained. \n",
      "Reasoning or thinking models \n",
      "Types of AI models specifically designed to think step-by-\n",
      "step through complex problems, showing improved \n",
      "capabil...\n"
     ]
    }
   ],
   "source": [
    "from fastmcp import FastMCP\n",
    "\n",
    "# Create the MCP Server instance\n",
    "mcp = FastMCP(\"RAG Agent Server\")\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def query_rag(query: str) -> str:\n",
    "    \"\"\"Semantically search the ebook library for technical concepts.\"\"\"\n",
    "    rag = get_rag(warmup=False)\n",
    "    docs = rag.query(query)\n",
    "    if not docs:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Source: {d.metadata.get('source', 'Unknown')}\\nContent: {d.page_content}\"\n",
    "            for d in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def ingest_document(path: str) -> str:\n",
    "    \"\"\"Ingest a PDF document into the RAG library.\"\"\"\n",
    "    rag = get_rag(warmup=False)\n",
    "    try:\n",
    "        rag.ingest_ebook(path)\n",
    "        return f\"Successfully ingested {path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error ingesting document: {str(e)}\"\n",
    "\n",
    "\n",
    "# Verify Tools are Registered\n",
    "# Note: mcp.list_tools() is typically handled by the server loop.\n",
    "# However, the decorators register them internally.\n",
    "print(\"Registered tools:\", query_rag, ingest_document)\n",
    "\n",
    "# We can call the decorated functions directly to test logic\n",
    "print(\"\\nTesting 'query_rag' function directly:\")\n",
    "print(query_rag(\"explain RAG retrieval\")[:500] + \"...\")\n",
    "\n",
    "# To run the server (blocking), one would normally do:\n",
    "# mcp.run()\n",
    "# For this notebook, we just demonstrate the definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c9090",
   "metadata": {},
   "source": [
    "## 4. Test Existing RAG MCP Server (Client Mode)\n",
    "\n",
    "**Pre-requisite:** Ensure the RAG MCP Server is running! (e.g., via `start_rag_server.bat`)\n",
    "This section uses LangChain's `MultiServerMCPClient` to connect to the RAG server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0910469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=7ebaa33b279f4b3dad7c408b67a48867 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=7ebaa33b279f4b3dad7c408b67a48867 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=7ebaa33b279f4b3dad7c408b67a48867 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MCP Server at http://127.0.0.1:8000/sse via LangChain MultiServerMCPClient...\n",
      "Client Initialized. Fetching tools...\n",
      "\n",
      "Available Tools: ['search_library', 'ingest_document', 'list_library_sources']\n",
      "\n",
      "Invoking 'search_library' with query: 'Inhaltsübersicht Künstliche Intelligenz Russell?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=65a98df8da4e4978af97d26620820de5 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=65a98df8da4e4978af97d26620820de5 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=65a98df8da4e4978af97d26620820de5 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=65a98df8da4e4978af97d26620820de5 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=cb9791c74260425596b40cd53ef724d9 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=cb9791c74260425596b40cd53ef724d9 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Response ---\n",
      "[{'type': 'text', 'text': '### Result 1 (Source: Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf)\\n\\nKünstliche Intelligenz\\n\\n---\\n\\n### Result 2 (Source: Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf)\\n\\n39\\n1.3  Die Geschichte der künstlichen Intelligenz \\nwurde eine Besprechung dieses Buches genauso bekannt wie das eigentliche Buch und \\nführte dazu, das Interesse am Behaviorismus so gut wie verschwinden zu lassen. Der \\nAutor dieser Besprechung war der Linguist Noam Chomsky, der gerade ein Buch über \\nseine eigene Theorie veröffentlicht hatte, Syntactic Structures. Chomsky zeigte, dass die \\nbehavioristische Theorie das Konzept der Kreativität in der Sprache nicht berücksichti-\\ngen konnte – sie erklärte nicht, wie ein Kind Sätze verstehen und erzeugen konnte, die es \\nnie zuvor gehört hatte. Die Theorie von Chomsky – die auf syntaktischen Modellen \\nbasierte, welche auf den indischen Linguistiker Panini zurückgehen (etwa 350 v. Chr.) – \\nkonnte dies erklären und anders als vorhergehende Theorien war sie formal genug, dass \\nsie im Prinzip programmiert werden konnte.\\nDie moderne Linguistik und die KI wurden also etwa gleichzeitig „geboren“ und wuch-\\nsen zusammen auf, mit einer Schnittmenge in einem hybriden Gebiet, der sogenannten \\nComputerlinguistik oder natürlichen Sprachverarbeitung. Das Problem, Sprache zu \\nverstehen, stellte sich bald als deutlich komplizierter heraus, als es 1957 erschien. \\nSprachverstehen bedingt, dass man sowohl das Thema als auch den Kontext versteht \\nund nicht nur die Struktur von Sätzen. Das scheint offensichtlich zu sein, wurde aber \\nerst in den 60er Jahren allgemein anerkannt. Ein Großteil der früheren Arbeiten im Hin-\\nblick auf die Wissensrepräsentation (wie man Wissen in einer Form darstellt, dass ein \\nComputer damit schlussfolgern kann) war an die Sprache gebunden und wurde von \\nLinguistikforschern erkundet, was wiederum mit Jahrzehnten von Arbeiten im Hinblick \\nauf die philosophische Analyse der Sprache verbunden war.\\n1.3\\nDie Geschichte der künstlichen Intelligenz \\nNachdem wir nun die Hintergründe beleuchtet haben, wollen wir die eigentliche Ent-\\nwicklung der KI betrachten.\\n1.3.1\\nDer Reifungsprozess der künstlichen Intelligenz (1943–1955)\\n\\n---\\n\\n### Result 3 (Source: Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf)\\n\\nleme detaillierter.\\nEin wichtiger Aspekt ist dabei stets zu beachten: Wir werden schnell erkennen, dass \\ndie perfekte Rationalität – also immer das Richtige zu tun – in komplexen Umgebun-\\ngen nicht erreichbar ist. Die Anforderungen an die Computerleistung sind einfach zu \\nhoch. Für einen Großteil dieses Buches wollen wir jedoch die Arbeitshypothese ein-\\nführen, dass eine perfekte Rationalität ein guter Ausgangspunkt für die Analyse ist. \\nDamit wird das Problem vereinfacht und eine geeignete Einstellung für die meisten \\nGrundlagen auf diesem Gebiet bereitgestellt. Kapitel 5 und 17 beschäftigen sich expli-\\nzit mit dem Problem der begrenzten Rationalität – wie man angemessen handelt, \\nwenn nicht genügend Zeit bleibt, alle wünschenswerten Berechnungen vorzunehmen.\\n1.2\\nDie Grundlagen der künstlichen Intelligenz \\nIn diesem Abschnitt geben wir einen kurzen Abriss der Disziplinen, der Ideen, Anschau-\\nungen und Techniken, die zur KI beigetragen haben. Wie bei jedem Abriss müssen wir \\nuns auf eine kleine Anzahl von Menschen, Ereignissen und Gedanken konzentrieren und \\nandere ignorieren, die möglicherweise ebenfalls wichtig waren. Wir stellen einige zen-\\ntrale Fragen in den Mittelpunkt der Diskussion.2 \\n2\\nWir wollen den Eindruck vermeiden, diese Fragen seien die einzigen, um die es in diesen Dis-\\nziplinen geht, oder dass die Disziplin die KI als ultimatives Ziel anstrebt.\\nTipp\\n\\n---\\n\\n### Result 4 (Source: Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf)\\n\\n1209\\n27.3  Gehen wir in die richtige Richtung? \\nandere Möglichkeit, um effektive Strategien zum Steuern der Deliberation zu erhalten: \\nIm Wesentlichen werden Berechnungen gefördert, die zu besseren Entscheidungen \\nführen, während diejenigen, die sich als wirkungslos erweisen, bestraft werden. Die-\\nser Ansatz vermeidet die Probleme, die sich durch die kurzsichtige Berechnung eines \\neinfachen Informationswertes ergeben.\\nDas Metaschließen ist ein spezifisches Beispiel für eine reflexive Architektur – d.h. für \\neine Architektur, die Deliberation über die Recheneinheiten und Aktionen erlaubt, die \\ninnerhalb der eigentlichen Architektur auftreten. Eine theoretische Grundlage für refle-\\nxive Architekturen kann erstellt werden, indem man einen gemeinsamen Zustandsraum \\ndefiniert, der sich aus dem Umgebungszustand und dem Rechenzustand des eigent-\\nlichen Agenten zusammensetzt. Algorithmen für Entscheidungsfindung und Lernen las-\\nsen sich so entwerfen, dass sie auf diesem gemeinsamen Zustandsraum arbeiten und \\ndabei herangezogen werden, um die Berechnungsaktivitäten des Agenten zu implemen-\\ntieren und zu verbessern. Letztlich erwarten wir, dass aufgabenspezifische Algorithmen \\nwie Alpha-Beta-Suche und Rückwärtsverkettung aus den KI-Systemen verschwinden \\nund durch allgemeine Methoden ersetzt werden, die diese Berechnungen des Agenten \\nin die Richtung einer effizienten Erstellung hochqualitativer Entscheidungen lenken.\\n27.3 Gehen wir in die richtige Richtung? \\nDer vorige Abschnitt hat einige Vorteile sowie viele Gelegenheiten für weiteren Fort-\\nschritt aufgezeigt. Aber wohin führt das alles? Dreyfus (1992) nennt die Analogie, zu \\nversuchen, auf den Mond zu gelangen, indem man auf einen Baum steigt. Man kann \\neinen stetigen Fortschritt berichten, solange man auf dem Weg zum Baumgipfel ist. In \\ndiesem Abschnitt betrachten wir, ob der aktuelle Weg der künstlichen Intelligenz \\nmehr wie eine Baumbesteigung oder mehr wie eine Raketenfahrt ist.', 'id': 'lc_38c9c973-d596-4bbc-939e-c1b3d02d0a87'}]...\n",
      "----------------\n",
      "\n",
      "Invoking 'list_library_sources'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=cb9791c74260425596b40cd53ef724d9 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=cb9791c74260425596b40cd53ef724d9 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sources ---\n",
      "[{'type': 'text', 'text': 'Indexed Documents:\\n- 2026 Agentic Coding Trends Report.pdf\\n- AI Fluency_ Key Terminology Cheat Sheet-OCR.pdf\\n- Anthropic-enterprise-ebook-digital.pdf\\n- Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf\\n- The-Complete-Guide-to-Building-Skill-for-Claude.pdf\\n- WEF_AI_Agents_in_Action_Foundations_for_Evaluation_and_Governance_2025.pdf\\n- a-practical-guide-to-building-agents.pdf\\n- claudes-constitution_webPDF_26-01.26a.pdf\\n- practices-for-governing-agentic-ai-systems.pdf\\n- woodridge_intelligent_agents.pdf', 'id': 'lc_ce8fcf14-dca1-40eb-875b-9df563765ef3'}]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "SERVER_URL = \"http://127.0.0.1:8000/sse\"\n",
    "\n",
    "\n",
    "async def test_langchain_mcp_client():\n",
    "    print(\n",
    "        f\"Connecting to MCP Server at {SERVER_URL} via LangChain MultiServerMCPClient...\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Initialize client with server configuration\n",
    "        client = MultiServerMCPClient(\n",
    "            connections={\"rag_server\": {\"url\": SERVER_URL, \"transport\": \"sse\"}}\n",
    "        )\n",
    "\n",
    "        print(\"Client Initialized. Fetching tools...\")\n",
    "\n",
    "        # 1. List Available Tools\n",
    "        # get_tools() retrieves tools from all connected servers\n",
    "        tools = await client.get_tools()\n",
    "        print(f\"\\nAvailable Tools: {[tool.name for tool in tools]}\")\n",
    "\n",
    "        # 2. Invoke 'search_library' tool\n",
    "        # NOTE: LangChain wraps tools as Runnable objects\n",
    "        # We can find the tool by name and invoke it\n",
    "        search_tool = next((t for t in tools if t.name == \"search_library\"), None)\n",
    "\n",
    "        if search_tool:\n",
    "            query = \"Inhaltsübersicht Künstliche Intelligenz Russell?\"\n",
    "            print(f\"\\nInvoking 'search_library' with query: '{query}'\")\n",
    "\n",
    "            # Invoke the tool\n",
    "            result = await search_tool.ainvoke({\"query\": query})\n",
    "            print(f\"\\n--- Response ---\\n{result[:500]}...\\n----------------\")\n",
    "        else:\n",
    "            print(\"Warning: 'search_library' tool not found on server.\")\n",
    "\n",
    "        # 3. Invoke 'list_library_sources' tool\n",
    "        list_tool = next((t for t in tools if t.name == \"list_library_sources\"), None)\n",
    "\n",
    "        if list_tool:\n",
    "            print(\"\\nInvoking 'list_library_sources'...\")\n",
    "            sources_result = await list_tool.ainvoke({})\n",
    "            print(f\"\\n--- Sources ---\\n{sources_result}\\n---------------\")\n",
    "        else:\n",
    "            print(\"Warning: 'list_library_sources' tool not found on server.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LangChain MCP Client Error: {e}\")\n",
    "\n",
    "\n",
    "# Run the async test\n",
    "await test_langchain_mcp_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9bed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"GEMINI_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    google_api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True,\n",
    ")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"ai-dev-agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae78f51",
   "metadata": {},
   "source": [
    "## 6. Official LangGraph Agentic RAG implementation\n",
    "\n",
    "This converts the execution into the standard LangGraph `StateGraph` pattern. It introduces a strict `SystemMessage` to prevent infinite tool calling loops and properly structures the output extraction for Jupyter Markdown rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "import operator\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "SERVER_URL = \"http://127.0.0.1:8000/sse\"\n",
    "\n",
    "\n",
    "# --- Schema Patching for Gemini ---\n",
    "def _remove_additional_properties(schema: Dict[str, Any]) -> None:\n",
    "    if not isinstance(schema, dict):\n",
    "        return\n",
    "    if \"additionalProperties\" in schema:\n",
    "        del schema[\"additionalProperties\"]\n",
    "    for key, value in list(schema.items()):\n",
    "        if isinstance(value, dict):\n",
    "            _remove_additional_properties(value)\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    _remove_additional_properties(item)\n",
    "\n",
    "\n",
    "def patch_tool_schema(tool):\n",
    "    if hasattr(tool, \"args_schema\") and tool.args_schema:\n",
    "        if isinstance(tool.args_schema, dict):\n",
    "            _remove_additional_properties(tool.args_schema)\n",
    "        else:\n",
    "            original_schema_method = tool.args_schema.schema\n",
    "\n",
    "            def custom_schema(*args, **kwargs):\n",
    "                s = original_schema_method(*args, **kwargs)\n",
    "                _remove_additional_properties(s)\n",
    "                return s\n",
    "\n",
    "            tool.args_schema.schema = custom_schema\n",
    "    return tool\n",
    "\n",
    "\n",
    "# --- LangGraph State Definition ---\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "# --- Execute Graph ---\n",
    "async def run_langgraph_rag(query: str):\n",
    "    print(\"Connecting to RAG MCP Server...\")\n",
    "    client = MultiServerMCPClient(\n",
    "        connections={\"rag_server\": {\"url\": SERVER_URL, \"transport\": \"sse\"}}\n",
    "    )\n",
    "    raw_tools = await client.get_tools()\n",
    "    tools = [patch_tool_schema(t) for t in raw_tools]\n",
    "    print(f\"Retrieved {len(tools)} tools: {[t.name for t in tools]}\\n\")\n",
    "\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # We define the LLM decision node\n",
    "    def agent_node(state: AgentState):\n",
    "        # Add a strict system message to prevent runaway tool loops\n",
    "        sys_msg = SystemMessage(\n",
    "            content=\"\"\"\n",
    "            You are an expert AI assistant performing Agentic RAG.\n",
    "            CRITICAL RULES:\n",
    "            1. If you have retrieved sufficient information to answer the user's question, DO NOT CALL ANY MORE TOOLS. Synthesize the final answer immediately.\n",
    "            2. ONLY query the library a maximum of 5 times. If you cannot find the answer after 5 searches, state that you cannot find it.\n",
    "        \"\"\"\n",
    "        )\n",
    "        messages = [sys_msg] + state[\"messages\"]\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # We define the explicit LangGraph workflow\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Add the reasoning node and the tool execution node\n",
    "    workflow.add_node(\"agent\", agent_node)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    # Build the edges (Reasoning -> Tool or END)\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\", tools_condition, {\"tools\": \"tools\", END: END}\n",
    "    )\n",
    "    # Once a tool executes, return to the agent reasoning node\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    app = workflow.compile()\n",
    "\n",
    "    # query = \"How should agent skills be build? Describe with an example which aspects are important\"\n",
    "    print(f\"Executing Native LangGraph with Query: {query}\\n\")\n",
    "\n",
    "    try:\n",
    "        async for output in app.astream(\n",
    "            {\"messages\": [HumanMessage(content=query)]}, stream_mode=\"updates\"\n",
    "        ):\n",
    "            for node_name, node_output in output.items():\n",
    "                # Get the most recent message\n",
    "                ai_msg = node_output[\"messages\"][-1]\n",
    "\n",
    "                if node_name == \"agent\":\n",
    "                    if hasattr(ai_msg, \"tool_calls\") and ai_msg.tool_calls:\n",
    "                        for call in ai_msg.tool_calls:\n",
    "                            print(\n",
    "                                f\"⚒️ LangGraph Routing to Tool: {call['name']} ({call['args']})\"\n",
    "                            )\n",
    "                    else:\n",
    "                        print(\"\\n--- LangGraph Final Output ---\")\n",
    "                        if isinstance(ai_msg.content, list):\n",
    "                            final_text = \"\\n\".join(\n",
    "                                [\n",
    "                                    chunk.get(\"text\", \"\")\n",
    "                                    for chunk in ai_msg.content\n",
    "                                    if isinstance(chunk, dict)\n",
    "                                    and chunk.get(\"type\") == \"text\"\n",
    "                                ]\n",
    "                            )\n",
    "                            display(Markdown(final_text))\n",
    "                        else:\n",
    "                            display(Markdown(str(ai_msg.content)))\n",
    "                elif node_name == \"tools\":\n",
    "                    print(\n",
    "                        f\"✅ Tool Node executed. Returning {len(str(ai_msg.content))} characters back to Agent.\\n\"\n",
    "                    )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# query = \"Wie entscheiden Agenten über ihre Aktionen?\"\n",
    "query = \"How should skills for Claude be created? List the most important aspects and provide exmaples. Provide also the sources of your answers\"\n",
    "await run_langgraph_rag(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursor-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
