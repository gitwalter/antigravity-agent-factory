{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# RAG Analysis & MCP Server Prototype\n",
    "\n",
    "This notebook serves two purposes:\n",
    "1. Validating the existing RAG pipeline (`scripts/ai/rag`) against the `ebook_library` Qdrant collection.\n",
    "2. Prototyping an MCP Server to expose this RAG functionality to Antigravity agents.\n",
    "3. Testing the running MCP Server (Client Mode) using LangChain's MultiServerMCPClient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install FastMCP and LangChain MCP Adapters\n",
    "# %pip install \"fastmcp>=3.0.0\" langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Ensure we can import from project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from scripts.ai.rag.rag_optimized import get_rag, OptimizedRAG\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9428b",
   "metadata": {},
   "source": [
    "## 1. Direct Qdrant Inspection\n",
    "First, let's verify we can connect to the Docker instance and that the collection exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6647bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/ebook_library/points/count \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Collections:\n",
      "- ebook_library\n",
      "\n",
      "Document Count in 'ebook_library': 28295\n"
     ]
    }
   ],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "collections = client.get_collections()\n",
    "\n",
    "print(\"Available Collections:\")\n",
    "for c in collections.collections:\n",
    "    print(f\"- {c.name}\")\n",
    "\n",
    "count = client.count(collection_name=\"ebook_library\")\n",
    "print(f\"\\nDocument Count in 'ebook_library': {count.count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79c795",
   "metadata": {},
   "source": [
    "## 2. Test Existing RAG Pipeline\n",
    "We will use the `get_rag()` factory from `scripts/ai/rag/rag_optimized.py`. This handles the embedding model, parent-child retrieval, and reranking logic automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3073462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scripts.ai.rag.rag_optimized:Connecting to Qdrant Docker (http://localhost:6333)...\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:6333/collections \"HTTP/1.1 200 OK\"\n",
      "INFO:scripts.ai.rag.rag_optimized:Connected to Qdrant Docker Service.\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:6333/collections \"HTTP/1.1 200 OK\"\n",
      "INFO:scripts.ai.rag.rag_optimized:Loading FastEmbed model: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:6333/collections/ebook_library \"HTTP/1.1 200 OK\"\n",
      "INFO:scripts.ai.rag.rag_optimized:Initializing In-Memory document store...\n",
      "INFO:scripts.ai.rag.rag_optimized:Loading from fast JSON cache: d:\\Users\\wpoga\\Documents\\Python Scripts\\antigravity-agent-factory\\data\\rag\\parent_store_cache.json\n",
      "INFO:scripts.ai.rag.rag_optimized:Hydrated 5839 documents from cache.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:6333/collections/ebook_library/points/count \"HTTP/1.1 200 OK\"\n",
      "INFO:scripts.ai.rag.rag_optimized:Performing RAG warm-up query...\n",
      "INFO:scripts.ai.rag.rag_optimized:RAG warm-up complete.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:6333/collections/ebook_library/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying: 'What are the key principles of agentic workflows?'...\n",
      "\n",
      "Result 1 [D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\a-practical-guide-to-building-agents.pdf]:\n",
      "When should you \n",
      "build an agent?\n",
      "Building agents requires rethinking how your systems make decisions and handle complexity. \n",
      "Unlike conventional automation, agents are uniquely suited to workflows whe...\n",
      "\n",
      "Result 2 [D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\a-practical-guide-to-building-agents.pdf]:\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "32\n",
      "32\n",
      "33\n",
      ")\n",
      "\n",
      " \n",
      " main():\n",
      "    msg = input(\n",
      ")\n",
      "\n",
      "    orchestrator_output = await Runner.run(\n",
      "        manager_agent,msg)\n",
      "\n",
      "    \n",
      " message \n",
      " orchestrator_output.new_messages:\n",
      "        \n",
      "(f\"  ...\n",
      "\n",
      "Result 3 [D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\a-practical-guide-to-building-agents.pdf]:\n",
      "Defining tools\n",
      "Tools extend your agent’s capabilities by using APIs from underlying applications or systems. For \n",
      "legacy systems without APIs, agents can rely on computer-use models to interact direct...\n",
      "\n",
      "Result 4 [D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\a-practical-guide-to-building-agents.pdf]:\n",
      "As you evaluate where agents can add value, prioritize workflows that have previously resisted \n",
      "automation, especially where traditional methods encounter friction:\n",
      "01\n",
      "Complex  \n",
      "decision-making: \n",
      "Work...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG (warmup=True will load models)\n",
    "rag = get_rag(warmup=True)\n",
    "\n",
    "query = \"What are the key principles of agentic workflows?\"\n",
    "print(f\"\\nQuerying: '{query}'...\\n\")\n",
    "\n",
    "results = rag.query(query)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "    print(f\"Result {i} [{source}]:\\n{doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfed333",
   "metadata": {},
   "source": [
    "## 3. MCP Server Prototype with FastMCP\n",
    "\n",
    "We will use the `fastmcp` library to define the server tools. \n",
    "This allows a very clean, decorator-based definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd805d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:6333/collections/ebook_library/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered tools: <function query_rag at 0x000002A0F529C900> <function ingest_document at 0x000002A0FB16E840>\n",
      "\n",
      "Testing 'query_rag' function directly:\n",
      "Source: D:\\Users\\wpoga\\Documents\\Ebooks\\Artificial Intelligence\\AI Fluency_ Key Terminology Cheat Sheet-OCR.pdf\n",
      "Content: A type of error when AI confidently states something that \n",
      "sounds plausible, but is actually incorrect. \n",
      "Knowledge cutoff date \n",
      "The point after which an AI model has no built-in knowledge \n",
      "of the world, based on when it was trained. \n",
      "Reasoning or thinking models \n",
      "Types of AI models specifically designed to think step-by-\n",
      "step through complex problems, showing improved \n",
      "capabil...\n"
     ]
    }
   ],
   "source": [
    "from fastmcp import FastMCP\n",
    "\n",
    "# Create the MCP Server instance\n",
    "mcp = FastMCP(\"RAG Agent Server\")\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def query_rag(query: str) -> str:\n",
    "    \"\"\"Semantically search the ebook library for technical concepts.\"\"\"\n",
    "    rag = get_rag(warmup=False)\n",
    "    docs = rag.query(query)\n",
    "    if not docs:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Source: {d.metadata.get('source', 'Unknown')}\\nContent: {d.page_content}\"\n",
    "            for d in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def ingest_document(path: str) -> str:\n",
    "    \"\"\"Ingest a PDF document into the RAG library.\"\"\"\n",
    "    rag = get_rag(warmup=False)\n",
    "    try:\n",
    "        rag.ingest_ebook(path)\n",
    "        return f\"Successfully ingested {path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error ingesting document: {str(e)}\"\n",
    "\n",
    "\n",
    "# Verify Tools are Registered\n",
    "# Note: mcp.list_tools() is typically handled by the server loop.\n",
    "# However, the decorators register them internally.\n",
    "print(\"Registered tools:\", query_rag, ingest_document)\n",
    "\n",
    "# We can call the decorated functions directly to test logic\n",
    "print(\"\\nTesting 'query_rag' function directly:\")\n",
    "print(query_rag(\"explain RAG retrieval\")[:500] + \"...\")\n",
    "\n",
    "# To run the server (blocking), one would normally do:\n",
    "# mcp.run()\n",
    "# For this notebook, we just demonstrate the definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c9090",
   "metadata": {},
   "source": [
    "## 4. Test Existing RAG MCP Server (Client Mode)\n",
    "\n",
    "**Pre-requisite:** Ensure the RAG MCP Server is running! (e.g., via `start_rag_server.bat`)\n",
    "This section uses LangChain's `MultiServerMCPClient` to connect to the RAG server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0910469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=cb3f9f01e8094df987f32bfba54bed18 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=cb3f9f01e8094df987f32bfba54bed18 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=cb3f9f01e8094df987f32bfba54bed18 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=72e328e65a544c8f90f1f7ef08465412 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=72e328e65a544c8f90f1f7ef08465412 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=72e328e65a544c8f90f1f7ef08465412 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=72e328e65a544c8f90f1f7ef08465412 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MCP Server at http://127.0.0.1:8000/sse via LangChain MultiServerMCPClient...\n",
      "Client Initialized. Fetching tools...\n",
      "\n",
      "Available Tools: ['search_library', 'get_ebook_toc', 'ingest_document', 'list_library_sources']\n",
      "\n",
      "Invoking 'search_library' with query: 'Inhaltsübersicht Künstliche Intelligenz Russell?'\n",
      "\n",
      "--- Response ---\n",
      "[{'type': 'text', 'text': '### Result 1 (Source: Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf)\\n\\nKünstliche Intelligenz\\n\\n---\\n\\n### Result 2 (Source: Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf)\\n\\nKünstliche Intelligenz\\n\\n---\\n\\n### Result 3 (Source: Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf)\\n\\n39\\n1.3  Die Geschichte der künstlichen Intelligenz \\nwurde eine Besprechung dieses Buches genauso bekannt wie das eigentliche Buch und \\nführte dazu, das Interesse am Behaviorismus so gut wie verschwinden zu lassen. Der \\nAutor dieser Besprechung war der Linguist Noam Chomsky, der gerade ein Buch über \\nseine eigene Theorie veröffentlicht hatte, Syntactic Structures. Chomsky zeigte, dass die \\nbehavioristische Theorie das Konzept der Kreativität in der Sprache nicht berücksichti-\\ngen konnte – sie erklärte nicht, wie ein Kind Sätze verstehen und erzeugen konnte, die es \\nnie zuvor gehört hatte. Die Theorie von Chomsky – die auf syntaktischen Modellen \\nbasierte, welche auf den indischen Linguistiker Panini zurückgehen (etwa 350 v. Chr.) – \\nkonnte dies erklären und anders als vorhergehende Theorien war sie formal genug, dass \\nsie im Prinzip programmiert werden konnte.\\nDie moderne Linguistik und die KI wurden also etwa gleichzeitig „geboren“ und wuch-\\nsen zusammen auf, mit einer Schnittmenge in einem hybriden Gebiet, der sogenannten \\nComputerlinguistik oder natürlichen Sprachverarbeitung. Das Problem, Sprache zu \\nverstehen, stellte sich bald als deutlich komplizierter heraus, als es 1957 erschien. \\nSprachverstehen bedingt, dass man sowohl das Thema als auch den Kontext versteht \\nund nicht nur die Struktur von Sätzen. Das scheint offensichtlich zu sein, wurde aber \\nerst in den 60er Jahren allgemein anerkannt. Ein Großteil der früheren Arbeiten im Hin-\\nblick auf die Wissensrepräsentation (wie man Wissen in einer Form darstellt, dass ein \\nComputer damit schlussfolgern kann) war an die Sprache gebunden und wurde von \\nLinguistikforschern erkundet, was wiederum mit Jahrzehnten von Arbeiten im Hinblick \\nauf die philosophische Analyse der Sprache verbunden war.\\n1.3\\nDie Geschichte der künstlichen Intelligenz \\nNachdem wir nun die Hintergründe beleuchtet haben, wollen wir die eigentliche Ent-\\nwicklung der KI betrachten.\\n1.3.1\\nDer Reifungsprozess der künstlichen Intelligenz (1943–1955)\\n\\n---\\n\\n### Result 4 (Source: Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf)\\n\\n39\\n1.3  Die Geschichte der künstlichen Intelligenz \\nwurde eine Besprechung dieses Buches genauso bekannt wie das eigentliche Buch und \\nführte dazu, das Interesse am Behaviorismus so gut wie verschwinden zu lassen. Der \\nAutor dieser Besprechung war der Linguist Noam Chomsky, der gerade ein Buch über \\nseine eigene Theorie veröffentlicht hatte, Syntactic Structures. Chomsky zeigte, dass die \\nbehavioristische Theorie das Konzept der Kreativität in der Sprache nicht berücksichti-\\ngen konnte – sie erklärte nicht, wie ein Kind Sätze verstehen und erzeugen konnte, die es \\nnie zuvor gehört hatte. Die Theorie von Chomsky – die auf syntaktischen Modellen \\nbasierte, welche auf den indischen Linguistiker Panini zurückgehen (etwa 350 v. Chr.) – \\nkonnte dies erklären und anders als vorhergehende Theorien war sie formal genug, dass \\nsie im Prinzip programmiert werden konnte.\\nDie moderne Linguistik und die KI wurden also etwa gleichzeitig „geboren“ und wuch-\\nsen zusammen auf, mit einer Schnittmenge in einem hybriden Gebiet, der sogenannten \\nComputerlinguistik oder natürlichen Sprachverarbeitung. Das Problem, Sprache zu \\nverstehen, stellte sich bald als deutlich komplizierter heraus, als es 1957 erschien. \\nSprachverstehen bedingt, dass man sowohl das Thema als auch den Kontext versteht \\nund nicht nur die Struktur von Sätzen. Das scheint offensichtlich zu sein, wurde aber \\nerst in den 60er Jahren allgemein anerkannt. Ein Großteil der früheren Arbeiten im Hin-\\nblick auf die Wissensrepräsentation (wie man Wissen in einer Form darstellt, dass ein \\nComputer damit schlussfolgern kann) war an die Sprache gebunden und wurde von \\nLinguistikforschern erkundet, was wiederum mit Jahrzehnten von Arbeiten im Hinblick \\nauf die philosophische Analyse der Sprache verbunden war.\\n1.3\\nDie Geschichte der künstlichen Intelligenz \\nNachdem wir nun die Hintergründe beleuchtet haben, wollen wir die eigentliche Ent-\\nwicklung der KI betrachten.\\n1.3.1\\nDer Reifungsprozess der künstlichen Intelligenz (1943–1955)', 'id': 'lc_d0051a8c-2e6f-4335-962e-da308d5f83a1'}]...\n",
      "----------------\n",
      "\n",
      "Invoking 'list_library_sources'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=5f3e3ae2d05e454fb5b58a79aaa4a558 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=5f3e3ae2d05e454fb5b58a79aaa4a558 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=5f3e3ae2d05e454fb5b58a79aaa4a558 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=5f3e3ae2d05e454fb5b58a79aaa4a558 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sources ---\n",
      "[{'type': 'text', 'text': 'Indexed Documents:\\n- 2026 Agentic Coding Trends Report.pdf\\n- AI Fluency_ Key Terminology Cheat Sheet-OCR.pdf\\n- Anthropic-enterprise-ebook-digital.pdf\\n- Stuart Russell_Künstliche_Intelligenz-_Ein_moderner_Ansatz_(3.,_aktualisierte_Auflage).pdf\\n- The-Complete-Guide-to-Building-Skill-for-Claude.pdf\\n- WEF_AI_Agents_in_Action_Foundations_for_Evaluation_and_Governance_2025.pdf\\n- a-practical-guide-to-building-agents.pdf\\n- claudes-constitution_webPDF_26-01.26a.pdf\\n- practices-for-governing-agentic-ai-systems.pdf\\n- woodridge_intelligent_agents.pdf', 'id': 'lc_467153b4-aeaa-4272-b81c-6bb5947cbe27'}]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "SERVER_URL = \"http://127.0.0.1:8000/sse\"\n",
    "\n",
    "\n",
    "async def test_langchain_mcp_client():\n",
    "    print(\n",
    "        f\"Connecting to MCP Server at {SERVER_URL} via LangChain MultiServerMCPClient...\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Initialize client with server configuration\n",
    "        client = MultiServerMCPClient(\n",
    "            connections={\"rag_server\": {\"url\": SERVER_URL, \"transport\": \"sse\"}}\n",
    "        )\n",
    "\n",
    "        print(\"Client Initialized. Fetching tools...\")\n",
    "\n",
    "        # 1. List Available Tools\n",
    "        # get_tools() retrieves tools from all connected servers\n",
    "        tools = await client.get_tools()\n",
    "        print(f\"\\nAvailable Tools: {[tool.name for tool in tools]}\")\n",
    "\n",
    "        # 2. Invoke 'search_library' tool\n",
    "        # NOTE: LangChain wraps tools as Runnable objects\n",
    "        # We can find the tool by name and invoke it\n",
    "        search_tool = next((t for t in tools if t.name == \"search_library\"), None)\n",
    "\n",
    "        if search_tool:\n",
    "            query = \"Inhaltsübersicht Künstliche Intelligenz Russell?\"\n",
    "            print(f\"\\nInvoking 'search_library' with query: '{query}'\")\n",
    "\n",
    "            # Invoke the tool\n",
    "            result = await search_tool.ainvoke({\"query\": query})\n",
    "            print(f\"\\n--- Response ---\\n{result[:500]}...\\n----------------\")\n",
    "        else:\n",
    "            print(\"Warning: 'search_library' tool not found on server.\")\n",
    "\n",
    "        # 3. Invoke 'list_library_sources' tool\n",
    "        list_tool = next((t for t in tools if t.name == \"list_library_sources\"), None)\n",
    "\n",
    "        if list_tool:\n",
    "            print(\"\\nInvoking 'list_library_sources'...\")\n",
    "            sources_result = await list_tool.ainvoke({})\n",
    "            print(f\"\\n--- Sources ---\\n{sources_result}\\n---------------\")\n",
    "        else:\n",
    "            print(\"Warning: 'list_library_sources' tool not found on server.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LangChain MCP Client Error: {e}\")\n",
    "\n",
    "\n",
    "# Run the async test\n",
    "await test_langchain_mcp_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9bed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"GEMINI_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    google_api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True,\n",
    ")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"ai-dev-agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae78f51",
   "metadata": {},
   "source": [
    "## 6. Official LangGraph Agentic RAG implementation\n",
    "\n",
    "This converts the execution into the standard LangGraph `StateGraph` pattern. It introduces a strict `SystemMessage` to prevent infinite tool calling loops and properly structures the output extraction for Jupyter Markdown rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb4d7117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=90c075b2bd1942e1966ac0dea72593b9 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=90c075b2bd1942e1966ac0dea72593b9 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=90c075b2bd1942e1966ac0dea72593b9 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to RAG MCP Server...\n",
      "Retrieved 4 tools: ['search_library', 'get_ebook_toc', 'ingest_document', 'list_library_sources']\n",
      "\n",
      "Executing Native LangGraph with Query: How should skills for Claude be created? List the most important aspects and provide exmaples. Provide also the sources of your answers\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=e3f3864dc2394c359549606a67362d0c \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=e3f3864dc2394c359549606a67362d0c \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=e3f3864dc2394c359549606a67362d0c \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚒️ LangGraph Routing to Tool: search_library ({'query': 'How to create skills for Claude'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=e3f3864dc2394c359549606a67362d0c \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tool Node executed. Returning 3625 characters back to Agent.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=3ec25a18cae845fbbd63b6ca1635fa69 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=3ec25a18cae845fbbd63b6ca1635fa69 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=3ec25a18cae845fbbd63b6ca1635fa69 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚒️ LangGraph Routing to Tool: search_library ({'query': 'best practices for creating Claude skills'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=3ec25a18cae845fbbd63b6ca1635fa69 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tool Node executed. Returning 3625 characters back to Agent.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=9536db5547b24cffa47e50a3d7256a1a \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚒️ LangGraph Routing to Tool: search_library ({'query': 'Claude skills structure and best practices'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=9536db5547b24cffa47e50a3d7256a1a \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=9536db5547b24cffa47e50a3d7256a1a \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=9536db5547b24cffa47e50a3d7256a1a \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tool Node executed. Returning 4354 characters back to Agent.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/sse \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=3e07297966fe47cc853a3bb1e62ecf89 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=3e07297966fe47cc853a3bb1e62ecf89 \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=3e07297966fe47cc853a3bb1e62ecf89 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚒️ LangGraph Routing to Tool: search_library ({'query': 'technical requirements for Claude skills'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/messages/?session_id=3e07297966fe47cc853a3bb1e62ecf89 \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tool Node executed. Returning 4339 characters back to Agent.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LangGraph Final Output ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Skills for Claude are created as a set of instructions packaged in a simple folder, teaching Claude to handle specific tasks or workflows. This allows for customization and ensures Claude follows specific processes consistently without needing repeated explanations.\n",
       "\n",
       "The most important aspects for creating skills include:\n",
       "\n",
       "*   **Structure:** A skill is a folder containing:\n",
       "    *   `SKILL.md` (required): Instructions in Markdown with YAML frontmatter.\n",
       "    *   `scripts/` (optional): Executable code (Python, Bash, etc.).\n",
       "    *   `references/` (optional): Documentation loaded as needed.\n",
       "    *   `assets/` (optional): Templates, fonts, icons used in output.\n",
       "*   **Core Design Principles:**\n",
       "    *   **Progressive Disclosure:** A three-level system (YAML frontmatter, `SKILL.md` body, and linked files) that minimizes token usage while maintaining expertise.\n",
       "    *   **Composability:** Skills should work well together, not assuming they are the only capability available.\n",
       "    *   **Portability:** Skills should work identically across Claude.ai, Claude Code, and the API without modification, provided the environment supports any dependencies.\n",
       "\n",
       "**Examples:**\n",
       "Skills are powerful for repeatable workflows such as:\n",
       "*   Generating frontend designs from specifications.\n",
       "*   Conducting research with a consistent methodology.\n",
       "*   Creating documents that adhere to a team's style guide.\n",
       "*   Orchestrating multi-step processes.\n",
       "\n",
       "**Sources:**\n",
       "*   The-Complete-Guide-to-Building-Skill-for-Claude.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "import operator\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "SERVER_URL = \"http://127.0.0.1:8000/sse\"\n",
    "\n",
    "\n",
    "# --- Schema Patching for Gemini ---\n",
    "def _remove_additional_properties(schema: Dict[str, Any]) -> None:\n",
    "    if not isinstance(schema, dict):\n",
    "        return\n",
    "    if \"additionalProperties\" in schema:\n",
    "        del schema[\"additionalProperties\"]\n",
    "    for key, value in list(schema.items()):\n",
    "        if isinstance(value, dict):\n",
    "            _remove_additional_properties(value)\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    _remove_additional_properties(item)\n",
    "\n",
    "\n",
    "def patch_tool_schema(tool):\n",
    "    if hasattr(tool, \"args_schema\") and tool.args_schema:\n",
    "        if isinstance(tool.args_schema, dict):\n",
    "            _remove_additional_properties(tool.args_schema)\n",
    "        else:\n",
    "            original_schema_method = tool.args_schema.schema\n",
    "\n",
    "            def custom_schema(*args, **kwargs):\n",
    "                s = original_schema_method(*args, **kwargs)\n",
    "                _remove_additional_properties(s)\n",
    "                return s\n",
    "\n",
    "            tool.args_schema.schema = custom_schema\n",
    "    return tool\n",
    "\n",
    "\n",
    "# --- LangGraph State Definition ---\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "# --- Execute Graph ---\n",
    "async def run_langgraph_rag(query: str):\n",
    "    print(\"Connecting to RAG MCP Server...\")\n",
    "    client = MultiServerMCPClient(\n",
    "        connections={\"rag_server\": {\"url\": SERVER_URL, \"transport\": \"sse\"}}\n",
    "    )\n",
    "    raw_tools = await client.get_tools()\n",
    "    tools = [patch_tool_schema(t) for t in raw_tools]\n",
    "    print(f\"Retrieved {len(tools)} tools: {[t.name for t in tools]}\\n\")\n",
    "\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # We define the LLM decision node\n",
    "    def agent_node(state: AgentState):\n",
    "        # Add a strict system message to prevent runaway tool loops\n",
    "        sys_msg = SystemMessage(\n",
    "            content=\"\"\"\n",
    "            You are an expert AI assistant performing Agentic RAG.\n",
    "            CRITICAL RULES:\n",
    "            1. If you have retrieved sufficient information to answer the user's question, DO NOT CALL ANY MORE TOOLS. Synthesize the final answer immediately.\n",
    "            2. ONLY query the library a maximum of 5 times. If you cannot find the answer after 5 searches, state that you cannot find it.\n",
    "        \"\"\"\n",
    "        )\n",
    "        messages = [sys_msg] + state[\"messages\"]\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # We define the explicit LangGraph workflow\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Add the reasoning node and the tool execution node\n",
    "    workflow.add_node(\"agent\", agent_node)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    # Build the edges (Reasoning -> Tool or END)\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\", tools_condition, {\"tools\": \"tools\", END: END}\n",
    "    )\n",
    "    # Once a tool executes, return to the agent reasoning node\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    app = workflow.compile()\n",
    "\n",
    "    # query = \"How should agent skills be build? Describe with an example which aspects are important\"\n",
    "    print(f\"Executing Native LangGraph with Query: {query}\\n\")\n",
    "\n",
    "    try:\n",
    "        async for output in app.astream(\n",
    "            {\"messages\": [HumanMessage(content=query)]}, stream_mode=\"updates\"\n",
    "        ):\n",
    "            for node_name, node_output in output.items():\n",
    "                # Get the most recent message\n",
    "                ai_msg = node_output[\"messages\"][-1]\n",
    "\n",
    "                if node_name == \"agent\":\n",
    "                    if hasattr(ai_msg, \"tool_calls\") and ai_msg.tool_calls:\n",
    "                        for call in ai_msg.tool_calls:\n",
    "                            print(\n",
    "                                f\"⚒️ LangGraph Routing to Tool: {call['name']} ({call['args']})\"\n",
    "                            )\n",
    "                    else:\n",
    "                        print(\"\\n--- LangGraph Final Output ---\")\n",
    "                        if isinstance(ai_msg.content, list):\n",
    "                            final_text = \"\\n\".join(\n",
    "                                [\n",
    "                                    chunk.get(\"text\", \"\")\n",
    "                                    for chunk in ai_msg.content\n",
    "                                    if isinstance(chunk, dict)\n",
    "                                    and chunk.get(\"type\") == \"text\"\n",
    "                                ]\n",
    "                            )\n",
    "                            display(Markdown(final_text))\n",
    "                        else:\n",
    "                            display(Markdown(str(ai_msg.content)))\n",
    "                elif node_name == \"tools\":\n",
    "                    print(\n",
    "                        f\"✅ Tool Node executed. Returning {len(str(ai_msg.content))} characters back to Agent.\\n\"\n",
    "                    )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# query = \"Wie entscheiden Agenten über ihre Aktionen?\"\n",
    "query = \"How should skills for Claude be created? List the most important aspects and provide exmaples. Provide also the sources of your answers\"\n",
    "await run_langgraph_rag(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursor-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
